{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ed851aa",
   "metadata": {},
   "source": [
    "## TimesFM（timesfm）公開API（関数・クラス）一覧を自動収集\n",
    "\n",
    "`inspect`（内省）と `pkgutil`（パッケージ走査）で、`timesfm` パッケージの公開API（関数・クラス）を列挙し、`pandas.DataFrame` に格納して表示します。\n",
    "\n",
    "- 収集対象：トップレベル + サブモジュール（importできる範囲）\n",
    "- import失敗（オプション依存など）はログとして別途確認可能\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecb600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python \"C:/moinfo/libs/timesfm/03_scripts/introspect_timesfm_features.py\" \\\n",
    "  --pkg timesfm \\\n",
    "  --out \"C:/moinfo/libs/timesfm/04_outputs/api/timesfm_public_api.json\" \\\n",
    "  --print\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5f7031",
   "metadata": {},
   "source": [
    "## TimesFM（timesfm）公開API（関数・クラス）を内省して DataFrame 化\n",
    "\n",
    "`inspect`（内省）と `pkgutil`（パッケージ走査）で、`timesfm` の公開API（関数・クラス）を抽出し、`pandas.DataFrame` に格納して表示します。\n",
    "\n",
    "- 収集対象：トップレベル + import可能なサブモジュール\n",
    "- import失敗（オプション依存など）は `modules_failed` に記録\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41969c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# 共通モジュールを import できるようにする\n",
    "COMMON_DIR = r\"C:\\moinfo\\common\"\n",
    "if COMMON_DIR not in sys.path:\n",
    "    sys.path.insert(0, COMMON_DIR)\n",
    "\n",
    "from moinfo_introspect.public_api import collect_public_api, to_dataframe\n",
    "\n",
    "# 公開APIを収集\n",
    "data = collect_public_api(\"timesfm\", include_submodules=True)\n",
    "\n",
    "# DataFrame化\n",
    "df = to_dataframe(data)\n",
    "\n",
    "# 概要（メトリクス）\n",
    "summary_df = pd.DataFrame({\n",
    "    \"metric\": [\"api_items\", \"modules_ok\", \"modules_failed\"],\n",
    "    \"value\": [len(data[\"api\"]), len(data[\"modules_ok\"]), len(data[\"modules_failed\"])]\n",
    "})\n",
    "display(summary_df)\n",
    "\n",
    "# 種別内訳（class/function）\n",
    "display(df[\"kind\"].value_counts(dropna=False).to_frame(\"count\"))\n",
    "\n",
    "# テーブル本体（重いなら df.head(50) に変更）\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c3613f",
   "metadata": {},
   "source": [
    "## 生成された JSON を読み込み、DataFrame として可視化\n",
    "\n",
    "CLIで書き出した `timesfm_public_api.json` を読み込んで表示します（NotebookとCLIの結果一致確認用）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56833fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "json_path = r\"C:\\moinfo\\libs\\timesfm\\04_outputs\\api\\timesfm_public_api.json\"\n",
    "\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data_from_json = json.load(f)\n",
    "\n",
    "df2 = pd.DataFrame(data_from_json.get(\"api\", []))\n",
    "\n",
    "display(pd.DataFrame({\n",
    "    \"metric\": [\"api_items\", \"modules_ok\", \"modules_failed\"],\n",
    "    \"value\": [len(df2), len(data_from_json.get(\"modules_ok\", [])), len(data_from_json.get(\"modules_failed\", []))]\n",
    "}))\n",
    "display(df2[\"kind\"].value_counts(dropna=False).to_frame(\"count\"))\n",
    "display(df2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2724ce",
   "metadata": {},
   "source": [
    "# TimesFM 機能（公開API）イントロスペクト\n",
    "\n",
    "このノートは `timesfm` パッケージの関数・クラス・メソッドを機械的に列挙し、`pandas.DataFrame` に格納します。  \n",
    "副作用（import時に重い依存関係がロードされる等）があり得るため、必要なら「サブモジュール走査」をオフにします。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2af0a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesfm module: timesfm\n",
      "timesfm file  : c:\\Users\\hashimoto.ryohei\\miniconda3\\envs\\kaiseki\\Lib\\site-packages\\timesfm\\__init__.py\n",
      "python        : 3.11.13 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 13:03:15) [MSC v.1929 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "ROOT = Path(r\"C:\\moinfo\")\n",
    "SRC_DIR = ROOT / \"libs\" / \"timesfm\" / \"02_src\"\n",
    "\n",
    "# ローカルモジュール（moinfo_timesfm_ext）を import できるようにする\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# timesfm がインストールされているか確認（失敗したら pip/conda 側の問題）\n",
    "import importlib\n",
    "timesfm = importlib.import_module(\"timesfm\")\n",
    "\n",
    "print(\"timesfm module:\", timesfm.__name__)\n",
    "print(\"timesfm file  :\", getattr(timesfm, \"__file__\", \"n/a\"))\n",
    "print(\"python        :\", sys.version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068df67f",
   "metadata": {},
   "source": [
    "## API 解析（関数・クラス・メソッド一覧）→ DataFrame 化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc59c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "ROOT = Path(r\"C:\\moinfo\")\n",
    "PKG_ROOT = ROOT / \"libs\" / \"timesfm\" / \"02_src\" / \"moinfo_timesfm_ext\"  # ★ここが重要\n",
    "\n",
    "if str(PKG_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PKG_ROOT))\n",
    "\n",
    "# 動作確認\n",
    "import moinfo_timesfm_ext\n",
    "print(\"moinfo_timesfm_ext:\", moinfo_timesfm_ext.__file__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84931730",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moinfo_timesfm_ext.moinfo_timesfm_ext.introspect_timesfm import collect_public_api, save_api_df\n",
    "\n",
    "df = collect_public_api(\n",
    "    package_name=\"timesfm\",\n",
    "    include_submodules=True,\n",
    "    include_private=False,\n",
    "    include_class_members=True,\n",
    "    max_doc_chars=250,\n",
    ")\n",
    "\n",
    "df.to_csv(r\"C:\\moinfo\\libs\\timesfm\\04_outputs\\tables\\timesfm_public_api_from_ext.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaac560",
   "metadata": {},
   "source": [
    "## TimesFM 公開APIの概要分析・分類・可視化\n",
    "\n",
    "- 種別（class/function/method/attribute/import_error）の分布\n",
    "- モジュール階層ごとの規模（どこに機能が集中しているか）\n",
    "- キーワードベースの機能分類（forecast / config / torch / util など）\n",
    "- ヒートマップで「種別×分類」の密度を見る\n",
    "- （任意）docstring を TF-IDF + k-means でクラスタリングして特徴語を抽出\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142a3aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 任意：doc_summary を TF-IDF + k-means でクラスタリング\n",
    "# 目的：ルール分類では拾いきれない「自然な機能グループ」を見つける\n",
    "\n",
    "try:\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.cluster import KMeans\n",
    "\n",
    "    texts = df2[\"doc_summary\"].fillna(\"\").astype(str)\n",
    "    # 空文は落とす（クラスタリングのノイズになる）\n",
    "    mask = texts.str.len() > 20\n",
    "    texts2 = texts[mask]\n",
    "    df_text = df2.loc[mask].copy()\n",
    "\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        lowercase=True,\n",
    "        stop_words=\"english\",\n",
    "        max_features=5000,\n",
    "        ngram_range=(1, 2),\n",
    "    )\n",
    "    X = vectorizer.fit_transform(texts2)\n",
    "\n",
    "    k = 8  # ざっくり。増やすと細かく割れる\n",
    "    km = KMeans(n_clusters=k, n_init=10, random_state=0)\n",
    "    labels = km.fit_predict(X)\n",
    "    df_text[\"cluster\"] = labels\n",
    "\n",
    "    # クラスタごとの代表語（上位）\n",
    "    terms = np.array(vectorizer.get_feature_names_out())\n",
    "    top_words = []\n",
    "    for i in range(k):\n",
    "        center = km.cluster_centers_[i]\n",
    "        top_idx = center.argsort()[::-1][:12]\n",
    "        top_words.append(\", \".join(terms[top_idx]))\n",
    "\n",
    "    cluster_sizes = df_text[\"cluster\"].value_counts().sort_index()\n",
    "    summary = pd.DataFrame({\n",
    "        \"cluster\": range(k),\n",
    "        \"size\": [cluster_sizes.get(i, 0) for i in range(k)],\n",
    "        \"top_terms\": top_words,\n",
    "    }).sort_values(\"size\", ascending=False)\n",
    "\n",
    "    display(summary)\n",
    "\n",
    "    # 可視化：クラスタサイズ\n",
    "    plt.figure()\n",
    "    plt.bar(summary[\"cluster\"].astype(str), summary[\"size\"])\n",
    "    plt.title(\"Docstring clusters (k-means) sizes\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT_FIG / \"timesfm_api_doc_clusters_sizes.png\", dpi=200)\n",
    "    plt.show()\n",
    "\n",
    "    # 保存\n",
    "    summary.to_csv(OUT_TBL / \"timesfm_api_doc_clusters_summary.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    df_text.to_csv(OUT_TBL / \"timesfm_api_with_doc_clusters.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(\"Saved cluster outputs to:\", OUT_TBL)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Clustering skipped (scikit-learn not available or other error):\", repr(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c441cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# 1) kind 分布（class/method/function...）\n",
    "kind_counts = df2[\"kind\"].value_counts().reset_index()\n",
    "kind_counts.columns = [\"kind\", \"count\"]\n",
    "\n",
    "fig = px.bar(kind_counts, x=\"kind\", y=\"count\", title=\"TimesFM API: kind distribution\")\n",
    "fig.update_layout(xaxis_tickangle=-45)\n",
    "fig.show()\n",
    "fig.write_html(str(OUT_FIG / \"timesfm_kind_distribution.html\"))\n",
    "\n",
    "# 2) category 分布（ルール分類）\n",
    "cat_counts = df2[\"category\"].value_counts().reset_index()\n",
    "cat_counts.columns = [\"category\", \"count\"]\n",
    "\n",
    "fig = px.bar(cat_counts, x=\"category\", y=\"count\", title=\"TimesFM API: category distribution (rule-based)\")\n",
    "fig.update_layout(xaxis_tickangle=-45)\n",
    "fig.show()\n",
    "fig.write_html(str(OUT_FIG / \"timesfm_category_distribution.html\"))\n",
    "\n",
    "# 3) 上位モジュール（どこに機能が集中してる？）\n",
    "top_modules = (\n",
    "    df2.groupby(\"module_l2\")\n",
    "       .size()\n",
    "       .sort_values(ascending=False)\n",
    "       .head(25)\n",
    "       .reset_index(name=\"count\")\n",
    ")\n",
    "\n",
    "fig = px.bar(top_modules[::-1], x=\"count\", y=\"module_l2\", orientation=\"h\",\n",
    "             title=\"Top modules (level2) by API surface\")\n",
    "fig.show()\n",
    "fig.write_html(str(OUT_FIG / \"timesfm_top_modules_l2.html\"))\n",
    "\n",
    "# 4) ヒートマップ：category × kind\n",
    "pivot = df2.pivot_table(index=\"category\", columns=\"kind\", values=\"qualname\", aggfunc=\"count\", fill_value=0)\n",
    "pivot = pivot.loc[pivot.sum(axis=1).sort_values(ascending=False).index, :]  # 行を多い順\n",
    "\n",
    "fig = px.imshow(\n",
    "    pivot,\n",
    "    title=\"Heatmap: category × kind (counts)\",\n",
    "    labels=dict(x=\"kind\", y=\"category\", color=\"count\"),\n",
    "    aspect=\"auto\",\n",
    ")\n",
    "fig.show()\n",
    "fig.write_html(str(OUT_FIG / \"timesfm_heatmap_category_kind.html\"))\n",
    "\n",
    "print(\"Saved HTML figures to:\", OUT_FIG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0cc567",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "ROOT = Path(r\"C:\\moinfo\")\n",
    "OUT_TBL = ROOT / \"libs\" / \"timesfm\" / \"04_outputs\" / \"tables\"\n",
    "OUT_TBL.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- どのDFに \"cluster\" が入っているか吸収（df_text があれば優先）---\n",
    "df_clustered = None\n",
    "if \"df_text\" in globals() and isinstance(df_text, pd.DataFrame) and \"cluster\" in df_text.columns:\n",
    "    df_clustered = df_text.copy()\n",
    "elif \"df2\" in globals() and isinstance(df2, pd.DataFrame) and \"cluster\" in df2.columns:\n",
    "    df_clustered = df2.copy()\n",
    "elif \"df\" in globals() and isinstance(df, pd.DataFrame) and \"cluster\" in df.columns:\n",
    "    df_clustered = df.copy()\n",
    "\n",
    "if df_clustered is None:\n",
    "    raise RuntimeError(\"cluster 列が見つかりません。先に TF-IDF+kmeans のセルを実行して df_text を作ってください。\")\n",
    "\n",
    "# --- summary（cluster/size/top_terms）がある前提 ---\n",
    "if \"summary\" not in globals():\n",
    "    raise RuntimeError(\"summary(DataFrame: cluster/size/top_terms) が見つかりません。先にクラスタ要約を作ってください。\")\n",
    "\n",
    "# --- top_terms からテーマ推定（“テーブルから取得”のため、top_terms依存で判定）---\n",
    "def infer_theme(top_terms: str) -> tuple[str, str]:\n",
    "    t = (top_terms or \"\").lower()\n",
    "\n",
    "    if re.search(r\"\\bcompile\\b|share_mem|torch\", t):\n",
    "        return (\"Torch統合・高速化\", \"モデルのコンパイル(compile)やTorch実行系（高速デコード等）\")\n",
    "    if re.search(r\"state_dict|hook\", t):\n",
    "        return (\"状態管理・フック\", \"重み(state_dict)の保存/読込、Hook(フック)で挙動を差し込む\")\n",
    "    if re.search(r\"yields|iterator|return iterator\", t):\n",
    "        return (\"探索・列挙\", \"モジュール/サブモジュール/パラメータをイテレータで列挙する\")\n",
    "    if re.search(r\"parameters|buffers\", t):\n",
    "        return (\"パラメータ・バッファ\", \"parameters/buffers の取得・操作（学習/推論の材料）\")\n",
    "    if re.search(r\"modifies|module place|in place\", t):\n",
    "        return (\"in-place変換\", \"dtype/device 変更など“その場で”モジュールを書き換える操作\")\n",
    "    if re.search(r\"target exists|exists|error\", t):\n",
    "        return (\"検証・例外\", \"存在チェックやエラー処理（主にユーティリティ/検証）\")\n",
    "    if re.search(r\"evaluation mode|evaluation|mode\", t):\n",
    "        return (\"モード切替\", \"train/eval など推論モード/学習モードの切り替え\")\n",
    "    if re.search(r\"child|accessed|using given|get_\", t):\n",
    "        return (\"階層アクセス\", \"子モジュール/特定パラメータへ名前でアクセス（get_submodule等）\")\n",
    "\n",
    "    return (\"その他\", \"雑多な補助機能\")\n",
    "\n",
    "# --- 各クラスタの代表API（qualname）を抽出 ---\n",
    "# 代表の選び方：doc_summaryが長い＝説明がありそう、さらに timesfm/forecast/compile を優先\n",
    "def pick_examples(g: pd.DataFrame, k: int = 6) -> pd.DataFrame:\n",
    "    g = g.copy()\n",
    "    txt = (g[\"qualname\"].fillna(\"\") + \" \" + g[\"doc_summary\"].fillna(\"\")).str.lower()\n",
    "    score = (\n",
    "        g[\"doc_summary\"].fillna(\"\").str.len()\n",
    "        + txt.str.contains(\"forecast\").astype(int) * 200\n",
    "        + txt.str.contains(\"compile\").astype(int) * 150\n",
    "        + txt.str.contains(\"timesfm\").astype(int) * 80\n",
    "    )\n",
    "    g[\"_score\"] = score\n",
    "    return g.sort_values(\"_score\", ascending=False).head(k)\n",
    "\n",
    "examples = (\n",
    "    df_clustered.groupby(\"cluster\", group_keys=False)\n",
    "    .apply(pick_examples, k=6)\n",
    "    .loc[:, [\"cluster\", \"qualname\", \"kind\", \"module\", \"doc_summary\"]]\n",
    ")\n",
    "\n",
    "# --- 概要テーブル作成 ---\n",
    "ov = summary.copy()\n",
    "ov[\"theme\"], ov[\"what_you_can_do\"] = zip(*ov[\"top_terms\"].apply(infer_theme))\n",
    "\n",
    "# 代表APIを文字列にまとめる\n",
    "ex_str = (\n",
    "    examples.assign(ex=lambda d: d[\"qualname\"] + \" [\" + d[\"kind\"].astype(str) + \"]\")\n",
    "    .groupby(\"cluster\")[\"ex\"]\n",
    "    .apply(lambda s: \"\\n\".join(s.tolist()))\n",
    "    .reset_index()\n",
    "    .rename(columns={\"ex\": \"representative_api\"})\n",
    ")\n",
    "\n",
    "overview = ov.merge(ex_str, on=\"cluster\", how=\"left\").sort_values(\"size\", ascending=False)\n",
    "\n",
    "display(overview)\n",
    "\n",
    "# 保存（フルパス）\n",
    "out_csv = OUT_TBL / \"timesfm_cluster_overview.csv\"\n",
    "overview.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"saved:\", out_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238d99b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "ROOT = Path(r\"C:\\moinfo\")\n",
    "OUT_FIG = ROOT / \"libs\" / \"timesfm\" / \"04_outputs\" / \"figs\"\n",
    "OUT_FIG.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Mermaidで壊れやすい文字を軽くエスケープ\n",
    "def esc(s: str) -> str:\n",
    "    s = (s or \"\")\n",
    "    s = s.replace('\"', \"'\")\n",
    "    s = re.sub(r\"[\\r\\n]+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "lines = []\n",
    "lines.append(\"graph TD\")\n",
    "lines.append('  R[\"timesfm API\"]')\n",
    "\n",
    "# overview がある前提\n",
    "for _, r in overview.iterrows():\n",
    "    c = int(r[\"cluster\"])\n",
    "    theme = esc(r[\"theme\"])\n",
    "    size = int(r[\"size\"])\n",
    "    node_c = f\"C{c}\"\n",
    "    lines.append(f'  R --> {node_c}[\"C{c}: {theme}\\\\n(size={size})\"]')\n",
    "\n",
    "    reps = (r.get(\"representative_api\") or \"\").splitlines()\n",
    "    reps = reps[:6]  # 多すぎると見づらいので上限\n",
    "    for i, api in enumerate(reps, start=1):\n",
    "        api = esc(api)\n",
    "        node_a = f\"{node_c}_{i}\"\n",
    "        lines.append(f'  {node_c} --> {node_a}[\"{api}\"]')\n",
    "\n",
    "mermaid_code = \"\\n\".join(lines)\n",
    "\n",
    "# 1) Notebook上に表示（Mermaid対応ビューアがある環境ならそのまま描画される）\n",
    "display(Markdown(\"```mermaid\\n\" + mermaid_code + \"\\n```\"))\n",
    "\n",
    "# 2) ファイル保存（フルパス）\n",
    "out_mmd = OUT_FIG / \"timesfm_api_clusters.mmd\"\n",
    "out_md  = OUT_FIG / \"timesfm_api_clusters_mermaid.md\"\n",
    "out_mmd.write_text(mermaid_code, encoding=\"utf-8\")\n",
    "out_md.write_text(\"```mermaid\\n\" + mermaid_code + \"\\n```\", encoding=\"utf-8\")\n",
    "\n",
    "print(\"saved:\", out_mmd)\n",
    "print(\"saved:\", out_md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c951fad",
   "metadata": {},
   "source": [
    "# 1. サンプルデータセットの作成\n",
    "TimesFMの機能検証を行うため、明確な**トレンド（傾向）**と**季節性（周期性）**を持つ人工的な時系列データを生成します。\n",
    "\n",
    "モデルのコンテキスト長（入力）と予測ホライゾン（出力）をカバーできる十分な長さのデータを、以下の仕様で作成します。\n",
    "\n",
    "* **データ構造**: Pandas DataFrame (ロング形式)\n",
    "    * `date`: 日時インデックス\n",
    "    * `unique_id`: 系列を識別するID（多変量/複数系列対応のため）\n",
    "    * `value`: 観測値\n",
    "* **データ内容**:\n",
    "    * `series_01`: サイン波 + 上昇トレンド（基本的な周期的変動）\n",
    "    * `series_02`: コサイン波 + 緩やかなトレンド（位相の異なる変動）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dda160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_synthetic_series(length=1000, start_date=\"2024-01-01\", freq=\"h\"):\n",
    "    \"\"\"\n",
    "    トレンド、季節性、ノイズを含む人工的な時系列データを生成する関数\n",
    "    \"\"\"\n",
    "    # 日時インデックスの作成\n",
    "    dates = pd.date_range(start=start_date, periods=length, freq=freq)\n",
    "    x = np.linspace(0, 8 * np.pi, length)\n",
    "\n",
    "    # 系列1: サイン波 + 強いトレンド\n",
    "    y1 = np.sin(x) + np.linspace(0, 5, length) + np.random.normal(0, 0.1, length)\n",
    "    df1 = pd.DataFrame({\n",
    "        \"unique_id\": \"series_01\",\n",
    "        \"date\": dates,\n",
    "        \"value\": y1\n",
    "    })\n",
    "\n",
    "    # 系列2: コサイン波 + 弱いトレンド + 異なるノイズ\n",
    "    y2 = np.cos(x) + np.linspace(0, 2, length) + np.random.normal(0, 0.15, length)\n",
    "    df2 = pd.DataFrame({\n",
    "        \"unique_id\": \"series_02\",\n",
    "        \"date\": dates,\n",
    "        \"value\": y2\n",
    "    })\n",
    "\n",
    "    # 結合してロング形式へ\n",
    "    df_combined = pd.concat([df1, df2], ignore_index=True)\n",
    "    return df_combined\n",
    "\n",
    "# データ生成（TimesFMのデフォルトコンテキスト長 512 + 予測 128 をカバーできるサイズ）\n",
    "DATA_LENGTH = 1024\n",
    "df_sample = generate_synthetic_series(length=DATA_LENGTH)\n",
    "\n",
    "# データの確認\n",
    "print(f\"Data Shape: {df_sample.shape}\")\n",
    "display(df_sample.head())\n",
    "\n",
    "# 可視化\n",
    "plt.figure(figsize=(12, 5))\n",
    "for uid, group in df_sample.groupby(\"unique_id\"):\n",
    "    plt.plot(group[\"date\"], group[\"value\"], label=uid, alpha=0.8)\n",
    "    \n",
    "plt.title(\"Synthetic Time Series Data for TimesFM Testing\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49212629",
   "metadata": {},
   "source": [
    "# 2. モデルのロード (修正版)\n",
    "\n",
    "`timesfm` ライブラリの仕様に合わせて、クラス名を `TimesFm` (mは小文字) に修正し、パラメータを `TimesFmHparams` 経由で設定します。\n",
    "\n",
    "**主な修正点:**\n",
    "1. クラス名: `TimesFM` -> `TimesFm`\n",
    "2. パラメータ指定: `hparams` 引数に `TimesFmHparams` オブジェクトを渡す形式に変更\n",
    "3. チェックポイント指定: `checkpoint` 引数に `TimesFmCheckpoint` オブジェクトを渡す形式に変更"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c36d4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timesfm\n",
    "import numpy as np\n",
    "\n",
    "# 1. 予測設定 (ForecastConfig) の定義\n",
    "# 実際の入力長や予測したい期間に合わせて設定します\n",
    "forecast_config = timesfm.configs.ForecastConfig(\n",
    "    max_context=512,       # 入力系列の最大長\n",
    "    max_horizon=128,       # 予測期間の最大長\n",
    "    per_core_batch_size=1  # 1回の推論で処理するバッチサイズ\n",
    ")\n",
    "\n",
    "# 2. モデルのコンパイル (必須手順)\n",
    "print(\"Compiling model (this may take a few seconds)...\")\n",
    "tfm.compile(forecast_config)\n",
    "print(\"Model compiled successfully.\")\n",
    "\n",
    "# 3. 動作確認: ダミーデータによる予測\n",
    "# サイン波のダミーデータを作成 (長さ512)\n",
    "dummy_input = [np.sin(np.linspace(0, 20, 512))]\n",
    "\n",
    "# 予測実行\n",
    "# 戻り値: (点予測の結果, その他の情報) のタプル\n",
    "forecast_result = tfm.forecast(horizon=128, inputs=dummy_input)\n",
    "\n",
    "# 結果の確認\n",
    "# forecast_result[0] が予測値のメイン配列です\n",
    "print(f\"Forecast result type: {type(forecast_result)}\")\n",
    "print(f\"Point forecast shape: {forecast_result[0].shape}\")  # (Batch, Horizon) -> (1, 128) expected\n",
    "\n",
    "# 簡易プロット (オプション)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.arange(512), dummy_input[0], label=\"History\")\n",
    "plt.plot(np.arange(512, 512+128), forecast_result[0][0], label=\"Forecast\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0eaf41",
   "metadata": {},
   "source": [
    "# TimesFM.load_checkpoint 引数・データ型一覧\n",
    "\n",
    "`inspect` モジュールを使用して、現在インストールされている `timesfm` ライブラリから `load_checkpoint` メソッドの引数定義を動的に抽出し、データ型とデフォルト値を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a33bce3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Docstring Summary ---\n",
      "Loads a TimesFM model from a checkpoint.\n",
      "-------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_990fc th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_990fc_row0_col0, #T_990fc_row0_col1, #T_990fc_row0_col2, #T_990fc_row0_col3, #T_990fc_row1_col0, #T_990fc_row1_col1, #T_990fc_row1_col2, #T_990fc_row1_col3 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_990fc\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_990fc_level0_col0\" class=\"col_heading level0 col0\" >引数名 (Name)</th>\n",
       "      <th id=\"T_990fc_level0_col1\" class=\"col_heading level0 col1\" >データ型 (Type Hint)</th>\n",
       "      <th id=\"T_990fc_level0_col2\" class=\"col_heading level0 col2\" >デフォルト値 (Default)</th>\n",
       "      <th id=\"T_990fc_level0_col3\" class=\"col_heading level0 col3\" >引数の種類 (Kind)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_990fc_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_990fc_row0_col0\" class=\"data row0 col0\" >self</td>\n",
       "      <td id=\"T_990fc_row0_col1\" class=\"data row0 col1\" >Unspecified</td>\n",
       "      <td id=\"T_990fc_row0_col2\" class=\"data row0 col2\" >**Required**</td>\n",
       "      <td id=\"T_990fc_row0_col3\" class=\"data row0 col3\" >POSITIONAL_OR_KEYWORD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_990fc_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_990fc_row1_col0\" class=\"data row1 col0\" >path</td>\n",
       "      <td id=\"T_990fc_row1_col1\" class=\"data row1 col1\" >str</td>\n",
       "      <td id=\"T_990fc_row1_col2\" class=\"data row1 col2\" >**Required**</td>\n",
       "      <td id=\"T_990fc_row1_col3\" class=\"data row1 col3\" >POSITIONAL_OR_KEYWORD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2b1f8438190>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "import pandas as pd\n",
    "import timesfm\n",
    "\n",
    "def get_method_arguments_table(method):\n",
    "    \"\"\"\n",
    "    指定されたメソッドの引数、型ヒント、デフォルト値を抽出し、DataFrameで返します。\n",
    "    \"\"\"\n",
    "    if not callable(method):\n",
    "        raise ValueError(f\"{method} is not callable.\")\n",
    "\n",
    "    # シグネチャ（引数定義）を取得\n",
    "    sig = inspect.signature(method)\n",
    "    \n",
    "    # Docstringの取得（概要表示用）\n",
    "    doc = inspect.getdoc(method)\n",
    "    print(f\"--- Docstring Summary ---\\n{doc}\\n-------------------------\\n\")\n",
    "\n",
    "    arg_list = []\n",
    "    for name, param in sig.parameters.items():\n",
    "        # 型ヒントの整形 (typing.Unionsなどを読みやすくする処理は適宜追加可能)\n",
    "        type_hint = param.annotation\n",
    "        if type_hint == inspect.Parameter.empty:\n",
    "            type_hint = \"Unspecified\"\n",
    "        else:\n",
    "            # クラス型などの表記を少しきれいにする\n",
    "            type_hint = str(type_hint).replace(\"<class '\", \"\").replace(\"'>\", \"\").replace(\"typing.\", \"\")\n",
    "\n",
    "        # デフォルト値の確認\n",
    "        default_val = param.default\n",
    "        if default_val == inspect.Parameter.empty:\n",
    "            default_val = \"**Required**\"\n",
    "        \n",
    "        arg_list.append({\n",
    "            \"引数名 (Name)\": name,\n",
    "            \"データ型 (Type Hint)\": type_hint,\n",
    "            \"デフォルト値 (Default)\": str(default_val),\n",
    "            \"引数の種類 (Kind)\": str(param.kind)\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(arg_list)\n",
    "\n",
    "# 1. クラスからメソッドを取得\n",
    "# 注: 提供されたログに基づき、標準的な TimesFM クラス、または内部実装クラスをターゲットにします\n",
    "try:\n",
    "    target_method = timesfm.TimesFM.load_checkpoint\n",
    "except AttributeError:\n",
    "    # 万が一 TimesFM がラッパー関数等の場合、具体的なクラス（ログに基づく）を試行\n",
    "    # ※環境に合わせて適宜調整してください\n",
    "    try:\n",
    "        from timesfm import TimesFM_2p5_200M_torch\n",
    "        target_method = TimesFM_2p5_200M_torch.load_checkpoint\n",
    "    except ImportError:\n",
    "        print(\"Warning: Could not import specific class directly. Using timesfm.TimesFM interface.\")\n",
    "        target_method = timesfm.TimesFM.load_checkpoint\n",
    "\n",
    "# 2. テーブルの生成と表示\n",
    "df_args = get_method_arguments_table(target_method)\n",
    "\n",
    "# 見やすく表示 (Stylerを使用)\n",
    "df_args.style.set_properties(**{'text-align': 'left'}).set_table_styles([\n",
    "    dict(selector='th', props=[('text-align', 'left')])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "387a9536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TimesFM Meta Execution Table ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_0a795 th {\n",
       "  text-align: left;\n",
       "  background-color: #f0f0f0;\n",
       "}\n",
       "#T_0a795 td {\n",
       "  padding: 8px;\n",
       "}\n",
       "#T_0a795_row0_col0, #T_0a795_row0_col1, #T_0a795_row0_col2, #T_0a795_row0_col4, #T_0a795_row0_col5, #T_0a795_row0_col6, #T_0a795_row0_col7, #T_0a795_row1_col0, #T_0a795_row1_col1, #T_0a795_row1_col2, #T_0a795_row1_col3, #T_0a795_row1_col4, #T_0a795_row1_col5, #T_0a795_row1_col6, #T_0a795_row1_col7, #T_0a795_row2_col0, #T_0a795_row2_col1, #T_0a795_row2_col2, #T_0a795_row2_col3, #T_0a795_row2_col4, #T_0a795_row2_col5, #T_0a795_row2_col6, #T_0a795_row2_col7, #T_0a795_row3_col0, #T_0a795_row3_col1, #T_0a795_row3_col2, #T_0a795_row3_col3, #T_0a795_row3_col4, #T_0a795_row3_col5, #T_0a795_row3_col6, #T_0a795_row3_col7, #T_0a795_row4_col0, #T_0a795_row4_col1, #T_0a795_row4_col2, #T_0a795_row4_col3, #T_0a795_row4_col4, #T_0a795_row4_col5, #T_0a795_row4_col6, #T_0a795_row4_col7, #T_0a795_row5_col0, #T_0a795_row5_col1, #T_0a795_row5_col2, #T_0a795_row5_col3, #T_0a795_row5_col4, #T_0a795_row5_col5, #T_0a795_row5_col6, #T_0a795_row5_col7, #T_0a795_row6_col0, #T_0a795_row6_col1, #T_0a795_row6_col2, #T_0a795_row6_col3, #T_0a795_row6_col4, #T_0a795_row6_col5, #T_0a795_row6_col6, #T_0a795_row6_col7, #T_0a795_row7_col0, #T_0a795_row7_col1, #T_0a795_row7_col2, #T_0a795_row7_col3, #T_0a795_row7_col4, #T_0a795_row7_col5, #T_0a795_row7_col6, #T_0a795_row7_col7, #T_0a795_row8_col0, #T_0a795_row8_col1, #T_0a795_row8_col2, #T_0a795_row8_col4, #T_0a795_row8_col5, #T_0a795_row8_col6, #T_0a795_row8_col7, #T_0a795_row9_col0, #T_0a795_row9_col1, #T_0a795_row9_col2, #T_0a795_row9_col4, #T_0a795_row9_col5, #T_0a795_row9_col6, #T_0a795_row9_col7, #T_0a795_row10_col0, #T_0a795_row10_col1, #T_0a795_row10_col2, #T_0a795_row10_col3, #T_0a795_row10_col4, #T_0a795_row10_col5, #T_0a795_row10_col7, #T_0a795_row11_col0, #T_0a795_row11_col1, #T_0a795_row11_col2, #T_0a795_row11_col3, #T_0a795_row11_col4, #T_0a795_row11_col5, #T_0a795_row11_col7, #T_0a795_row12_col0, #T_0a795_row12_col1, #T_0a795_row12_col2, #T_0a795_row12_col3, #T_0a795_row12_col4, #T_0a795_row12_col5, #T_0a795_row12_col6, #T_0a795_row12_col7, #T_0a795_row13_col0, #T_0a795_row13_col1, #T_0a795_row13_col2, #T_0a795_row13_col3, #T_0a795_row13_col4, #T_0a795_row13_col5, #T_0a795_row13_col6, #T_0a795_row13_col7, #T_0a795_row14_col0, #T_0a795_row14_col1, #T_0a795_row14_col2, #T_0a795_row14_col3, #T_0a795_row14_col4, #T_0a795_row14_col5, #T_0a795_row14_col6, #T_0a795_row14_col7, #T_0a795_row15_col0, #T_0a795_row15_col1, #T_0a795_row15_col2, #T_0a795_row15_col3, #T_0a795_row15_col4, #T_0a795_row15_col5, #T_0a795_row15_col6, #T_0a795_row15_col7, #T_0a795_row16_col0, #T_0a795_row16_col1, #T_0a795_row16_col2, #T_0a795_row16_col3, #T_0a795_row16_col4, #T_0a795_row16_col5, #T_0a795_row16_col6, #T_0a795_row16_col7, #T_0a795_row17_col0, #T_0a795_row17_col1, #T_0a795_row17_col2, #T_0a795_row17_col3, #T_0a795_row17_col4, #T_0a795_row17_col5, #T_0a795_row17_col6, #T_0a795_row17_col7, #T_0a795_row18_col0, #T_0a795_row18_col1, #T_0a795_row18_col2, #T_0a795_row18_col3, #T_0a795_row18_col4, #T_0a795_row18_col5, #T_0a795_row18_col6, #T_0a795_row18_col7, #T_0a795_row19_col0, #T_0a795_row19_col1, #T_0a795_row19_col2, #T_0a795_row19_col3, #T_0a795_row19_col4, #T_0a795_row19_col5, #T_0a795_row19_col6, #T_0a795_row19_col7, #T_0a795_row20_col0, #T_0a795_row20_col1, #T_0a795_row20_col2, #T_0a795_row20_col4, #T_0a795_row20_col5, #T_0a795_row20_col7, #T_0a795_row21_col0, #T_0a795_row21_col1, #T_0a795_row21_col2, #T_0a795_row21_col4, #T_0a795_row21_col5, #T_0a795_row21_col7, #T_0a795_row22_col0, #T_0a795_row22_col1, #T_0a795_row22_col2, #T_0a795_row22_col4, #T_0a795_row22_col5, #T_0a795_row22_col7, #T_0a795_row23_col0, #T_0a795_row23_col1, #T_0a795_row23_col2, #T_0a795_row23_col4, #T_0a795_row23_col5, #T_0a795_row23_col6, #T_0a795_row23_col7, #T_0a795_row24_col0, #T_0a795_row24_col1, #T_0a795_row24_col2, #T_0a795_row24_col3, #T_0a795_row24_col4, #T_0a795_row24_col5, #T_0a795_row24_col6, #T_0a795_row24_col7, #T_0a795_row25_col0, #T_0a795_row25_col1, #T_0a795_row25_col2, #T_0a795_row25_col3, #T_0a795_row25_col4, #T_0a795_row25_col5, #T_0a795_row25_col6, #T_0a795_row25_col7, #T_0a795_row26_col0, #T_0a795_row26_col1, #T_0a795_row26_col2, #T_0a795_row26_col3, #T_0a795_row26_col4, #T_0a795_row26_col5, #T_0a795_row26_col6, #T_0a795_row26_col7, #T_0a795_row27_col0, #T_0a795_row27_col1, #T_0a795_row27_col2, #T_0a795_row27_col3, #T_0a795_row27_col4, #T_0a795_row27_col5, #T_0a795_row27_col6, #T_0a795_row27_col7, #T_0a795_row28_col0, #T_0a795_row28_col1, #T_0a795_row28_col2, #T_0a795_row28_col4, #T_0a795_row28_col5, #T_0a795_row28_col6, #T_0a795_row28_col7 {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_0a795_row0_col3, #T_0a795_row8_col3, #T_0a795_row9_col3, #T_0a795_row20_col3, #T_0a795_row21_col3, #T_0a795_row22_col3, #T_0a795_row23_col3, #T_0a795_row28_col3 {\n",
       "  background-color: #ffcccc;\n",
       "  text-align: left;\n",
       "}\n",
       "#T_0a795_row10_col6, #T_0a795_row11_col6, #T_0a795_row20_col6, #T_0a795_row21_col6, #T_0a795_row22_col6 {\n",
       "  font-weight: bold;\n",
       "  color: #005a9c;\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_0a795\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_0a795_level0_col0\" class=\"col_heading level0 col0\" >Category</th>\n",
       "      <th id=\"T_0a795_level0_col1\" class=\"col_heading level0 col1\" >Function/Class</th>\n",
       "      <th id=\"T_0a795_level0_col2\" class=\"col_heading level0 col2\" >Argument</th>\n",
       "      <th id=\"T_0a795_level0_col3\" class=\"col_heading level0 col3\" >Required</th>\n",
       "      <th id=\"T_0a795_level0_col4\" class=\"col_heading level0 col4\" >Type Hint</th>\n",
       "      <th id=\"T_0a795_level0_col5\" class=\"col_heading level0 col5\" >Default</th>\n",
       "      <th id=\"T_0a795_level0_col6\" class=\"col_heading level0 col6\" >✅ Verified Specs / Constraints</th>\n",
       "      <th id=\"T_0a795_level0_col7\" class=\"col_heading level0 col7\" >Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_0a795_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_0a795_row0_col0\" class=\"data row0 col0\" >2. Loading</td>\n",
       "      <td id=\"T_0a795_row0_col1\" class=\"data row0 col1\" >from_pretrained</td>\n",
       "      <td id=\"T_0a795_row0_col2\" class=\"data row0 col2\" >pretrained_model_name_or_path</td>\n",
       "      <td id=\"T_0a795_row0_col3\" class=\"data row0 col3\" >✅</td>\n",
       "      <td id=\"T_0a795_row0_col4\" class=\"data row0 col4\" >Union[str, pathlib.Path]</td>\n",
       "      <td id=\"T_0a795_row0_col5\" class=\"data row0 col5\" >**Required**</td>\n",
       "      <td id=\"T_0a795_row0_col6\" class=\"data row0 col6\" ></td>\n",
       "      <td id=\"T_0a795_row0_col7\" class=\"data row0 col7\" > - Either the `model_id` (string) of a model hosted on the Hub, e.g. `bigscience/bloom`. - Or a path...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0a795_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_0a795_row1_col0\" class=\"data row1 col0\" >2. Loading</td>\n",
       "      <td id=\"T_0a795_row1_col1\" class=\"data row1 col1\" >from_pretrained</td>\n",
       "      <td id=\"T_0a795_row1_col2\" class=\"data row1 col2\" >force_download</td>\n",
       "      <td id=\"T_0a795_row1_col3\" class=\"data row1 col3\" ></td>\n",
       "      <td id=\"T_0a795_row1_col4\" class=\"data row1 col4\" >bool</td>\n",
       "      <td id=\"T_0a795_row1_col5\" class=\"data row1 col5\" >False</td>\n",
       "      <td id=\"T_0a795_row1_col6\" class=\"data row1 col6\" ></td>\n",
       "      <td id=\"T_0a795_row1_col7\" class=\"data row1 col7\" > Whether to force (re-)downloading the model weights and configuration files from the Hub, overridin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0a795_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_0a795_row2_col0\" class=\"data row2 col0\" >2. Loading</td>\n",
       "      <td id=\"T_0a795_row2_col1\" class=\"data row2 col1\" >from_pretrained</td>\n",
       "      <td id=\"T_0a795_row2_col2\" class=\"data row2 col2\" >resume_download</td>\n",
       "      <td id=\"T_0a795_row2_col3\" class=\"data row2 col3\" ></td>\n",
       "      <td id=\"T_0a795_row2_col4\" class=\"data row2 col4\" >Optional[bool]</td>\n",
       "      <td id=\"T_0a795_row2_col5\" class=\"data row2 col5\" >None</td>\n",
       "      <td id=\"T_0a795_row2_col6\" class=\"data row2 col6\" ></td>\n",
       "      <td id=\"T_0a795_row2_col7\" class=\"data row2 col7\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0a795_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_0a795_row3_col0\" class=\"data row3 col0\" >2. Loading</td>\n",
       "      <td id=\"T_0a795_row3_col1\" class=\"data row3 col1\" >from_pretrained</td>\n",
       "      <td id=\"T_0a795_row3_col2\" class=\"data row3 col2\" >proxies</td>\n",
       "      <td id=\"T_0a795_row3_col3\" class=\"data row3 col3\" ></td>\n",
       "      <td id=\"T_0a795_row3_col4\" class=\"data row3 col4\" >Optional[Dict]</td>\n",
       "      <td id=\"T_0a795_row3_col5\" class=\"data row3 col5\" >None</td>\n",
       "      <td id=\"T_0a795_row3_col6\" class=\"data row3 col6\" ></td>\n",
       "      <td id=\"T_0a795_row3_col7\" class=\"data row3 col7\" > A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128', 'http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0a795_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_0a795_row4_col0\" class=\"data row4 col0\" >2. Loading</td>\n",
       "      <td id=\"T_0a795_row4_col1\" class=\"data row4 col1\" >from_pretrained</td>\n",
       "      <td id=\"T_0a795_row4_col2\" class=\"data row4 col2\" >token</td>\n",
       "      <td id=\"T_0a795_row4_col3\" class=\"data row4 col3\" ></td>\n",
       "      <td id=\"T_0a795_row4_col4\" class=\"data row4 col4\" >Union[bool, str, NoneType]</td>\n",
       "      <td id=\"T_0a795_row4_col5\" class=\"data row4 col5\" >None</td>\n",
       "      <td id=\"T_0a795_row4_col6\" class=\"data row4 col6\" ></td>\n",
       "      <td id=\"T_0a795_row4_col7\" class=\"data row4 col7\" > The token to use as HTTP bearer authorization for remote files. By default, it will use the token c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0a795_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_0a795_row5_col0\" class=\"data row5 col0\" >2. Loading</td>\n",
       "      <td id=\"T_0a795_row5_col1\" class=\"data row5 col1\" >from_pretrained</td>\n",
       "      <td id=\"T_0a795_row5_col2\" class=\"data row5 col2\" >cache_dir</td>\n",
       "      <td id=\"T_0a795_row5_col3\" class=\"data row5 col3\" ></td>\n",
       "      <td id=\"T_0a795_row5_col4\" class=\"data row5 col4\" >Union[str, pathlib.Path, NoneType]</td>\n",
       "      <td id=\"T_0a795_row5_col5\" class=\"data row5 col5\" >None</td>\n",
       "      <td id=\"T_0a795_row5_col6\" class=\"data row5 col6\" ></td>\n",
       "      <td id=\"T_0a795_row5_col7\" class=\"data row5 col7\" > Path to the folder where cached files are stored.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0a795_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_0a795_row6_col0\" class=\"data row6 col0\" >2. Loading</td>\n",
       "      <td id=\"T_0a795_row6_col1\" class=\"data row6 col1\" >from_pretrained</td>\n",
       "      <td id=\"T_0a795_row6_col2\" class=\"data row6 col2\" >local_files_only</td>\n",
       "      <td id=\"T_0a795_row6_col3\" class=\"data row6 col3\" ></td>\n",
       "      <td id=\"T_0a795_row6_col4\" class=\"data row6 col4\" >bool</td>\n",
       "      <td id=\"T_0a795_row6_col5\" class=\"data row6 col5\" >False</td>\n",
       "      <td id=\"T_0a795_row6_col6\" class=\"data row6 col6\" ></td>\n",
       "      <td id=\"T_0a795_row6_col7\" class=\"data row6 col7\" > If `True`, avoid downloading the file and return the path to the local cached file if it exists.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0a795_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_0a795_row7_col0\" class=\"data row7 col0\" >2. Loading</td>\n",
       "      <td id=\"T_0a795_row7_col1\" class=\"data row7 col1\" >from_pretrained</td>\n",
       "      <td id=\"T_0a795_row7_col2\" class=\"data row7 col2\" >revision</td>\n",
       "      <td id=\"T_0a795_row7_col3\" class=\"data row7 col3\" ></td>\n",
       "      <td id=\"T_0a795_row7_col4\" class=\"data row7 col4\" >Optional[str]</td>\n",
       "      <td id=\"T_0a795_row7_col5\" class=\"data row7 col5\" >None</td>\n",
       "      <td id=\"T_0a795_row7_col6\" class=\"data row7 col6\" ></td>\n",
       "      <td id=\"T_0a795_row7_col7\" class=\"data row7 col7\" > Revision of the model on the Hub. Can be a branch name, a git tag or any commit id. Defaults to the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0a795_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_0a795_row8_col0\" class=\"data row8 col0\" >2. Loading</td>\n",
       "      <td id=\"T_0a795_row8_col1\" class=\"data row8 col1\" >from_pretrained</td>\n",
       "      <td id=\"T_0a795_row8_col2\" class=\"data row8 col2\" >model_kwargs</td>\n",
       "      <td id=\"T_0a795_row8_col3\" class=\"data row8 col3\" >✅</td>\n",
       "      <td id=\"T_0a795_row8_col4\" class=\"data row8 col4\" >Any</td>\n",
       "      <td id=\"T_0a795_row8_col5\" class=\"data row8 col5\" >**Required**</td>\n",
       "      <td id=\"T_0a795_row8_col6\" class=\"data row8 col6\" ></td>\n",
       "      <td id=\"T_0a795_row8_col7\" class=\"data row8 col7\" > Additional kwargs to pass to the model during initialization.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0a795_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_0a795_row9_col0\" class=\"data row9 col0\" >2. Loading</td>\n",
       "      <td id=\"T_0a795_row9_col1\" class=\"data row9 col1\" >load_checkpoint</td>\n",
       "      <td id=\"T_0a795_row9_col2\" class=\"data row9 col2\" >path</td>\n",
       "      <td id=\"T_0a795_row9_col3\" class=\"data row9 col3\" >✅</td>\n",
       "      <td id=\"T_0a795_row9_col4\" class=\"data row9 col4\" >str</td>\n",
       "      <td id=\"T_0a795_row9_col5\" class=\"data row9 col5\" >**Required**</td>\n",
       "      <td id=\"T_0a795_row9_col6\" class=\"data row9 col6\" ></td>\n",
       "      <td id=\"T_0a795_row9_col7\" class=\"data row9 col7\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0a795_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_0a795_row10_col0\" class=\"data row10 col0\" >3. Config</td>\n",
       "      <td id=\"T_0a795_row10_col1\" class=\"data row10 col1\" >ForecastConfig (__init__)</td>\n",
       "      <td id=\"T_0a795_row10_col2\" class=\"data row10 col2\" >max_context</td>\n",
       "      <td id=\"T_0a795_row10_col3\" class=\"data row10 col3\" ></td>\n",
       "      <td id=\"T_0a795_row10_col4\" class=\"data row10 col4\" >int</td>\n",
       "      <td id=\"T_0a795_row10_col5\" class=\"data row10 col5\" >0</td>\n",
       "      <td id=\"T_0a795_row10_col6\" class=\"data row10 col6\" >32 ~ 512 (推奨: 512)</td>\n",
       "      <td id=\"T_0a795_row10_col7\" class=\"data row10 col7\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0a795_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_0a795_row11_col0\" class=\"data row11 col0\" >3. Config</td>\n",
       "      <td id=\"T_0a795_row11_col1\" class=\"data row11 col1\" >ForecastConfig (__init__)</td>\n",
       "      <td id=\"T_0a795_row11_col2\" class=\"data row11 col2\" >max_horizon</td>\n",
       "      <td id=\"T_0a795_row11_col3\" class=\"data row11 col3\" ></td>\n",
       "      <td id=\"T_0a795_row11_col4\" class=\"data row11 col4\" >int</td>\n",
       "      <td id=\"T_0a795_row11_col5\" class=\"data row11 col5\" >0</td>\n",
       "      <td id=\"T_0a795_row11_col6\" class=\"data row11 col6\" >1 ~ 512+ (推奨: 128)</td>\n",
       "      <td id=\"T_0a795_row11_col7\" class=\"data row11 col7\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0a795_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_0a795_row12_col0\" class=\"data row12 col0\" >3. Config</td>\n",
       "      <td id=\"T_0a795_row12_col1\" class=\"data row12 col1\" >ForecastConfig (__init__)</td>\n",
       "      <td id=\"T_0a795_row12_col2\" class=\"data row12 col2\" >normalize_inputs</td>\n",
       "      <td id=\"T_0a795_row12_col3\" class=\"data row12 col3\" ></td>\n",
       "      <td id=\"T_0a795_row12_col4\" class=\"data row12 col4\" >bool</td>\n",
       "      <td id=\"T_0a795_row12_col5\" class=\"data row12 col5\" >False</td>\n",
       "      <td id=\"T_0a795_row12_col6\" class=\"data row12 col6\" ></td>\n",
       "      <td id=\"T_0a795_row12_col7\" class=\"data row12 col7\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0a795_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_0a795_row13_col0\" class=\"data row13 col0\" >3. Config</td>\n",
       "      <td id=\"T_0a795_row13_col1\" class=\"data row13 col1\" >ForecastConfig (__init__)</td>\n",
       "      <td id=\"T_0a795_row13_col2\" class=\"data row13 col2\" >window_size</td>\n",
       "      <td id=\"T_0a795_row13_col3\" class=\"data row13 col3\" ></td>\n",
       "      <td id=\"T_0a795_row13_col4\" class=\"data row13 col4\" >int</td>\n",
       "      <td id=\"T_0a795_row13_col5\" class=\"data row13 col5\" >0</td>\n",
       "      <td id=\"T_0a795_row13_col6\" class=\"data row13 col6\" ></td>\n",
       "      <td id=\"T_0a795_row13_col7\" class=\"data row13 col7\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0a795_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_0a795_row14_col0\" class=\"data row14 col0\" >3. Config</td>\n",
       "      <td id=\"T_0a795_row14_col1\" class=\"data row14 col1\" >ForecastConfig (__init__)</td>\n",
       "      <td id=\"T_0a795_row14_col2\" class=\"data row14 col2\" >per_core_batch_size</td>\n",
       "      <td id=\"T_0a795_row14_col3\" class=\"data row14 col3\" ></td>\n",
       "      <td id=\"T_0a795_row14_col4\" class=\"data row14 col4\" >int</td>\n",
       "      <td id=\"T_0a795_row14_col5\" class=\"data row14 col5\" >1</td>\n",
       "      <td id=\"T_0a795_row14_col6\" class=\"data row14 col6\" ></td>\n",
       "      <td id=\"T_0a795_row14_col7\" class=\"data row14 col7\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0a795_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_0a795_row15_col0\" class=\"data row15 col0\" >3. Config</td>\n",
       "      <td id=\"T_0a795_row15_col1\" class=\"data row15 col1\" >ForecastConfig (__init__)</td>\n",
       "      <td id=\"T_0a795_row15_col2\" class=\"data row15 col2\" >use_continuous_quantile_head</td>\n",
       "      <td id=\"T_0a795_row15_col3\" class=\"data row15 col3\" ></td>\n",
       "      <td id=\"T_0a795_row15_col4\" class=\"data row15 col4\" >bool</td>\n",
       "      <td id=\"T_0a795_row15_col5\" class=\"data row15 col5\" >False</td>\n",
       "      <td id=\"T_0a795_row15_col6\" class=\"data row15 col6\" ></td>\n",
       "      <td id=\"T_0a795_row15_col7\" class=\"data row15 col7\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0a795_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_0a795_row16_col0\" class=\"data row16 col0\" >3. Config</td>\n",
       "      <td id=\"T_0a795_row16_col1\" class=\"data row16 col1\" >ForecastConfig (__init__)</td>\n",
       "      <td id=\"T_0a795_row16_col2\" class=\"data row16 col2\" >force_flip_invariance</td>\n",
       "      <td id=\"T_0a795_row16_col3\" class=\"data row16 col3\" ></td>\n",
       "      <td id=\"T_0a795_row16_col4\" class=\"data row16 col4\" >bool</td>\n",
       "      <td id=\"T_0a795_row16_col5\" class=\"data row16 col5\" >True</td>\n",
       "      <td id=\"T_0a795_row16_col6\" class=\"data row16 col6\" ></td>\n",
       "      <td id=\"T_0a795_row16_col7\" class=\"data row16 col7\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0a795_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_0a795_row17_col0\" class=\"data row17 col0\" >3. Config</td>\n",
       "      <td id=\"T_0a795_row17_col1\" class=\"data row17 col1\" >ForecastConfig (__init__)</td>\n",
       "      <td id=\"T_0a795_row17_col2\" class=\"data row17 col2\" >infer_is_positive</td>\n",
       "      <td id=\"T_0a795_row17_col3\" class=\"data row17 col3\" ></td>\n",
       "      <td id=\"T_0a795_row17_col4\" class=\"data row17 col4\" >bool</td>\n",
       "      <td id=\"T_0a795_row17_col5\" class=\"data row17 col5\" >True</td>\n",
       "      <td id=\"T_0a795_row17_col6\" class=\"data row17 col6\" ></td>\n",
       "      <td id=\"T_0a795_row17_col7\" class=\"data row17 col7\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0a795_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_0a795_row18_col0\" class=\"data row18 col0\" >3. Config</td>\n",
       "      <td id=\"T_0a795_row18_col1\" class=\"data row18 col1\" >ForecastConfig (__init__)</td>\n",
       "      <td id=\"T_0a795_row18_col2\" class=\"data row18 col2\" >fix_quantile_crossing</td>\n",
       "      <td id=\"T_0a795_row18_col3\" class=\"data row18 col3\" ></td>\n",
       "      <td id=\"T_0a795_row18_col4\" class=\"data row18 col4\" >bool</td>\n",
       "      <td id=\"T_0a795_row18_col5\" class=\"data row18 col5\" >False</td>\n",
       "      <td id=\"T_0a795_row18_col6\" class=\"data row18 col6\" ></td>\n",
       "      <td id=\"T_0a795_row18_col7\" class=\"data row18 col7\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0a795_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_0a795_row19_col0\" class=\"data row19 col0\" >3. Config</td>\n",
       "      <td id=\"T_0a795_row19_col1\" class=\"data row19 col1\" >ForecastConfig (__init__)</td>\n",
       "      <td id=\"T_0a795_row19_col2\" class=\"data row19 col2\" >return_backcast</td>\n",
       "      <td id=\"T_0a795_row19_col3\" class=\"data row19 col3\" ></td>\n",
       "      <td id=\"T_0a795_row19_col4\" class=\"data row19 col4\" >bool</td>\n",
       "      <td id=\"T_0a795_row19_col5\" class=\"data row19 col5\" >False</td>\n",
       "      <td id=\"T_0a795_row19_col6\" class=\"data row19 col6\" ></td>\n",
       "      <td id=\"T_0a795_row19_col7\" class=\"data row19 col7\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0a795_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "      <td id=\"T_0a795_row20_col0\" class=\"data row20 col0\" >4. Execution</td>\n",
       "      <td id=\"T_0a795_row20_col1\" class=\"data row20 col1\" >compile</td>\n",
       "      <td id=\"T_0a795_row20_col2\" class=\"data row20 col2\" >forecast_config</td>\n",
       "      <td id=\"T_0a795_row20_col3\" class=\"data row20 col3\" >✅</td>\n",
       "      <td id=\"T_0a795_row20_col4\" class=\"data row20 col4\" >timesfm.configs.ForecastConfig</td>\n",
       "      <td id=\"T_0a795_row20_col5\" class=\"data row20 col5\" >**Required**</td>\n",
       "      <td id=\"T_0a795_row20_col6\" class=\"data row20 col6\" >ForecastConfigオブジェクト</td>\n",
       "      <td id=\"T_0a795_row20_col7\" class=\"data row20 col7\" >Configuration for forecasting flags. **kwargs: Additional keyword arguments to pass to model.compile...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0a795_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "      <td id=\"T_0a795_row21_col0\" class=\"data row21 col0\" >4. Execution</td>\n",
       "      <td id=\"T_0a795_row21_col1\" class=\"data row21 col1\" >forecast</td>\n",
       "      <td id=\"T_0a795_row21_col2\" class=\"data row21 col2\" >horizon</td>\n",
       "      <td id=\"T_0a795_row21_col3\" class=\"data row21 col3\" >✅</td>\n",
       "      <td id=\"T_0a795_row21_col4\" class=\"data row21 col4\" >int</td>\n",
       "      <td id=\"T_0a795_row21_col5\" class=\"data row21 col5\" >**Required**</td>\n",
       "      <td id=\"T_0a795_row21_col6\" class=\"data row21 col6\" >int (max_horizon以下)</td>\n",
       "      <td id=\"T_0a795_row21_col7\" class=\"data row21 col7\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0a795_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "      <td id=\"T_0a795_row22_col0\" class=\"data row22 col0\" >4. Execution</td>\n",
       "      <td id=\"T_0a795_row22_col1\" class=\"data row22 col1\" >forecast</td>\n",
       "      <td id=\"T_0a795_row22_col2\" class=\"data row22 col2\" >inputs</td>\n",
       "      <td id=\"T_0a795_row22_col3\" class=\"data row22 col3\" >✅</td>\n",
       "      <td id=\"T_0a795_row22_col4\" class=\"data row22 col4\" >list[numpy.ndarray]</td>\n",
       "      <td id=\"T_0a795_row22_col5\" class=\"data row22 col5\" >**Required**</td>\n",
       "      <td id=\"T_0a795_row22_col6\" class=\"data row22 col6\" >List[float] 形状: (Batch, Length)</td>\n",
       "      <td id=\"T_0a795_row22_col7\" class=\"data row22 col7\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0a795_level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
       "      <td id=\"T_0a795_row23_col0\" class=\"data row23 col0\" >5. Saving</td>\n",
       "      <td id=\"T_0a795_row23_col1\" class=\"data row23 col1\" >save_pretrained</td>\n",
       "      <td id=\"T_0a795_row23_col2\" class=\"data row23 col2\" >save_directory</td>\n",
       "      <td id=\"T_0a795_row23_col3\" class=\"data row23 col3\" >✅</td>\n",
       "      <td id=\"T_0a795_row23_col4\" class=\"data row23 col4\" >Union[str, pathlib.Path]</td>\n",
       "      <td id=\"T_0a795_row23_col5\" class=\"data row23 col5\" >**Required**</td>\n",
       "      <td id=\"T_0a795_row23_col6\" class=\"data row23 col6\" ></td>\n",
       "      <td id=\"T_0a795_row23_col7\" class=\"data row23 col7\" > Path to directory in which the model weights and configuration will be saved.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0a795_level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
       "      <td id=\"T_0a795_row24_col0\" class=\"data row24 col0\" >5. Saving</td>\n",
       "      <td id=\"T_0a795_row24_col1\" class=\"data row24 col1\" >save_pretrained</td>\n",
       "      <td id=\"T_0a795_row24_col2\" class=\"data row24 col2\" >config</td>\n",
       "      <td id=\"T_0a795_row24_col3\" class=\"data row24 col3\" ></td>\n",
       "      <td id=\"T_0a795_row24_col4\" class=\"data row24 col4\" >Union[dict, huggingface_hub.hub_mixin.DataclassInstance, NoneType]</td>\n",
       "      <td id=\"T_0a795_row24_col5\" class=\"data row24 col5\" >None</td>\n",
       "      <td id=\"T_0a795_row24_col6\" class=\"data row24 col6\" ></td>\n",
       "      <td id=\"T_0a795_row24_col7\" class=\"data row24 col7\" > Model configuration specified as a key/value dictionary or a dataclass instance.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0a795_level0_row25\" class=\"row_heading level0 row25\" >25</th>\n",
       "      <td id=\"T_0a795_row25_col0\" class=\"data row25 col0\" >5. Saving</td>\n",
       "      <td id=\"T_0a795_row25_col1\" class=\"data row25 col1\" >save_pretrained</td>\n",
       "      <td id=\"T_0a795_row25_col2\" class=\"data row25 col2\" >repo_id</td>\n",
       "      <td id=\"T_0a795_row25_col3\" class=\"data row25 col3\" ></td>\n",
       "      <td id=\"T_0a795_row25_col4\" class=\"data row25 col4\" >Optional[str]</td>\n",
       "      <td id=\"T_0a795_row25_col5\" class=\"data row25 col5\" >None</td>\n",
       "      <td id=\"T_0a795_row25_col6\" class=\"data row25 col6\" ></td>\n",
       "      <td id=\"T_0a795_row25_col7\" class=\"data row25 col7\" > ID of your repository on the Hub. Used only if `push_to_hub=True`. Will default to the folder name ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0a795_level0_row26\" class=\"row_heading level0 row26\" >26</th>\n",
       "      <td id=\"T_0a795_row26_col0\" class=\"data row26 col0\" >5. Saving</td>\n",
       "      <td id=\"T_0a795_row26_col1\" class=\"data row26 col1\" >save_pretrained</td>\n",
       "      <td id=\"T_0a795_row26_col2\" class=\"data row26 col2\" >push_to_hub</td>\n",
       "      <td id=\"T_0a795_row26_col3\" class=\"data row26 col3\" ></td>\n",
       "      <td id=\"T_0a795_row26_col4\" class=\"data row26 col4\" >bool</td>\n",
       "      <td id=\"T_0a795_row26_col5\" class=\"data row26 col5\" >False</td>\n",
       "      <td id=\"T_0a795_row26_col6\" class=\"data row26 col6\" ></td>\n",
       "      <td id=\"T_0a795_row26_col7\" class=\"data row26 col7\" > Whether or not to push your model to the Huggingface Hub after saving it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0a795_level0_row27\" class=\"row_heading level0 row27\" >27</th>\n",
       "      <td id=\"T_0a795_row27_col0\" class=\"data row27 col0\" >5. Saving</td>\n",
       "      <td id=\"T_0a795_row27_col1\" class=\"data row27 col1\" >save_pretrained</td>\n",
       "      <td id=\"T_0a795_row27_col2\" class=\"data row27 col2\" >model_card_kwargs</td>\n",
       "      <td id=\"T_0a795_row27_col3\" class=\"data row27 col3\" ></td>\n",
       "      <td id=\"T_0a795_row27_col4\" class=\"data row27 col4\" >Optional[Dict[str, Any]]</td>\n",
       "      <td id=\"T_0a795_row27_col5\" class=\"data row27 col5\" >None</td>\n",
       "      <td id=\"T_0a795_row27_col6\" class=\"data row27 col6\" ></td>\n",
       "      <td id=\"T_0a795_row27_col7\" class=\"data row27 col7\" > Additional arguments passed to the model card template to customize the model card.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0a795_level0_row28\" class=\"row_heading level0 row28\" >28</th>\n",
       "      <td id=\"T_0a795_row28_col0\" class=\"data row28 col0\" >5. Saving</td>\n",
       "      <td id=\"T_0a795_row28_col1\" class=\"data row28 col1\" >save_pretrained</td>\n",
       "      <td id=\"T_0a795_row28_col2\" class=\"data row28 col2\" >push_to_hub_kwargs</td>\n",
       "      <td id=\"T_0a795_row28_col3\" class=\"data row28 col3\" >✅</td>\n",
       "      <td id=\"T_0a795_row28_col4\" class=\"data row28 col4\" >Any</td>\n",
       "      <td id=\"T_0a795_row28_col5\" class=\"data row28 col5\" >**Required**</td>\n",
       "      <td id=\"T_0a795_row28_col6\" class=\"data row28 col6\" ></td>\n",
       "      <td id=\"T_0a795_row28_col7\" class=\"data row28 col7\" > Additional key word arguments passed along to the [`~ModelHubMixin.push_to_hub`] method.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2b20040e790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import inspect\n",
    "import pandas as pd\n",
    "import timesfm\n",
    "import re\n",
    "from typing import get_type_hints\n",
    "\n",
    "# ==========================================\n",
    "# 1. 知識ベース: TimesFM 2.5 の具体的な制約・推奨値\n",
    "# ==========================================\n",
    "# 自動抽出だけでは分からない「正解値」をここで定義します\n",
    "KNOWN_SPECS = {\n",
    "    (\"ForecastConfig\", \"max_context\"): \"32 ~ 512 (推奨: 512)\",\n",
    "    (\"ForecastConfig\", \"max_horizon\"): \"1 ~ 512+ (推奨: 128)\",\n",
    "    (\"from_pretrained\", \"repo_id\"): \"例: 'google/timesfm-2.5-200m-pytorch'\",\n",
    "    (\"from_pretrained\", \"backend\"): \"['cpu', 'gpu', 'cuda', 'mps']\",\n",
    "    (\"forecast\", \"inputs\"): \"List[float] 形状: (Batch, Length)\",\n",
    "    (\"forecast\", \"freq\"): \"不要 (v2.5はZero-shot自動対応)\",\n",
    "    (\"forecast\", \"horizon\"): \"int (max_horizon以下)\",\n",
    "    (\"compile\", \"forecast_config\"): \"ForecastConfigオブジェクト\",\n",
    "    (\"TimesFM_2p5_200M_torch\", \"backend\"): \"['cpu', 'gpu', 'cuda']\",\n",
    "}\n",
    "\n",
    "# ==========================================\n",
    "# 2. 高度なDocstringパーサー\n",
    "# ==========================================\n",
    "def parse_docstring_args(docstring):\n",
    "    \"\"\"Google/NumPyスタイルのDocstringから引数説明を抽出する\"\"\"\n",
    "    if not docstring:\n",
    "        return {}\n",
    "    \n",
    "    args_info = {}\n",
    "    # Args: セクションを探す\n",
    "    match = re.search(r'Args:\\s*(.*?)(?:Raises:|Returns:|Example:|$)', docstring, re.DOTALL)\n",
    "    if match:\n",
    "        content = match.group(1)\n",
    "        # \"  arg_name (type): description\" のパターンを抽出\n",
    "        # 行頭の空白と引数名をキャプチャ\n",
    "        lines = content.split('\\n')\n",
    "        current_arg = None\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line: continue\n",
    "            \n",
    "            # 引数定義行の検出 (例: \"input_len (int): ...\")\n",
    "            arg_match = re.match(r'^(\\w+)(?:\\s*\\(.*?\\))?:\\s*(.*)', line)\n",
    "            if arg_match:\n",
    "                current_arg = arg_match.group(1)\n",
    "                desc = arg_match.group(2)\n",
    "                args_info[current_arg] = desc\n",
    "            elif current_arg:\n",
    "                # 前の引数の説明の続き\n",
    "                args_info[current_arg] += \" \" + line\n",
    "    return args_info\n",
    "\n",
    "# ==========================================\n",
    "# 3. 解析ロジック\n",
    "# ==========================================\n",
    "def analyze_meta_table(targets):\n",
    "    data = []\n",
    "    \n",
    "    for category, items in targets.items():\n",
    "        for func_or_class in items:\n",
    "            name = func_or_class.__name__\n",
    "            \n",
    "            # クラスなら __init__ を対象にする\n",
    "            if inspect.isclass(func_or_class):\n",
    "                target_func = func_or_class.__init__\n",
    "                is_class = True\n",
    "                display_name = name + \" (__init__)\"\n",
    "            else:\n",
    "                target_func = func_or_class\n",
    "                is_class = False\n",
    "                display_name = name\n",
    "\n",
    "            try:\n",
    "                sig = inspect.signature(target_func)\n",
    "                doc = inspect.getdoc(target_func)\n",
    "                doc_args = parse_docstring_args(doc) # Docstringから説明を抽出\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            for param_name, param in sig.parameters.items():\n",
    "                if param_name in [\"self\", \"kwargs\", \"args\"]:\n",
    "                    continue\n",
    "\n",
    "                # --- 情報収集 ---\n",
    "                # 1. 型ヒント\n",
    "                type_hint = str(param.annotation).replace(\"typing.\", \"\").replace(\"<class '\", \"\").replace(\"'>\", \"\")\n",
    "                if type_hint == \"inspect._empty\": type_hint = \"Any\"\n",
    "\n",
    "                # 2. 必須チェック\n",
    "                is_required = param.default == inspect.Parameter.empty\n",
    "                default_val = \"**Required**\" if is_required else str(param.default)\n",
    "\n",
    "                # 3. 知識ベースからの補完 (具体的な値)\n",
    "                # キー: (関数名, 引数名) または (クラス名, 引数名)\n",
    "                verified_val = KNOWN_SPECS.get((name, param_name), \"\")\n",
    "                if not verified_val and is_class:\n",
    "                     verified_val = KNOWN_SPECS.get((func_or_class.__name__, param_name), \"\")\n",
    "\n",
    "                # 4. Docstring説明\n",
    "                description = doc_args.get(param_name, \"\")\n",
    "                if not description and doc:\n",
    "                     # 簡易検索フォールバック\n",
    "                     if param_name in doc:\n",
    "                         description = \"See docstring\"\n",
    "\n",
    "                data.append({\n",
    "                    \"Category\": category,\n",
    "                    \"Function/Class\": display_name,\n",
    "                    \"Argument\": param_name,\n",
    "                    \"Required\": \"✅\" if is_required else \"\",\n",
    "                    \"Type Hint\": type_hint,\n",
    "                    \"Default\": default_val,\n",
    "                    \"✅ Verified Specs / Constraints\": verified_val,  # ★ここが重要\n",
    "                    \"Description\": description[:100] + \"...\" if len(description) > 100 else description\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# ==========================================\n",
    "# 4. 実行設定\n",
    "# ==========================================\n",
    "# クラスの特定 (環境に合わせて自動選択)\n",
    "try:\n",
    "    MainClass = timesfm.TimesFM_2p5_200M_torch\n",
    "except AttributeError:\n",
    "    MainClass = timesfm.TimesFM\n",
    "\n",
    "targets = {\n",
    "    \"1. Initialization\": [MainClass],\n",
    "    \"2. Loading\": [MainClass.from_pretrained, MainClass.load_checkpoint],\n",
    "    \"3. Config\": [timesfm.configs.ForecastConfig],\n",
    "    \"4. Execution\": [MainClass.compile, MainClass.forecast],\n",
    "    \"5. Saving\": [MainClass.save_pretrained],\n",
    "}\n",
    "\n",
    "# 解析実行\n",
    "df_meta = analyze_meta_table(targets)\n",
    "\n",
    "# ==========================================\n",
    "# 5. 見やすいスタイルで表示\n",
    "# ==========================================\n",
    "# pandas Stylerを使って「確認すべき場所」を強調\n",
    "def highlight_required(s):\n",
    "    return ['background-color: #ffcccc' if v == '✅' else '' for v in s]\n",
    "\n",
    "def highlight_verified(s):\n",
    "    return ['font-weight: bold; color: #005a9c' if v != '' else '' for v in s]\n",
    "\n",
    "styler = df_meta.style\\\n",
    "    .apply(highlight_required, subset=['Required'])\\\n",
    "    .apply(highlight_verified, subset=['✅ Verified Specs / Constraints'])\\\n",
    "    .set_properties(**{'text-align': 'left'})\\\n",
    "    .set_table_styles([\n",
    "        {'selector': 'th', 'props': [('text-align', 'left'), ('background-color', '#f0f0f0')]},\n",
    "        {'selector': 'td', 'props': [('padding', '8px')]}\n",
    "    ])\n",
    "\n",
    "print(\"=== TimesFM Meta Execution Table ===\")\n",
    "display(styler)\n",
    "\n",
    "# CSV保存\n",
    "df_meta.to_csv(\"timesfm_meta_execution_table.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da52519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a670879",
   "metadata": {},
   "source": [
    "# 2-1. モデルロード方法の比較検証\n",
    "\n",
    "ご要望の通り、以下の2つのロード方法を両方試して検証します。\n",
    "\n",
    "1.  **`from_pretrained()`**: Hugging Face Hub からモデル定義と重みを一括でロードする（標準的方法）。\n",
    "2.  **`load_checkpoint()`**: ローカルに保存された重みファイルを、初期化済みのインスタンスに読み込む。\n",
    "\n",
    "**検証フロー:**\n",
    "1.  `from_pretrained` でモデル (A) をロード。\n",
    "2.  モデル (A) をローカルディレクトリに保存 (`save_pretrained`)。\n",
    "3.  新しい空のモデルインスタンス (B) を作成。\n",
    "4.  モデル (B) に `load_checkpoint` を使用して、保存したローカルの重みを読み込む。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e7e216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Loading Source Model ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eee237b1e9643c3986c04f46006ecbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/475 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded.\n",
      "\n",
      "=== Approach 1: save_pretrained -> from_pretrained (Recommended) ===\n",
      "Model saved to (HF format): ./timesfm_hf_local\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e3919a84054b21beb768fb13f39320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/475 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded.\n",
      "Success: Loaded locally using from_pretrained()!\n",
      "\n",
      "=== Approach 2: torch.save -> load_checkpoint (Low-level) ===\n",
      "Model weights saved to (PyTorch format): ./timesfm_state_dict.pt\n",
      "Failed to load checkpoint: \n",
      "Debug: Keys in saved file (top 5): ['tokenizer.hidden_layer.weight', 'tokenizer.hidden_layer.bias', 'tokenizer.output_layer.weight', 'tokenizer.output_layer.bias', 'tokenizer.residual_layer.weight']\n"
     ]
    }
   ],
   "source": [
    "import timesfm\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# デバイス設定\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# --- 準備: 元モデルのロード ---\n",
    "print(\"=== Loading Source Model ===\")\n",
    "tfm_hub = timesfm.TimesFM_2p5_200M_torch.from_pretrained(\n",
    "    \"google/timesfm-2.5-200m-pytorch\",\n",
    "    backend=device\n",
    ")\n",
    "\n",
    "# =========================================================\n",
    "# アプローチ1: 正攻法 (save_pretrained -> from_pretrained)\n",
    "# =========================================================\n",
    "print(\"\\n=== Approach 1: save_pretrained -> from_pretrained (Recommended) ===\")\n",
    "save_dir = \"./timesfm_hf_local\"\n",
    "tfm_hub.save_pretrained(save_dir)\n",
    "print(f\"Model saved to (HF format): {save_dir}\")\n",
    "\n",
    "# ローカルパスを指定してロード\n",
    "try:\n",
    "    tfm_local_hf = timesfm.TimesFM_2p5_200M_torch.from_pretrained(save_dir, backend=device)\n",
    "    print(\"Success: Loaded locally using from_pretrained()!\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed: {e}\")\n",
    "\n",
    "# =========================================================\n",
    "# アプローチ2: load_checkpoint の検証 (torch.save -> load_checkpoint)\n",
    "# =========================================================\n",
    "print(\"\\n=== Approach 2: torch.save -> load_checkpoint (Low-level) ===\")\n",
    "pt_file = \"./timesfm_state_dict.pt\"\n",
    "\n",
    "# 1. 生の state_dict を .pt ファイルとして保存\n",
    "# (Hugging Faceの形式ではなく、PyTorch標準の単一ファイル形式)\n",
    "torch.save(tfm_hub.model.state_dict(), pt_file)\n",
    "print(f\"Model weights saved to (PyTorch format): {pt_file}\")\n",
    "\n",
    "# 2. 空のモデルを作成して load_checkpoint で読み込む\n",
    "tfm_local_pt = timesfm.TimesFM_2p5_200M_torch(backend=device)\n",
    "\n",
    "try:\n",
    "    # load_checkpoint は内部で torch.load を呼んでいる可能性が高い\n",
    "    tfm_local_pt.load_checkpoint(pt_file)\n",
    "    print(\"Success: Loaded locally using load_checkpoint()!\")\n",
    "    \n",
    "    # コンパイル確認\n",
    "    print(\"Compiling loaded model...\")\n",
    "    tfm_local_pt.compile(timesfm.configs.ForecastConfig(max_context=64, max_horizon=32))\n",
    "    print(\"Compilation successful.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Failed to load checkpoint: {e}\")\n",
    "    # 失敗した場合、モデルの構造(キー名)を確認するデバッグ情報を出す\n",
    "    try:\n",
    "        loaded_dict = torch.load(pt_file)\n",
    "        print(\"Debug: Keys in saved file (top 5):\", list(loaded_dict.keys())[:5])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25067591",
   "metadata": {},
   "source": [
    "# 3. 予測設定 (Forecast Configuration)\n",
    "\n",
    "モデルのコンパイルを行う前に、予測時の挙動を制御する **`ForecastConfig`** オブジェクトを作成します。\n",
    "ここで設定するパラメータは、メモリ使用量や推論速度、予測の精度に影響します。\n",
    "\n",
    "主な設定項目:\n",
    "* `max_context`: モデルに入力する過去データの最大長（これより長い系列は切り詰められます）。\n",
    "* `max_horizon`: 一度に予測する未来の期間（予測ホライゾン）。\n",
    "* `per_core_batch_size`: 1回の推論処理でまとめて処理する系列の数（バッチサイズ）。\n",
    "* `normalize_inputs`: 入力データを正規化するかどうか（デフォルトは `False` ですが、データによっては有効）。\n",
    "* `use_continuous_quantile_head`: 分位点予測（不確実性の推定）を行う場合の設定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7110befa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timesfm\n",
    "\n",
    "# 予測設定の定義\n",
    "# API定義に基づき、timesfm.configs.ForecastConfig を使用します\n",
    "forecast_config = timesfm.configs.ForecastConfig(\n",
    "    max_context=512,       # 入力コンテキスト長 (例: 過去512ステップ)\n",
    "    max_horizon=128,       # 予測ホライゾン (例: 未来128ステップ)\n",
    "    per_core_batch_size=32,# バッチサイズ (GPUメモリに応じて調整)\n",
    "    \n",
    "    # その他のオプション設定 (必要に応じて有効化)\n",
    "    normalize_inputs=True, # 入力系列の正規化を行うか\n",
    "    # quantiles=[0.1, 0.5, 0.9] # 分位点予測が必要な場合 (モデルの対応状況による)\n",
    ")\n",
    "\n",
    "# 設定内容の確認\n",
    "print(\"Forecast Config created:\")\n",
    "print(f\"  Max Context: {forecast_config.max_context}\")\n",
    "print(f\"  Max Horizon: {forecast_config.max_horizon}\")\n",
    "print(f\"  Batch Size : {forecast_config.per_core_batch_size}\")\n",
    "print(f\"  Normalize  : {forecast_config.normalize_inputs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020722ce",
   "metadata": {},
   "source": [
    "# 3-1. ForecastConfig 引数の網羅的確認\n",
    "\n",
    "`ForecastConfig` で設定可能なすべての引数を確認します。\n",
    "解析結果およびライブラリの仕様に基づく設定項目は以下の通りです。\n",
    "\n",
    "| 引数名 | 型 | デフォルト値 | 概要 (推定) |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **`max_context`** | `int` | `0` (必須) | 入力系列の最大長。これを超える過去データは切り捨てられます。 |\n",
    "| **`max_horizon`** | `int` | `0` (必須) | 予測する未来の期間（予測ホライゾン）の最大長。 |\n",
    "| **`per_core_batch_size`** | `int` | `1` | 1回の推論ステップで処理する系列数（バッチサイズ）。 |\n",
    "| **`normalize_inputs`** | `bool` | `False` | 入力系列をモデルに入力する前に正規化（平均0、分散1など）するかどうか。 |\n",
    "| **`window_size`** | `int` | `0` | (高度な設定) 特定のウィンドウ処理を行う場合のサイズ。 |\n",
    "| **`use_continuous_quantile_head`** | `bool` | `False` | 連続的な分位点予測ヘッドを使用するかどうか（不確実性予測用）。 |\n",
    "| **`force_flip_invariance`** | `bool` | `True` | 上下反転に対する不変性を強制するかどうか。 |\n",
    "| **`infer_is_positive`** | `bool` | `True` | 値が正であることを推論時に考慮するかどうか。 |\n",
    "| **`fix_quantile_crossing`** | `bool` | `False` | 分位点同士の交差（矛盾）を修正するかどうか。 |\n",
    "| **`return_backcast`** | `bool` | `False` | 予測だけでなく、入力期間の適合値（バックキャスト）も返すかどうか。 |\n",
    "\n",
    "以下のコードで、実際のライブラリ定義からドキュメントとシグネチャを出力して確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7ef09e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ForecastConfig Signature ---\n",
      "max_context: <class 'int'> = 0\n",
      "max_horizon: <class 'int'> = 0\n",
      "normalize_inputs: <class 'bool'> = False\n",
      "window_size: <class 'int'> = 0\n",
      "per_core_batch_size: <class 'int'> = 1\n",
      "use_continuous_quantile_head: <class 'bool'> = False\n",
      "force_flip_invariance: <class 'bool'> = True\n",
      "infer_is_positive: <class 'bool'> = True\n",
      "fix_quantile_crossing: <class 'bool'> = False\n",
      "return_backcast: <class 'bool'> = False\n",
      "\n",
      "--- Docstring ---\n",
      "Options for forecasting.\n",
      "\n",
      "  Attributes:\n",
      "    max_context: The maximum context length. This is used by the complied decode\n",
      "      function at inference time during batched inference. Any input time series\n",
      "      with length less than max_context will be padded with zeros, and with\n",
      "      length greater than max_context will be truncated.\n",
      "    max_horizon: The maximum horizon length. This is used by the complied decode\n",
      "      function at inference time during batched inference. The compiled cached\n",
      "      decoding function will by default forecast till max_horizon.\n",
      "    normalize_inputs: Whether to normalize the inputs. This is useful when the\n",
      "      raw inputs are of extremely large or small magnitudes which may result in\n",
      "      numerical issues.\n",
      "    window_size: The window size for decomposed forecasting.\n",
      "      TODO(siriuz42):implement it.\n",
      "    per_core_batch_size: The batch size per core. Used at inference time during\n",
      "      batched inference when multiple GPU / TPU devices are used.\n",
      "    use_continuous_quantile_head: Whether to use a separate continuous quantile\n",
      "      head to avoid quantile collapsing.\n",
      "    force_flip_invariance: Whether to force flip invariance. TimesFM guarantees\n",
      "      that TimesFM(aX + b) = a * TimesFM(x) + b for a >= 0 by default. This flag\n",
      "      extends it to a < 0 as well.\n",
      "    infer_is_positive: Whether to guarantee nonnegativity of the output if the\n",
      "      input is nonnegative.\n",
      "    fix_quantile_crossing: Whether to fix quantile crossing.\n",
      "    return_backcast: Whether to return backcast.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import timesfm\n",
    "import inspect\n",
    "\n",
    "# ForecastConfig クラスのシグネチャ（引数定義）を取得して表示\n",
    "print(\"--- ForecastConfig Signature ---\")\n",
    "sig = inspect.signature(timesfm.configs.ForecastConfig)\n",
    "for name, param in sig.parameters.items():\n",
    "    print(f\"{name}: {param.annotation} = {param.default}\")\n",
    "\n",
    "print(\"\\n--- Docstring ---\")\n",
    "print(timesfm.configs.ForecastConfig.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50963234",
   "metadata": {},
   "source": [
    "# 3-2. 設定パラメータの一覧化と網羅的検証\n",
    "\n",
    "`ForecastConfig` に設定可能なすべての引数をカラム（列）として持ち、検証したい値を格納した一覧表（DataFrame）を作成します。\n",
    "この表データを基に、実際に `ForecastConfig` オブジェクトを生成し、すべての値が正しく反映されるか検証します。\n",
    "\n",
    "**検証パターン:**\n",
    "1. **Case 0 (Standard)**: 一般的な推奨設定（正規化ON、非負制約ONなど）。\n",
    "2. **Case 1 (Experimental)**: すべてのオプション機能を有効化または変更した設定（分位点ヘッド使用、バックキャスト取得など）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e03d1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ForecastConfig 検証用パラメータ一覧表 ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_context</th>\n",
       "      <th>max_horizon</th>\n",
       "      <th>per_core_batch_size</th>\n",
       "      <th>normalize_inputs</th>\n",
       "      <th>infer_is_positive</th>\n",
       "      <th>force_flip_invariance</th>\n",
       "      <th>use_continuous_quantile_head</th>\n",
       "      <th>fix_quantile_crossing</th>\n",
       "      <th>return_backcast</th>\n",
       "      <th>window_size</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Case</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1024</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      max_context  max_horizon  per_core_batch_size  normalize_inputs  \\\n",
       "Case                                                                    \n",
       "0             512          128                   32              True   \n",
       "1            1024          256                    1             False   \n",
       "\n",
       "      infer_is_positive  force_flip_invariance  use_continuous_quantile_head  \\\n",
       "Case                                                                           \n",
       "0                  True                   True                         False   \n",
       "1                 False                  False                          True   \n",
       "\n",
       "      fix_quantile_crossing  return_backcast  window_size  \n",
       "Case                                                       \n",
       "0                     False            False            0  \n",
       "1                      True             True           10  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 設定反映の検証結果 ---\n",
      "[Case 0] OK: 全てのパラメータが正常に設定されました。\n",
      "[Case 1] OK: 全てのパラメータが正常に設定されました。\n",
      "\n",
      "最終的に採用する設定 (Case 0):\n",
      "ForecastConfig(max_context=512, max_horizon=128, normalize_inputs=True, window_size=0, per_core_batch_size=32, use_continuous_quantile_head=False, force_flip_invariance=True, infer_is_positive=True, fix_quantile_crossing=False, return_backcast=False)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import timesfm\n",
    "\n",
    "# 1. 検証用パラメータセットの定義\n",
    "# 引数名をキー、設定したい値のリストを値とする辞書を作成\n",
    "# Case 0: 実用的で標準的な設定\n",
    "# Case 1: オプション機能を網羅的に変更した設定\n",
    "validation_data = {\n",
    "    \"max_context\":                  [512,   1024],\n",
    "    \"max_horizon\":                  [128,   256],\n",
    "    \"per_core_batch_size\":          [32,    1],\n",
    "    \"normalize_inputs\":             [True,  False],\n",
    "    \"infer_is_positive\":            [True,  False],\n",
    "    \"force_flip_invariance\":        [True,  False],\n",
    "    \"use_continuous_quantile_head\": [False, True],\n",
    "    \"fix_quantile_crossing\":        [False, True],\n",
    "    \"return_backcast\":              [False, True],\n",
    "    \"window_size\":                  [0,     10]  # TODO機能だが引数としては存在\n",
    "}\n",
    "\n",
    "# 一覧表 (DataFrame) の作成\n",
    "df_configs = pd.DataFrame(validation_data)\n",
    "df_configs.index.name = \"Case\"\n",
    "\n",
    "print(\"--- ForecastConfig 検証用パラメータ一覧表 ---\")\n",
    "display(df_configs)\n",
    "\n",
    "# 2. 網羅的な検証実行\n",
    "print(\"\\n--- 設定反映の検証結果 ---\")\n",
    "\n",
    "generated_configs = []\n",
    "\n",
    "for idx, row in df_configs.iterrows():\n",
    "    # DataFrameの行を辞書に変換\n",
    "    params = row.to_dict()\n",
    "    \n",
    "    # 型の修正 (PandasはIntをFloatにすることがあるため、明示的にintへキャスト)\n",
    "    int_fields = [\"max_context\", \"max_horizon\", \"per_core_batch_size\", \"window_size\"]\n",
    "    for field in int_fields:\n",
    "        params[field] = int(params[field])\n",
    "    \n",
    "    try:\n",
    "        # Configオブジェクトの生成（引数展開 **params を使用）\n",
    "        config = timesfm.configs.ForecastConfig(**params)\n",
    "        \n",
    "        # 検証ロジック: 入力値とオブジェクトの属性値がすべて一致するか確認\n",
    "        mismatches = []\n",
    "        for key, expected_val in params.items():\n",
    "            actual_val = getattr(config, key)\n",
    "            if actual_val != expected_val:\n",
    "                mismatches.append(f\"{key}: expected {expected_val}, got {actual_val}\")\n",
    "        \n",
    "        if not mismatches:\n",
    "            print(f\"[Case {idx}] OK: 全てのパラメータが正常に設定されました。\")\n",
    "            generated_configs.append(config)\n",
    "        else:\n",
    "            print(f\"[Case {idx}] FAILED: パラメータの不一致があります -> {mismatches}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"[Case {idx}] ERROR: 設定中にエラーが発生しました -> {e}\")\n",
    "\n",
    "# 最後に、検証に使用する設定（Case 0）を変数に保持\n",
    "forecast_config = generated_configs[0]\n",
    "print(f\"\\n最終的に採用する設定 (Case 0):\\n{forecast_config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d36c8c",
   "metadata": {},
   "source": [
    "# 3-3. 予測設定クラスのアクセス方法検証\n",
    "\n",
    "TimesFMのAPIには、トップレベルの `timesfm.ForecastConfig` と、サブモジュールの `timesfm.configs.ForecastConfig` の2つのアクセス経路が存在します。\n",
    "両方を使用して設定オブジェクトを生成し、動作に違いがないか、およびこれらが同一のクラス定義を指しているかを確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbdfdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timesfm\n",
    "\n",
    "# テスト用の設定値\n",
    "test_params = {\n",
    "    \"max_context\": 512,\n",
    "    \"max_horizon\": 128,\n",
    "    \"per_core_batch_size\": 16,\n",
    "    \"normalize_inputs\": True\n",
    "}\n",
    "\n",
    "print(\"=== 1. timesfm.ForecastConfig (Top-level) ===\")\n",
    "try:\n",
    "    config_v1 = timesfm.ForecastConfig(**test_params)\n",
    "    print(\"Success: Instance created via timesfm.ForecastConfig\")\n",
    "    print(f\"Object type: {type(config_v1)}\")\n",
    "    print(f\"Values: context={config_v1.max_context}, horizon={config_v1.max_horizon}\")\n",
    "except AttributeError:\n",
    "    print(\"Error: timesfm.ForecastConfig not found.\")\n",
    "\n",
    "print(\"\\n=== 2. timesfm.configs.ForecastConfig (Sub-module) ===\")\n",
    "try:\n",
    "    config_v2 = timesfm.configs.ForecastConfig(**test_params)\n",
    "    print(\"Success: Instance created via timesfm.configs.ForecastConfig\")\n",
    "    print(f\"Object type: {type(config_v2)}\")\n",
    "    print(f\"Values: context={config_v2.max_context}, horizon={config_v2.max_horizon}\")\n",
    "except AttributeError:\n",
    "    print(\"Error: timesfm.configs.ForecastConfig not found.\")\n",
    "\n",
    "print(\"\\n=== 3. Identity Verification ===\")\n",
    "# 両者が同じクラス定義を指しているか確認\n",
    "try:\n",
    "    is_same_class = timesfm.ForecastConfig is timesfm.configs.ForecastConfig\n",
    "    print(f\"Are they the same class definition? : {is_same_class}\")\n",
    "    \n",
    "    # データの内容が一致するか確認 (属性レベル)\n",
    "    if 'config_v1' in locals() and 'config_v2' in locals():\n",
    "        # __dict__ を比較して属性が全て同じかチェック\n",
    "        attrs_match = config_v1.__dict__ == config_v2.__dict__\n",
    "        print(f\"Do the instances have identical attributes? : {attrs_match}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Comparison failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad157e22",
   "metadata": {},
   "source": [
    "# 引数確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd785d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TimesFM API Inspection (Target: TimesFM_2p5_200M_torch) ===\n",
      "\n",
      "--- 1. Model Initialization ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_80438_row0_col0, #T_80438_row0_col1, #T_80438_row0_col2, #T_80438_row0_col3, #T_80438_row1_col0, #T_80438_row1_col1, #T_80438_row1_col2, #T_80438_row1_col3 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_80438\">\n",
       "  <caption>Function: TimesFM_2p5_200M_torch</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_80438_level0_col0\" class=\"col_heading level0 col0\" >Method/Class</th>\n",
       "      <th id=\"T_80438_level0_col1\" class=\"col_heading level0 col1\" >Argument</th>\n",
       "      <th id=\"T_80438_level0_col2\" class=\"col_heading level0 col2\" >Type Hint</th>\n",
       "      <th id=\"T_80438_level0_col3\" class=\"col_heading level0 col3\" >Default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_80438_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_80438_row0_col0\" class=\"data row0 col0\" >TimesFM_2p5_200M_torch</td>\n",
       "      <td id=\"T_80438_row0_col1\" class=\"data row0 col1\" >args</td>\n",
       "      <td id=\"T_80438_row0_col2\" class=\"data row0 col2\" ></td>\n",
       "      <td id=\"T_80438_row0_col3\" class=\"data row0 col3\" >**Required**</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_80438_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_80438_row1_col0\" class=\"data row1 col0\" >TimesFM_2p5_200M_torch</td>\n",
       "      <td id=\"T_80438_row1_col1\" class=\"data row1 col1\" >kwargs</td>\n",
       "      <td id=\"T_80438_row1_col2\" class=\"data row1 col2\" ></td>\n",
       "      <td id=\"T_80438_row1_col3\" class=\"data row1 col3\" >**Required**</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2b1f7a5e790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- 2. Loading ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_f5b23_row0_col0, #T_f5b23_row0_col1, #T_f5b23_row0_col2, #T_f5b23_row0_col3, #T_f5b23_row1_col0, #T_f5b23_row1_col1, #T_f5b23_row1_col2, #T_f5b23_row1_col3, #T_f5b23_row2_col0, #T_f5b23_row2_col1, #T_f5b23_row2_col2, #T_f5b23_row2_col3, #T_f5b23_row3_col0, #T_f5b23_row3_col1, #T_f5b23_row3_col2, #T_f5b23_row3_col3, #T_f5b23_row4_col0, #T_f5b23_row4_col1, #T_f5b23_row4_col2, #T_f5b23_row4_col3, #T_f5b23_row5_col0, #T_f5b23_row5_col1, #T_f5b23_row5_col2, #T_f5b23_row5_col3, #T_f5b23_row6_col0, #T_f5b23_row6_col1, #T_f5b23_row6_col2, #T_f5b23_row6_col3, #T_f5b23_row7_col0, #T_f5b23_row7_col1, #T_f5b23_row7_col2, #T_f5b23_row7_col3, #T_f5b23_row8_col0, #T_f5b23_row8_col1, #T_f5b23_row8_col2, #T_f5b23_row8_col3 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_f5b23\">\n",
       "  <caption>Function: from_pretrained</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_f5b23_level0_col0\" class=\"col_heading level0 col0\" >Method/Class</th>\n",
       "      <th id=\"T_f5b23_level0_col1\" class=\"col_heading level0 col1\" >Argument</th>\n",
       "      <th id=\"T_f5b23_level0_col2\" class=\"col_heading level0 col2\" >Type Hint</th>\n",
       "      <th id=\"T_f5b23_level0_col3\" class=\"col_heading level0 col3\" >Default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f5b23_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_f5b23_row0_col0\" class=\"data row0 col0\" >from_pretrained</td>\n",
       "      <td id=\"T_f5b23_row0_col1\" class=\"data row0 col1\" >pretrained_model_name_or_path</td>\n",
       "      <td id=\"T_f5b23_row0_col2\" class=\"data row0 col2\" >Union[str, pathlib.Path]</td>\n",
       "      <td id=\"T_f5b23_row0_col3\" class=\"data row0 col3\" >**Required**</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5b23_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_f5b23_row1_col0\" class=\"data row1 col0\" >from_pretrained</td>\n",
       "      <td id=\"T_f5b23_row1_col1\" class=\"data row1 col1\" >force_download</td>\n",
       "      <td id=\"T_f5b23_row1_col2\" class=\"data row1 col2\" >bool</td>\n",
       "      <td id=\"T_f5b23_row1_col3\" class=\"data row1 col3\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5b23_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_f5b23_row2_col0\" class=\"data row2 col0\" >from_pretrained</td>\n",
       "      <td id=\"T_f5b23_row2_col1\" class=\"data row2 col1\" >resume_download</td>\n",
       "      <td id=\"T_f5b23_row2_col2\" class=\"data row2 col2\" >Optional[bool]</td>\n",
       "      <td id=\"T_f5b23_row2_col3\" class=\"data row2 col3\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5b23_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_f5b23_row3_col0\" class=\"data row3 col0\" >from_pretrained</td>\n",
       "      <td id=\"T_f5b23_row3_col1\" class=\"data row3 col1\" >proxies</td>\n",
       "      <td id=\"T_f5b23_row3_col2\" class=\"data row3 col2\" >Optional[Dict]</td>\n",
       "      <td id=\"T_f5b23_row3_col3\" class=\"data row3 col3\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5b23_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_f5b23_row4_col0\" class=\"data row4 col0\" >from_pretrained</td>\n",
       "      <td id=\"T_f5b23_row4_col1\" class=\"data row4 col1\" >token</td>\n",
       "      <td id=\"T_f5b23_row4_col2\" class=\"data row4 col2\" >Union[bool, str, NoneType]</td>\n",
       "      <td id=\"T_f5b23_row4_col3\" class=\"data row4 col3\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5b23_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_f5b23_row5_col0\" class=\"data row5 col0\" >from_pretrained</td>\n",
       "      <td id=\"T_f5b23_row5_col1\" class=\"data row5 col1\" >cache_dir</td>\n",
       "      <td id=\"T_f5b23_row5_col2\" class=\"data row5 col2\" >Union[str, pathlib.Path, NoneType]</td>\n",
       "      <td id=\"T_f5b23_row5_col3\" class=\"data row5 col3\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5b23_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_f5b23_row6_col0\" class=\"data row6 col0\" >from_pretrained</td>\n",
       "      <td id=\"T_f5b23_row6_col1\" class=\"data row6 col1\" >local_files_only</td>\n",
       "      <td id=\"T_f5b23_row6_col2\" class=\"data row6 col2\" >bool</td>\n",
       "      <td id=\"T_f5b23_row6_col3\" class=\"data row6 col3\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5b23_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_f5b23_row7_col0\" class=\"data row7 col0\" >from_pretrained</td>\n",
       "      <td id=\"T_f5b23_row7_col1\" class=\"data row7 col1\" >revision</td>\n",
       "      <td id=\"T_f5b23_row7_col2\" class=\"data row7 col2\" >Optional[str]</td>\n",
       "      <td id=\"T_f5b23_row7_col3\" class=\"data row7 col3\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5b23_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_f5b23_row8_col0\" class=\"data row8 col0\" >from_pretrained</td>\n",
       "      <td id=\"T_f5b23_row8_col1\" class=\"data row8 col1\" >model_kwargs</td>\n",
       "      <td id=\"T_f5b23_row8_col2\" class=\"data row8 col2\" ></td>\n",
       "      <td id=\"T_f5b23_row8_col3\" class=\"data row8 col3\" >**Required**</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2b2018aff90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_69010_row0_col0, #T_69010_row0_col1, #T_69010_row0_col2, #T_69010_row0_col3, #T_69010_row1_col0, #T_69010_row1_col1, #T_69010_row1_col2, #T_69010_row1_col3 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_69010\">\n",
       "  <caption>Function: load_checkpoint</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_69010_level0_col0\" class=\"col_heading level0 col0\" >Method/Class</th>\n",
       "      <th id=\"T_69010_level0_col1\" class=\"col_heading level0 col1\" >Argument</th>\n",
       "      <th id=\"T_69010_level0_col2\" class=\"col_heading level0 col2\" >Type Hint</th>\n",
       "      <th id=\"T_69010_level0_col3\" class=\"col_heading level0 col3\" >Default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_69010_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_69010_row0_col0\" class=\"data row0 col0\" >load_checkpoint</td>\n",
       "      <td id=\"T_69010_row0_col1\" class=\"data row0 col1\" >self</td>\n",
       "      <td id=\"T_69010_row0_col2\" class=\"data row0 col2\" ></td>\n",
       "      <td id=\"T_69010_row0_col3\" class=\"data row0 col3\" >**Required**</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_69010_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_69010_row1_col0\" class=\"data row1 col0\" >load_checkpoint</td>\n",
       "      <td id=\"T_69010_row1_col1\" class=\"data row1 col1\" >path</td>\n",
       "      <td id=\"T_69010_row1_col2\" class=\"data row1 col2\" >str</td>\n",
       "      <td id=\"T_69010_row1_col3\" class=\"data row1 col3\" >**Required**</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2b1f7a5e790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- 3. Saving ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_70314_row0_col0, #T_70314_row0_col1, #T_70314_row0_col2, #T_70314_row0_col3, #T_70314_row1_col0, #T_70314_row1_col1, #T_70314_row1_col2, #T_70314_row1_col3, #T_70314_row2_col0, #T_70314_row2_col1, #T_70314_row2_col2, #T_70314_row2_col3, #T_70314_row3_col0, #T_70314_row3_col1, #T_70314_row3_col2, #T_70314_row3_col3, #T_70314_row4_col0, #T_70314_row4_col1, #T_70314_row4_col2, #T_70314_row4_col3, #T_70314_row5_col0, #T_70314_row5_col1, #T_70314_row5_col2, #T_70314_row5_col3, #T_70314_row6_col0, #T_70314_row6_col1, #T_70314_row6_col2, #T_70314_row6_col3 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_70314\">\n",
       "  <caption>Function: save_pretrained</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_70314_level0_col0\" class=\"col_heading level0 col0\" >Method/Class</th>\n",
       "      <th id=\"T_70314_level0_col1\" class=\"col_heading level0 col1\" >Argument</th>\n",
       "      <th id=\"T_70314_level0_col2\" class=\"col_heading level0 col2\" >Type Hint</th>\n",
       "      <th id=\"T_70314_level0_col3\" class=\"col_heading level0 col3\" >Default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_70314_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_70314_row0_col0\" class=\"data row0 col0\" >save_pretrained</td>\n",
       "      <td id=\"T_70314_row0_col1\" class=\"data row0 col1\" >self</td>\n",
       "      <td id=\"T_70314_row0_col2\" class=\"data row0 col2\" ></td>\n",
       "      <td id=\"T_70314_row0_col3\" class=\"data row0 col3\" >**Required**</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_70314_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_70314_row1_col0\" class=\"data row1 col0\" >save_pretrained</td>\n",
       "      <td id=\"T_70314_row1_col1\" class=\"data row1 col1\" >save_directory</td>\n",
       "      <td id=\"T_70314_row1_col2\" class=\"data row1 col2\" >Union[str, pathlib.Path]</td>\n",
       "      <td id=\"T_70314_row1_col3\" class=\"data row1 col3\" >**Required**</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_70314_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_70314_row2_col0\" class=\"data row2 col0\" >save_pretrained</td>\n",
       "      <td id=\"T_70314_row2_col1\" class=\"data row2 col1\" >config</td>\n",
       "      <td id=\"T_70314_row2_col2\" class=\"data row2 col2\" >Union[dict, huggingface_hub.hub_mixin.DataclassInstance, NoneType]</td>\n",
       "      <td id=\"T_70314_row2_col3\" class=\"data row2 col3\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_70314_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_70314_row3_col0\" class=\"data row3 col0\" >save_pretrained</td>\n",
       "      <td id=\"T_70314_row3_col1\" class=\"data row3 col1\" >repo_id</td>\n",
       "      <td id=\"T_70314_row3_col2\" class=\"data row3 col2\" >Optional[str]</td>\n",
       "      <td id=\"T_70314_row3_col3\" class=\"data row3 col3\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_70314_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_70314_row4_col0\" class=\"data row4 col0\" >save_pretrained</td>\n",
       "      <td id=\"T_70314_row4_col1\" class=\"data row4 col1\" >push_to_hub</td>\n",
       "      <td id=\"T_70314_row4_col2\" class=\"data row4 col2\" >bool</td>\n",
       "      <td id=\"T_70314_row4_col3\" class=\"data row4 col3\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_70314_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_70314_row5_col0\" class=\"data row5 col0\" >save_pretrained</td>\n",
       "      <td id=\"T_70314_row5_col1\" class=\"data row5 col1\" >model_card_kwargs</td>\n",
       "      <td id=\"T_70314_row5_col2\" class=\"data row5 col2\" >Optional[Dict[str, Any]]</td>\n",
       "      <td id=\"T_70314_row5_col3\" class=\"data row5 col3\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_70314_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_70314_row6_col0\" class=\"data row6 col0\" >save_pretrained</td>\n",
       "      <td id=\"T_70314_row6_col1\" class=\"data row6 col1\" >push_to_hub_kwargs</td>\n",
       "      <td id=\"T_70314_row6_col2\" class=\"data row6 col2\" ></td>\n",
       "      <td id=\"T_70314_row6_col3\" class=\"data row6 col3\" >**Required**</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2b1f7a5e790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- 4. Configuration ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_b1e5a_row0_col0, #T_b1e5a_row0_col1, #T_b1e5a_row0_col2, #T_b1e5a_row0_col3, #T_b1e5a_row1_col0, #T_b1e5a_row1_col1, #T_b1e5a_row1_col2, #T_b1e5a_row1_col3, #T_b1e5a_row2_col0, #T_b1e5a_row2_col1, #T_b1e5a_row2_col2, #T_b1e5a_row2_col3, #T_b1e5a_row3_col0, #T_b1e5a_row3_col1, #T_b1e5a_row3_col2, #T_b1e5a_row3_col3, #T_b1e5a_row4_col0, #T_b1e5a_row4_col1, #T_b1e5a_row4_col2, #T_b1e5a_row4_col3, #T_b1e5a_row5_col0, #T_b1e5a_row5_col1, #T_b1e5a_row5_col2, #T_b1e5a_row5_col3, #T_b1e5a_row6_col0, #T_b1e5a_row6_col1, #T_b1e5a_row6_col2, #T_b1e5a_row6_col3, #T_b1e5a_row7_col0, #T_b1e5a_row7_col1, #T_b1e5a_row7_col2, #T_b1e5a_row7_col3, #T_b1e5a_row8_col0, #T_b1e5a_row8_col1, #T_b1e5a_row8_col2, #T_b1e5a_row8_col3, #T_b1e5a_row9_col0, #T_b1e5a_row9_col1, #T_b1e5a_row9_col2, #T_b1e5a_row9_col3 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_b1e5a\">\n",
       "  <caption>Function: ForecastConfig</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_b1e5a_level0_col0\" class=\"col_heading level0 col0\" >Method/Class</th>\n",
       "      <th id=\"T_b1e5a_level0_col1\" class=\"col_heading level0 col1\" >Argument</th>\n",
       "      <th id=\"T_b1e5a_level0_col2\" class=\"col_heading level0 col2\" >Type Hint</th>\n",
       "      <th id=\"T_b1e5a_level0_col3\" class=\"col_heading level0 col3\" >Default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_b1e5a_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_b1e5a_row0_col0\" class=\"data row0 col0\" >ForecastConfig</td>\n",
       "      <td id=\"T_b1e5a_row0_col1\" class=\"data row0 col1\" >max_context</td>\n",
       "      <td id=\"T_b1e5a_row0_col2\" class=\"data row0 col2\" >int</td>\n",
       "      <td id=\"T_b1e5a_row0_col3\" class=\"data row0 col3\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b1e5a_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_b1e5a_row1_col0\" class=\"data row1 col0\" >ForecastConfig</td>\n",
       "      <td id=\"T_b1e5a_row1_col1\" class=\"data row1 col1\" >max_horizon</td>\n",
       "      <td id=\"T_b1e5a_row1_col2\" class=\"data row1 col2\" >int</td>\n",
       "      <td id=\"T_b1e5a_row1_col3\" class=\"data row1 col3\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b1e5a_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_b1e5a_row2_col0\" class=\"data row2 col0\" >ForecastConfig</td>\n",
       "      <td id=\"T_b1e5a_row2_col1\" class=\"data row2 col1\" >normalize_inputs</td>\n",
       "      <td id=\"T_b1e5a_row2_col2\" class=\"data row2 col2\" >bool</td>\n",
       "      <td id=\"T_b1e5a_row2_col3\" class=\"data row2 col3\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b1e5a_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_b1e5a_row3_col0\" class=\"data row3 col0\" >ForecastConfig</td>\n",
       "      <td id=\"T_b1e5a_row3_col1\" class=\"data row3 col1\" >window_size</td>\n",
       "      <td id=\"T_b1e5a_row3_col2\" class=\"data row3 col2\" >int</td>\n",
       "      <td id=\"T_b1e5a_row3_col3\" class=\"data row3 col3\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b1e5a_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_b1e5a_row4_col0\" class=\"data row4 col0\" >ForecastConfig</td>\n",
       "      <td id=\"T_b1e5a_row4_col1\" class=\"data row4 col1\" >per_core_batch_size</td>\n",
       "      <td id=\"T_b1e5a_row4_col2\" class=\"data row4 col2\" >int</td>\n",
       "      <td id=\"T_b1e5a_row4_col3\" class=\"data row4 col3\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b1e5a_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_b1e5a_row5_col0\" class=\"data row5 col0\" >ForecastConfig</td>\n",
       "      <td id=\"T_b1e5a_row5_col1\" class=\"data row5 col1\" >use_continuous_quantile_head</td>\n",
       "      <td id=\"T_b1e5a_row5_col2\" class=\"data row5 col2\" >bool</td>\n",
       "      <td id=\"T_b1e5a_row5_col3\" class=\"data row5 col3\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b1e5a_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_b1e5a_row6_col0\" class=\"data row6 col0\" >ForecastConfig</td>\n",
       "      <td id=\"T_b1e5a_row6_col1\" class=\"data row6 col1\" >force_flip_invariance</td>\n",
       "      <td id=\"T_b1e5a_row6_col2\" class=\"data row6 col2\" >bool</td>\n",
       "      <td id=\"T_b1e5a_row6_col3\" class=\"data row6 col3\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b1e5a_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_b1e5a_row7_col0\" class=\"data row7 col0\" >ForecastConfig</td>\n",
       "      <td id=\"T_b1e5a_row7_col1\" class=\"data row7 col1\" >infer_is_positive</td>\n",
       "      <td id=\"T_b1e5a_row7_col2\" class=\"data row7 col2\" >bool</td>\n",
       "      <td id=\"T_b1e5a_row7_col3\" class=\"data row7 col3\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b1e5a_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_b1e5a_row8_col0\" class=\"data row8 col0\" >ForecastConfig</td>\n",
       "      <td id=\"T_b1e5a_row8_col1\" class=\"data row8 col1\" >fix_quantile_crossing</td>\n",
       "      <td id=\"T_b1e5a_row8_col2\" class=\"data row8 col2\" >bool</td>\n",
       "      <td id=\"T_b1e5a_row8_col3\" class=\"data row8 col3\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b1e5a_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_b1e5a_row9_col0\" class=\"data row9 col0\" >ForecastConfig</td>\n",
       "      <td id=\"T_b1e5a_row9_col1\" class=\"data row9 col1\" >return_backcast</td>\n",
       "      <td id=\"T_b1e5a_row9_col2\" class=\"data row9 col2\" >bool</td>\n",
       "      <td id=\"T_b1e5a_row9_col3\" class=\"data row9 col3\" >False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2b1f7a5e790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- 5. Forecasting ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_49f18_row0_col0, #T_49f18_row0_col1, #T_49f18_row0_col2, #T_49f18_row0_col3, #T_49f18_row1_col0, #T_49f18_row1_col1, #T_49f18_row1_col2, #T_49f18_row1_col3, #T_49f18_row2_col0, #T_49f18_row2_col1, #T_49f18_row2_col2, #T_49f18_row2_col3 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_49f18\">\n",
       "  <caption>Function: forecast</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_49f18_level0_col0\" class=\"col_heading level0 col0\" >Method/Class</th>\n",
       "      <th id=\"T_49f18_level0_col1\" class=\"col_heading level0 col1\" >Argument</th>\n",
       "      <th id=\"T_49f18_level0_col2\" class=\"col_heading level0 col2\" >Type Hint</th>\n",
       "      <th id=\"T_49f18_level0_col3\" class=\"col_heading level0 col3\" >Default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_49f18_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_49f18_row0_col0\" class=\"data row0 col0\" >forecast</td>\n",
       "      <td id=\"T_49f18_row0_col1\" class=\"data row0 col1\" >self</td>\n",
       "      <td id=\"T_49f18_row0_col2\" class=\"data row0 col2\" ></td>\n",
       "      <td id=\"T_49f18_row0_col3\" class=\"data row0 col3\" >**Required**</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_49f18_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_49f18_row1_col0\" class=\"data row1 col0\" >forecast</td>\n",
       "      <td id=\"T_49f18_row1_col1\" class=\"data row1 col1\" >horizon</td>\n",
       "      <td id=\"T_49f18_row1_col2\" class=\"data row1 col2\" >int</td>\n",
       "      <td id=\"T_49f18_row1_col3\" class=\"data row1 col3\" >**Required**</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_49f18_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_49f18_row2_col0\" class=\"data row2 col0\" >forecast</td>\n",
       "      <td id=\"T_49f18_row2_col1\" class=\"data row2 col1\" >inputs</td>\n",
       "      <td id=\"T_49f18_row2_col2\" class=\"data row2 col2\" >list[numpy.ndarray]</td>\n",
       "      <td id=\"T_49f18_row2_col3\" class=\"data row2 col3\" >**Required**</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2b1f8447750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_d8fa6_row0_col0, #T_d8fa6_row0_col1, #T_d8fa6_row0_col2, #T_d8fa6_row0_col3, #T_d8fa6_row1_col0, #T_d8fa6_row1_col1, #T_d8fa6_row1_col2, #T_d8fa6_row1_col3, #T_d8fa6_row2_col0, #T_d8fa6_row2_col1, #T_d8fa6_row2_col2, #T_d8fa6_row2_col3, #T_d8fa6_row3_col0, #T_d8fa6_row3_col1, #T_d8fa6_row3_col2, #T_d8fa6_row3_col3, #T_d8fa6_row4_col0, #T_d8fa6_row4_col1, #T_d8fa6_row4_col2, #T_d8fa6_row4_col3, #T_d8fa6_row5_col0, #T_d8fa6_row5_col1, #T_d8fa6_row5_col2, #T_d8fa6_row5_col3, #T_d8fa6_row6_col0, #T_d8fa6_row6_col1, #T_d8fa6_row6_col2, #T_d8fa6_row6_col3, #T_d8fa6_row7_col0, #T_d8fa6_row7_col1, #T_d8fa6_row7_col2, #T_d8fa6_row7_col3, #T_d8fa6_row8_col0, #T_d8fa6_row8_col1, #T_d8fa6_row8_col2, #T_d8fa6_row8_col3, #T_d8fa6_row9_col0, #T_d8fa6_row9_col1, #T_d8fa6_row9_col2, #T_d8fa6_row9_col3, #T_d8fa6_row10_col0, #T_d8fa6_row10_col1, #T_d8fa6_row10_col2, #T_d8fa6_row10_col3 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_d8fa6\">\n",
       "  <caption>Function: forecast_with_covariates</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_d8fa6_level0_col0\" class=\"col_heading level0 col0\" >Method/Class</th>\n",
       "      <th id=\"T_d8fa6_level0_col1\" class=\"col_heading level0 col1\" >Argument</th>\n",
       "      <th id=\"T_d8fa6_level0_col2\" class=\"col_heading level0 col2\" >Type Hint</th>\n",
       "      <th id=\"T_d8fa6_level0_col3\" class=\"col_heading level0 col3\" >Default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_d8fa6_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_d8fa6_row0_col0\" class=\"data row0 col0\" >forecast_with_covariates</td>\n",
       "      <td id=\"T_d8fa6_row0_col1\" class=\"data row0 col1\" >self</td>\n",
       "      <td id=\"T_d8fa6_row0_col2\" class=\"data row0 col2\" ></td>\n",
       "      <td id=\"T_d8fa6_row0_col3\" class=\"data row0 col3\" >**Required**</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d8fa6_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_d8fa6_row1_col0\" class=\"data row1 col0\" >forecast_with_covariates</td>\n",
       "      <td id=\"T_d8fa6_row1_col1\" class=\"data row1 col1\" >inputs</td>\n",
       "      <td id=\"T_d8fa6_row1_col2\" class=\"data row1 col2\" >list[Sequence[float]]</td>\n",
       "      <td id=\"T_d8fa6_row1_col3\" class=\"data row1 col3\" >**Required**</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d8fa6_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_d8fa6_row2_col0\" class=\"data row2 col0\" >forecast_with_covariates</td>\n",
       "      <td id=\"T_d8fa6_row2_col1\" class=\"data row2 col1\" >dynamic_numerical_covariates</td>\n",
       "      <td id=\"T_d8fa6_row2_col2\" class=\"data row2 col2\" >dict[str, Sequence[Sequence[float]]] | None</td>\n",
       "      <td id=\"T_d8fa6_row2_col3\" class=\"data row2 col3\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d8fa6_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_d8fa6_row3_col0\" class=\"data row3 col0\" >forecast_with_covariates</td>\n",
       "      <td id=\"T_d8fa6_row3_col1\" class=\"data row3 col1\" >dynamic_categorical_covariates</td>\n",
       "      <td id=\"T_d8fa6_row3_col2\" class=\"data row3 col2\" >dict[str, Sequence[Sequence[int | str]]] | None</td>\n",
       "      <td id=\"T_d8fa6_row3_col3\" class=\"data row3 col3\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d8fa6_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_d8fa6_row4_col0\" class=\"data row4 col0\" >forecast_with_covariates</td>\n",
       "      <td id=\"T_d8fa6_row4_col1\" class=\"data row4 col1\" >static_numerical_covariates</td>\n",
       "      <td id=\"T_d8fa6_row4_col2\" class=\"data row4 col2\" >dict[str, Sequence[float]] | None</td>\n",
       "      <td id=\"T_d8fa6_row4_col3\" class=\"data row4 col3\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d8fa6_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_d8fa6_row5_col0\" class=\"data row5 col0\" >forecast_with_covariates</td>\n",
       "      <td id=\"T_d8fa6_row5_col1\" class=\"data row5 col1\" >static_categorical_covariates</td>\n",
       "      <td id=\"T_d8fa6_row5_col2\" class=\"data row5 col2\" >dict[str, Sequence[int | str]] | None</td>\n",
       "      <td id=\"T_d8fa6_row5_col3\" class=\"data row5 col3\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d8fa6_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_d8fa6_row6_col0\" class=\"data row6 col0\" >forecast_with_covariates</td>\n",
       "      <td id=\"T_d8fa6_row6_col1\" class=\"data row6 col1\" >xreg_mode</td>\n",
       "      <td id=\"T_d8fa6_row6_col2\" class=\"data row6 col2\" >str</td>\n",
       "      <td id=\"T_d8fa6_row6_col3\" class=\"data row6 col3\" >xreg + timesfm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d8fa6_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_d8fa6_row7_col0\" class=\"data row7 col0\" >forecast_with_covariates</td>\n",
       "      <td id=\"T_d8fa6_row7_col1\" class=\"data row7 col1\" >normalize_xreg_target_per_input</td>\n",
       "      <td id=\"T_d8fa6_row7_col2\" class=\"data row7 col2\" >bool</td>\n",
       "      <td id=\"T_d8fa6_row7_col3\" class=\"data row7 col3\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d8fa6_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_d8fa6_row8_col0\" class=\"data row8 col0\" >forecast_with_covariates</td>\n",
       "      <td id=\"T_d8fa6_row8_col1\" class=\"data row8 col1\" >ridge</td>\n",
       "      <td id=\"T_d8fa6_row8_col2\" class=\"data row8 col2\" >float</td>\n",
       "      <td id=\"T_d8fa6_row8_col3\" class=\"data row8 col3\" >0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d8fa6_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_d8fa6_row9_col0\" class=\"data row9 col0\" >forecast_with_covariates</td>\n",
       "      <td id=\"T_d8fa6_row9_col1\" class=\"data row9 col1\" >max_rows_per_col</td>\n",
       "      <td id=\"T_d8fa6_row9_col2\" class=\"data row9 col2\" >int</td>\n",
       "      <td id=\"T_d8fa6_row9_col3\" class=\"data row9 col3\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d8fa6_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_d8fa6_row10_col0\" class=\"data row10 col0\" >forecast_with_covariates</td>\n",
       "      <td id=\"T_d8fa6_row10_col1\" class=\"data row10 col1\" >force_on_cpu</td>\n",
       "      <td id=\"T_d8fa6_row10_col2\" class=\"data row10 col2\" >bool</td>\n",
       "      <td id=\"T_d8fa6_row10_col3\" class=\"data row10 col3\" >False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2b20184ab90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- 6. Compilation ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_5a78a_row0_col0, #T_5a78a_row0_col1, #T_5a78a_row0_col2, #T_5a78a_row0_col3, #T_5a78a_row1_col0, #T_5a78a_row1_col1, #T_5a78a_row1_col2, #T_5a78a_row1_col3, #T_5a78a_row2_col0, #T_5a78a_row2_col1, #T_5a78a_row2_col2, #T_5a78a_row2_col3 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_5a78a\">\n",
       "  <caption>Function: compile</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_5a78a_level0_col0\" class=\"col_heading level0 col0\" >Method/Class</th>\n",
       "      <th id=\"T_5a78a_level0_col1\" class=\"col_heading level0 col1\" >Argument</th>\n",
       "      <th id=\"T_5a78a_level0_col2\" class=\"col_heading level0 col2\" >Type Hint</th>\n",
       "      <th id=\"T_5a78a_level0_col3\" class=\"col_heading level0 col3\" >Default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_5a78a_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_5a78a_row0_col0\" class=\"data row0 col0\" >compile</td>\n",
       "      <td id=\"T_5a78a_row0_col1\" class=\"data row0 col1\" >self</td>\n",
       "      <td id=\"T_5a78a_row0_col2\" class=\"data row0 col2\" ></td>\n",
       "      <td id=\"T_5a78a_row0_col3\" class=\"data row0 col3\" >**Required**</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5a78a_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_5a78a_row1_col0\" class=\"data row1 col0\" >compile</td>\n",
       "      <td id=\"T_5a78a_row1_col1\" class=\"data row1 col1\" >forecast_config</td>\n",
       "      <td id=\"T_5a78a_row1_col2\" class=\"data row1 col2\" >timesfm.configs.ForecastConfig</td>\n",
       "      <td id=\"T_5a78a_row1_col3\" class=\"data row1 col3\" >**Required**</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5a78a_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_5a78a_row2_col0\" class=\"data row2 col0\" >compile</td>\n",
       "      <td id=\"T_5a78a_row2_col1\" class=\"data row2 col1\" >kwargs</td>\n",
       "      <td id=\"T_5a78a_row2_col2\" class=\"data row2 col2\" ></td>\n",
       "      <td id=\"T_5a78a_row2_col3\" class=\"data row2 col3\" >**Required**</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2b2018aff90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "import pandas as pd\n",
    "import timesfm\n",
    "\n",
    "# 解析対象のクラスやメソッドを定義\n",
    "# (必要に応じてクラス名を timesfm.TimesFM_2p5_200M_torch などに変更してください)\n",
    "try:\n",
    "    # メインクラスの特定（バージョンによって異なる場合があるため）\n",
    "    TargetClass = timesfm.TimesFM_2p5_200M_torch \n",
    "except AttributeError:\n",
    "    try:\n",
    "        TargetClass = timesfm.TimesFM\n",
    "    except AttributeError:\n",
    "        print(\"Error: Could not find the main TimesFM class. Please check imports.\")\n",
    "        TargetClass = None\n",
    "\n",
    "# 解析したい機能のリスト (カテゴリ, メソッド/クラス)\n",
    "targets = {\n",
    "    \"1. Model Initialization\": [TargetClass],  # __init__\n",
    "    \"2. Loading\": [TargetClass.from_pretrained, TargetClass.load_checkpoint],\n",
    "    \"3. Saving\": [TargetClass.save_pretrained],\n",
    "    \"4. Configuration\": [timesfm.configs.ForecastConfig],\n",
    "    \"5. Forecasting\": [TargetClass.forecast, TargetClass.forecast_with_covariates],\n",
    "    \"6. Compilation\": [TargetClass.compile],\n",
    "}\n",
    "\n",
    "def analyze_signature(func_or_class):\n",
    "    \"\"\"関数またはクラスのシグネチャを解析してDataFrameを返す\"\"\"\n",
    "    if func_or_class is None:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    name = func_or_class.__name__\n",
    "    try:\n",
    "        sig = inspect.signature(func_or_class)\n",
    "    except ValueError:\n",
    "        return pd.DataFrame([{\"Arg\": \"Error\", \"Type\": \"N/A\", \"Default\": \"Could not inspect\"}])\n",
    "\n",
    "    arg_data = []\n",
    "    for param_name, param in sig.parameters.items():\n",
    "        # 型ヒントの整形\n",
    "        type_hint = param.annotation\n",
    "        if type_hint == inspect.Parameter.empty:\n",
    "            type_hint = \"\"\n",
    "        else:\n",
    "            type_hint = str(type_hint).replace(\"typing.\", \"\").replace(\"<class '\", \"\").replace(\"'>\", \"\")\n",
    "        \n",
    "        # デフォルト値の整形\n",
    "        default_val = param.default\n",
    "        if default_val == inspect.Parameter.empty:\n",
    "            default_val = \"**Required**\"\n",
    "        \n",
    "        arg_data.append({\n",
    "            \"Method/Class\": name,\n",
    "            \"Argument\": param_name,\n",
    "            \"Type Hint\": type_hint,\n",
    "            \"Default\": str(default_val)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(arg_data)\n",
    "\n",
    "# 一括実行と表示\n",
    "print(f\"=== TimesFM API Inspection (Target: {TargetClass.__name__}) ===\\n\")\n",
    "\n",
    "all_dfs = []\n",
    "\n",
    "for category, func_list in targets.items():\n",
    "    print(f\"--- {category} ---\")\n",
    "    for func in func_list:\n",
    "        df = analyze_signature(func)\n",
    "        if not df.empty:\n",
    "            # 見やすく表示\n",
    "            display(df.style.set_caption(f\"Function: {func.__name__}\").set_properties(**{'text-align': 'left'}))\n",
    "            all_dfs.append(df)\n",
    "        else:\n",
    "            print(f\"Skipping {func.__name__}: No signature found.\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d7691e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Class: TimesFM_2p5_200M_torch\n",
      "Total Arguments Found: 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Function/Class</th>\n",
       "      <th>Argument</th>\n",
       "      <th>Type Hint</th>\n",
       "      <th>Default</th>\n",
       "      <th>Docstring Hint</th>\n",
       "      <th>Verified Range/Options</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1. Initialization</td>\n",
       "      <td>TimesFM_2p5_200M_torch</td>\n",
       "      <td>args</td>\n",
       "      <td>Any</td>\n",
       "      <td>**Required**</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1. Initialization</td>\n",
       "      <td>TimesFM_2p5_200M_torch</td>\n",
       "      <td>kwargs</td>\n",
       "      <td>Any</td>\n",
       "      <td>**Required**</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2. Loading</td>\n",
       "      <td>from_pretrained</td>\n",
       "      <td>pretrained_model_name_or_path</td>\n",
       "      <td>Union[str, pathlib.Path]</td>\n",
       "      <td>**Required**</td>\n",
       "      <td>pretrained_model_name_or_path (`str`, `Path`): - Either the `model_id` (string) of a model hoste...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2. Loading</td>\n",
       "      <td>from_pretrained</td>\n",
       "      <td>force_download</td>\n",
       "      <td>bool</td>\n",
       "      <td>False</td>\n",
       "      <td>force_download (`bool`, *optional*, defaults to `False`): Whether to force (re-)downloading the ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2. Loading</td>\n",
       "      <td>from_pretrained</td>\n",
       "      <td>resume_download</td>\n",
       "      <td>Optional[bool]</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2. Loading</td>\n",
       "      <td>from_pretrained</td>\n",
       "      <td>proxies</td>\n",
       "      <td>Optional[Dict]</td>\n",
       "      <td>None</td>\n",
       "      <td>proxies (`Dict[str, str]`, *optional*): A dictionary of proxy servers to use by protocol or endp...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2. Loading</td>\n",
       "      <td>from_pretrained</td>\n",
       "      <td>token</td>\n",
       "      <td>Union[bool, str, NoneType]</td>\n",
       "      <td>None</td>\n",
       "      <td>token (`str` or `bool`, *optional*): The token to use as HTTP bearer authorization for remote fi...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2. Loading</td>\n",
       "      <td>from_pretrained</td>\n",
       "      <td>cache_dir</td>\n",
       "      <td>Union[str, pathlib.Path, NoneType]</td>\n",
       "      <td>None</td>\n",
       "      <td>cache_dir (`str`, `Path`, *optional*): Path to the folder where cached files are stored. local_f...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2. Loading</td>\n",
       "      <td>from_pretrained</td>\n",
       "      <td>local_files_only</td>\n",
       "      <td>bool</td>\n",
       "      <td>False</td>\n",
       "      <td>local_files_only (`bool`, *optional*, defaults to `False`): If `True`, avoid downloading the fil...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2. Loading</td>\n",
       "      <td>from_pretrained</td>\n",
       "      <td>revision</td>\n",
       "      <td>Optional[str]</td>\n",
       "      <td>None</td>\n",
       "      <td>revision (`str`, *optional*): Revision of the model on the Hub. Can be a branch name, a git tag ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2. Loading</td>\n",
       "      <td>from_pretrained</td>\n",
       "      <td>model_kwargs</td>\n",
       "      <td>Any</td>\n",
       "      <td>**Required**</td>\n",
       "      <td>model_kwargs (`Dict`, *optional*): Additional kwargs to pass to the model during initialization.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2. Loading</td>\n",
       "      <td>load_checkpoint</td>\n",
       "      <td>path</td>\n",
       "      <td>str</td>\n",
       "      <td>**Required**</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3. Configuration</td>\n",
       "      <td>ForecastConfig</td>\n",
       "      <td>max_context</td>\n",
       "      <td>int</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3. Configuration</td>\n",
       "      <td>ForecastConfig</td>\n",
       "      <td>max_horizon</td>\n",
       "      <td>int</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3. Configuration</td>\n",
       "      <td>ForecastConfig</td>\n",
       "      <td>normalize_inputs</td>\n",
       "      <td>bool</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3. Configuration</td>\n",
       "      <td>ForecastConfig</td>\n",
       "      <td>window_size</td>\n",
       "      <td>int</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3. Configuration</td>\n",
       "      <td>ForecastConfig</td>\n",
       "      <td>per_core_batch_size</td>\n",
       "      <td>int</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3. Configuration</td>\n",
       "      <td>ForecastConfig</td>\n",
       "      <td>use_continuous_quantile_head</td>\n",
       "      <td>bool</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3. Configuration</td>\n",
       "      <td>ForecastConfig</td>\n",
       "      <td>force_flip_invariance</td>\n",
       "      <td>bool</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3. Configuration</td>\n",
       "      <td>ForecastConfig</td>\n",
       "      <td>infer_is_positive</td>\n",
       "      <td>bool</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3. Configuration</td>\n",
       "      <td>ForecastConfig</td>\n",
       "      <td>fix_quantile_crossing</td>\n",
       "      <td>bool</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3. Configuration</td>\n",
       "      <td>ForecastConfig</td>\n",
       "      <td>return_backcast</td>\n",
       "      <td>bool</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4. Compilation</td>\n",
       "      <td>compile</td>\n",
       "      <td>forecast_config</td>\n",
       "      <td>timesfm.configs.ForecastConfig</td>\n",
       "      <td>**Required**</td>\n",
       "      <td>forecast_config: Configuration for forecasting flags. **kwargs: Additional keyword arguments to ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4. Compilation</td>\n",
       "      <td>compile</td>\n",
       "      <td>kwargs</td>\n",
       "      <td>Any</td>\n",
       "      <td>**Required**</td>\n",
       "      <td>**kwargs: Additional keyword arguments to pass to model.compile().</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5. Forecasting</td>\n",
       "      <td>forecast</td>\n",
       "      <td>horizon</td>\n",
       "      <td>int</td>\n",
       "      <td>**Required**</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5. Forecasting</td>\n",
       "      <td>forecast</td>\n",
       "      <td>inputs</td>\n",
       "      <td>list[numpy.ndarray]</td>\n",
       "      <td>**Required**</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5. Forecasting</td>\n",
       "      <td>forecast_with_covariates</td>\n",
       "      <td>inputs</td>\n",
       "      <td>list[Sequence[float]]</td>\n",
       "      <td>**Required**</td>\n",
       "      <td>inputs: A list of time series forecast contexts. Each context time series should be in a format ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5. Forecasting</td>\n",
       "      <td>forecast_with_covariates</td>\n",
       "      <td>dynamic_numerical_covariates</td>\n",
       "      <td>dict[str, Sequence[Sequence[float]]] | None</td>\n",
       "      <td>None</td>\n",
       "      <td>dynamic_numerical_covariates: A dict of dynamic numerical covariates. dynamic_categorical_covari...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5. Forecasting</td>\n",
       "      <td>forecast_with_covariates</td>\n",
       "      <td>dynamic_categorical_covariates</td>\n",
       "      <td>dict[str, Sequence[Sequence[int | str]]] | None</td>\n",
       "      <td>None</td>\n",
       "      <td>dynamic_categorical_covariates: A dict of dynamic categorical covariates. static_numerical_covar...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5. Forecasting</td>\n",
       "      <td>forecast_with_covariates</td>\n",
       "      <td>static_numerical_covariates</td>\n",
       "      <td>dict[str, Sequence[float]] | None</td>\n",
       "      <td>None</td>\n",
       "      <td>static_numerical_covariates: A dict of static numerical covariates. static_categorical_covariate...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>5. Forecasting</td>\n",
       "      <td>forecast_with_covariates</td>\n",
       "      <td>static_categorical_covariates</td>\n",
       "      <td>dict[str, Sequence[int | str]] | None</td>\n",
       "      <td>None</td>\n",
       "      <td>static_categorical_covariates: A dict of static categorical covariates. xreg_mode: one of \"xreg ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>5. Forecasting</td>\n",
       "      <td>forecast_with_covariates</td>\n",
       "      <td>xreg_mode</td>\n",
       "      <td>str</td>\n",
       "      <td>xreg + timesfm</td>\n",
       "      <td>xreg_mode: one of \"xreg + timesfm\" or \"timesfm + xreg\". \"xreg + timesfm\" fits a model on the res...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>5. Forecasting</td>\n",
       "      <td>forecast_with_covariates</td>\n",
       "      <td>normalize_xreg_target_per_input</td>\n",
       "      <td>bool</td>\n",
       "      <td>True</td>\n",
       "      <td>normalize_xreg_target_per_input: whether to normalize the xreg target per input in the given bat...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>5. Forecasting</td>\n",
       "      <td>forecast_with_covariates</td>\n",
       "      <td>ridge</td>\n",
       "      <td>float</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ridge: ridge penalty for the linear model. max_rows_per_col: max number of rows per column for t...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>5. Forecasting</td>\n",
       "      <td>forecast_with_covariates</td>\n",
       "      <td>max_rows_per_col</td>\n",
       "      <td>int</td>\n",
       "      <td>0</td>\n",
       "      <td>max_rows_per_col: max number of rows per column for the linear model. force_on_cpu: whether to f...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>5. Forecasting</td>\n",
       "      <td>forecast_with_covariates</td>\n",
       "      <td>force_on_cpu</td>\n",
       "      <td>bool</td>\n",
       "      <td>False</td>\n",
       "      <td>force_on_cpu: whether to force running on cpu for the linear model. Returns:</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>6. Saving</td>\n",
       "      <td>save_pretrained</td>\n",
       "      <td>save_directory</td>\n",
       "      <td>Union[str, pathlib.Path]</td>\n",
       "      <td>**Required**</td>\n",
       "      <td>save_directory (`str` or `Path`): Path to directory in which the model weights and configuration...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>6. Saving</td>\n",
       "      <td>save_pretrained</td>\n",
       "      <td>config</td>\n",
       "      <td>Union[dict, huggingface_hub.hub_mixin.DataclassInstance, NoneType]</td>\n",
       "      <td>None</td>\n",
       "      <td>Path to directory in which the model weights and configuration will be saved. config (`dict` or ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>6. Saving</td>\n",
       "      <td>save_pretrained</td>\n",
       "      <td>repo_id</td>\n",
       "      <td>Optional[str]</td>\n",
       "      <td>None</td>\n",
       "      <td>repo_id (`str`, *optional*): ID of your repository on the Hub. Used only if `push_to_hub=True`. ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>6. Saving</td>\n",
       "      <td>save_pretrained</td>\n",
       "      <td>push_to_hub</td>\n",
       "      <td>bool</td>\n",
       "      <td>False</td>\n",
       "      <td>push_to_hub (`bool`, *optional*, defaults to `False`): Whether or not to push your model to the ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>6. Saving</td>\n",
       "      <td>save_pretrained</td>\n",
       "      <td>model_card_kwargs</td>\n",
       "      <td>Optional[Dict[str, Any]]</td>\n",
       "      <td>None</td>\n",
       "      <td>model_card_kwargs (`Dict[str, Any]`, *optional*): Additional arguments passed to the model card ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>6. Saving</td>\n",
       "      <td>save_pretrained</td>\n",
       "      <td>push_to_hub_kwargs</td>\n",
       "      <td>Any</td>\n",
       "      <td>**Required**</td>\n",
       "      <td>push_to_hub_kwargs: Additional key word arguments passed along to the [`~ModelHubMixin.push_to_h...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Category            Function/Class  \\\n",
       "0   1. Initialization    TimesFM_2p5_200M_torch   \n",
       "1   1. Initialization    TimesFM_2p5_200M_torch   \n",
       "2          2. Loading           from_pretrained   \n",
       "3          2. Loading           from_pretrained   \n",
       "4          2. Loading           from_pretrained   \n",
       "5          2. Loading           from_pretrained   \n",
       "6          2. Loading           from_pretrained   \n",
       "7          2. Loading           from_pretrained   \n",
       "8          2. Loading           from_pretrained   \n",
       "9          2. Loading           from_pretrained   \n",
       "10         2. Loading           from_pretrained   \n",
       "11         2. Loading           load_checkpoint   \n",
       "12   3. Configuration            ForecastConfig   \n",
       "13   3. Configuration            ForecastConfig   \n",
       "14   3. Configuration            ForecastConfig   \n",
       "15   3. Configuration            ForecastConfig   \n",
       "16   3. Configuration            ForecastConfig   \n",
       "17   3. Configuration            ForecastConfig   \n",
       "18   3. Configuration            ForecastConfig   \n",
       "19   3. Configuration            ForecastConfig   \n",
       "20   3. Configuration            ForecastConfig   \n",
       "21   3. Configuration            ForecastConfig   \n",
       "22     4. Compilation                   compile   \n",
       "23     4. Compilation                   compile   \n",
       "24     5. Forecasting                  forecast   \n",
       "25     5. Forecasting                  forecast   \n",
       "26     5. Forecasting  forecast_with_covariates   \n",
       "27     5. Forecasting  forecast_with_covariates   \n",
       "28     5. Forecasting  forecast_with_covariates   \n",
       "29     5. Forecasting  forecast_with_covariates   \n",
       "30     5. Forecasting  forecast_with_covariates   \n",
       "31     5. Forecasting  forecast_with_covariates   \n",
       "32     5. Forecasting  forecast_with_covariates   \n",
       "33     5. Forecasting  forecast_with_covariates   \n",
       "34     5. Forecasting  forecast_with_covariates   \n",
       "35     5. Forecasting  forecast_with_covariates   \n",
       "36          6. Saving           save_pretrained   \n",
       "37          6. Saving           save_pretrained   \n",
       "38          6. Saving           save_pretrained   \n",
       "39          6. Saving           save_pretrained   \n",
       "40          6. Saving           save_pretrained   \n",
       "41          6. Saving           save_pretrained   \n",
       "\n",
       "                           Argument  \\\n",
       "0                              args   \n",
       "1                            kwargs   \n",
       "2     pretrained_model_name_or_path   \n",
       "3                    force_download   \n",
       "4                   resume_download   \n",
       "5                           proxies   \n",
       "6                             token   \n",
       "7                         cache_dir   \n",
       "8                  local_files_only   \n",
       "9                          revision   \n",
       "10                     model_kwargs   \n",
       "11                             path   \n",
       "12                      max_context   \n",
       "13                      max_horizon   \n",
       "14                 normalize_inputs   \n",
       "15                      window_size   \n",
       "16              per_core_batch_size   \n",
       "17     use_continuous_quantile_head   \n",
       "18            force_flip_invariance   \n",
       "19                infer_is_positive   \n",
       "20            fix_quantile_crossing   \n",
       "21                  return_backcast   \n",
       "22                  forecast_config   \n",
       "23                           kwargs   \n",
       "24                          horizon   \n",
       "25                           inputs   \n",
       "26                           inputs   \n",
       "27     dynamic_numerical_covariates   \n",
       "28   dynamic_categorical_covariates   \n",
       "29      static_numerical_covariates   \n",
       "30    static_categorical_covariates   \n",
       "31                        xreg_mode   \n",
       "32  normalize_xreg_target_per_input   \n",
       "33                            ridge   \n",
       "34                 max_rows_per_col   \n",
       "35                     force_on_cpu   \n",
       "36                   save_directory   \n",
       "37                           config   \n",
       "38                          repo_id   \n",
       "39                      push_to_hub   \n",
       "40                model_card_kwargs   \n",
       "41               push_to_hub_kwargs   \n",
       "\n",
       "                                                             Type Hint  \\\n",
       "0                                                                  Any   \n",
       "1                                                                  Any   \n",
       "2                                             Union[str, pathlib.Path]   \n",
       "3                                                                 bool   \n",
       "4                                                       Optional[bool]   \n",
       "5                                                       Optional[Dict]   \n",
       "6                                           Union[bool, str, NoneType]   \n",
       "7                                   Union[str, pathlib.Path, NoneType]   \n",
       "8                                                                 bool   \n",
       "9                                                        Optional[str]   \n",
       "10                                                                 Any   \n",
       "11                                                                 str   \n",
       "12                                                                 int   \n",
       "13                                                                 int   \n",
       "14                                                                bool   \n",
       "15                                                                 int   \n",
       "16                                                                 int   \n",
       "17                                                                bool   \n",
       "18                                                                bool   \n",
       "19                                                                bool   \n",
       "20                                                                bool   \n",
       "21                                                                bool   \n",
       "22                                      timesfm.configs.ForecastConfig   \n",
       "23                                                                 Any   \n",
       "24                                                                 int   \n",
       "25                                                 list[numpy.ndarray]   \n",
       "26                                               list[Sequence[float]]   \n",
       "27                         dict[str, Sequence[Sequence[float]]] | None   \n",
       "28                     dict[str, Sequence[Sequence[int | str]]] | None   \n",
       "29                                   dict[str, Sequence[float]] | None   \n",
       "30                               dict[str, Sequence[int | str]] | None   \n",
       "31                                                                 str   \n",
       "32                                                                bool   \n",
       "33                                                               float   \n",
       "34                                                                 int   \n",
       "35                                                                bool   \n",
       "36                                            Union[str, pathlib.Path]   \n",
       "37  Union[dict, huggingface_hub.hub_mixin.DataclassInstance, NoneType]   \n",
       "38                                                       Optional[str]   \n",
       "39                                                                bool   \n",
       "40                                            Optional[Dict[str, Any]]   \n",
       "41                                                                 Any   \n",
       "\n",
       "           Default  \\\n",
       "0     **Required**   \n",
       "1     **Required**   \n",
       "2     **Required**   \n",
       "3            False   \n",
       "4             None   \n",
       "5             None   \n",
       "6             None   \n",
       "7             None   \n",
       "8            False   \n",
       "9             None   \n",
       "10    **Required**   \n",
       "11    **Required**   \n",
       "12               0   \n",
       "13               0   \n",
       "14           False   \n",
       "15               0   \n",
       "16               1   \n",
       "17           False   \n",
       "18            True   \n",
       "19            True   \n",
       "20           False   \n",
       "21           False   \n",
       "22    **Required**   \n",
       "23    **Required**   \n",
       "24    **Required**   \n",
       "25    **Required**   \n",
       "26    **Required**   \n",
       "27            None   \n",
       "28            None   \n",
       "29            None   \n",
       "30            None   \n",
       "31  xreg + timesfm   \n",
       "32            True   \n",
       "33             0.0   \n",
       "34               0   \n",
       "35           False   \n",
       "36    **Required**   \n",
       "37            None   \n",
       "38            None   \n",
       "39           False   \n",
       "40            None   \n",
       "41    **Required**   \n",
       "\n",
       "                                                                                         Docstring Hint  \\\n",
       "0                                                                                                         \n",
       "1                                                                                                         \n",
       "2   pretrained_model_name_or_path (`str`, `Path`): - Either the `model_id` (string) of a model hoste...   \n",
       "3   force_download (`bool`, *optional*, defaults to `False`): Whether to force (re-)downloading the ...   \n",
       "4                                                                                                         \n",
       "5   proxies (`Dict[str, str]`, *optional*): A dictionary of proxy servers to use by protocol or endp...   \n",
       "6   token (`str` or `bool`, *optional*): The token to use as HTTP bearer authorization for remote fi...   \n",
       "7   cache_dir (`str`, `Path`, *optional*): Path to the folder where cached files are stored. local_f...   \n",
       "8   local_files_only (`bool`, *optional*, defaults to `False`): If `True`, avoid downloading the fil...   \n",
       "9   revision (`str`, *optional*): Revision of the model on the Hub. Can be a branch name, a git tag ...   \n",
       "10     model_kwargs (`Dict`, *optional*): Additional kwargs to pass to the model during initialization.   \n",
       "11                                                                                                        \n",
       "12                                                                                                        \n",
       "13                                                                                                        \n",
       "14                                                                                                        \n",
       "15                                                                                                        \n",
       "16                                                                                                        \n",
       "17                                                                                                        \n",
       "18                                                                                                        \n",
       "19                                                                                                        \n",
       "20                                                                                                        \n",
       "21                                                                                                        \n",
       "22  forecast_config: Configuration for forecasting flags. **kwargs: Additional keyword arguments to ...   \n",
       "23                                  **kwargs: Additional keyword arguments to pass to model.compile().    \n",
       "24                                                                                                        \n",
       "25                                                                                                        \n",
       "26  inputs: A list of time series forecast contexts. Each context time series should be in a format ...   \n",
       "27  dynamic_numerical_covariates: A dict of dynamic numerical covariates. dynamic_categorical_covari...   \n",
       "28  dynamic_categorical_covariates: A dict of dynamic categorical covariates. static_numerical_covar...   \n",
       "29  static_numerical_covariates: A dict of static numerical covariates. static_categorical_covariate...   \n",
       "30  static_categorical_covariates: A dict of static categorical covariates. xreg_mode: one of \"xreg ...   \n",
       "31  xreg_mode: one of \"xreg + timesfm\" or \"timesfm + xreg\". \"xreg + timesfm\" fits a model on the res...   \n",
       "32  normalize_xreg_target_per_input: whether to normalize the xreg target per input in the given bat...   \n",
       "33  ridge: ridge penalty for the linear model. max_rows_per_col: max number of rows per column for t...   \n",
       "34  max_rows_per_col: max number of rows per column for the linear model. force_on_cpu: whether to f...   \n",
       "35                         force_on_cpu: whether to force running on cpu for the linear model. Returns:   \n",
       "36  save_directory (`str` or `Path`): Path to directory in which the model weights and configuration...   \n",
       "37  Path to directory in which the model weights and configuration will be saved. config (`dict` or ...   \n",
       "38  repo_id (`str`, *optional*): ID of your repository on the Hub. Used only if `push_to_hub=True`. ...   \n",
       "39  push_to_hub (`bool`, *optional*, defaults to `False`): Whether or not to push your model to the ...   \n",
       "40  model_card_kwargs (`Dict[str, Any]`, *optional*): Additional arguments passed to the model card ...   \n",
       "41  push_to_hub_kwargs: Additional key word arguments passed along to the [`~ModelHubMixin.push_to_h...   \n",
       "\n",
       "   Verified Range/Options  \n",
       "0                          \n",
       "1                          \n",
       "2                          \n",
       "3                          \n",
       "4                          \n",
       "5                          \n",
       "6                          \n",
       "7                          \n",
       "8                          \n",
       "9                          \n",
       "10                         \n",
       "11                         \n",
       "12                         \n",
       "13                         \n",
       "14                         \n",
       "15                         \n",
       "16                         \n",
       "17                         \n",
       "18                         \n",
       "19                         \n",
       "20                         \n",
       "21                         \n",
       "22                         \n",
       "23                         \n",
       "24                         \n",
       "25                         \n",
       "26                         \n",
       "27                         \n",
       "28                         \n",
       "29                         \n",
       "30                         \n",
       "31                         \n",
       "32                         \n",
       "33                         \n",
       "34                         \n",
       "35                         \n",
       "36                         \n",
       "37                         \n",
       "38                         \n",
       "39                         \n",
       "40                         \n",
       "41                         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Table saved to: timesfm_api_verification_table.csv\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "import pandas as pd\n",
    "import timesfm\n",
    "import re\n",
    "\n",
    "# ==========================================\n",
    "# 1. 解析対象の定義\n",
    "# ==========================================\n",
    "# 環境に合わせてクラスを取得 (v2.5系を優先)\n",
    "try:\n",
    "    MainClass = timesfm.TimesFM_2p5_200M_torch\n",
    "    print(f\"Target Class: {MainClass.__name__}\")\n",
    "except AttributeError:\n",
    "    try:\n",
    "        MainClass = timesfm.TimesFM\n",
    "        print(f\"Target Class: {MainClass.__name__}\")\n",
    "    except AttributeError:\n",
    "        print(\"Error: Could not find TimesFM class.\")\n",
    "        MainClass = None\n",
    "\n",
    "# 解析したいメソッドやクラスのリスト\n",
    "targets = {\n",
    "    \"1. Initialization\": [MainClass], # __init__\n",
    "    \"2. Loading\": [MainClass.from_pretrained, MainClass.load_checkpoint],\n",
    "    \"3. Configuration\": [timesfm.configs.ForecastConfig],\n",
    "    \"4. Compilation\": [MainClass.compile],\n",
    "    \"5. Forecasting\": [MainClass.forecast, MainClass.forecast_with_covariates],\n",
    "    \"6. Saving\": [MainClass.save_pretrained],\n",
    "}\n",
    "\n",
    "# ==========================================\n",
    "# 2. 解析関数の定義\n",
    "# ==========================================\n",
    "def extract_doc_info(docstring, arg_name):\n",
    "    \"\"\"Docstringから引数に関する説明や制約を簡易抽出する\"\"\"\n",
    "    if not docstring:\n",
    "        return \"\"\n",
    "    \n",
    "    # 引数名が含まれる行を探す（簡易的な検索）\n",
    "    lines = docstring.split('\\n')\n",
    "    for i, line in enumerate(lines):\n",
    "        if arg_name in line:\n",
    "            # その行と次の数行を取得して説明とする\n",
    "            desc = line.strip() + \" \" + \" \".join([l.strip() for l in lines[i+1:i+3] if l.strip()])\n",
    "            return desc[:200] + \"...\" if len(desc) > 200 else desc\n",
    "    return \"\"\n",
    "\n",
    "def analyze_arguments(category, func_or_class):\n",
    "    \"\"\"関数/クラスのシグネチャとドキュメントを解析して辞書リストを返す\"\"\"\n",
    "    if func_or_class is None:\n",
    "        return []\n",
    "\n",
    "    name = func_or_class.__name__\n",
    "    \n",
    "    # クラスの場合は __init__ を解析対象にする\n",
    "    if inspect.isclass(func_or_class):\n",
    "        target_func = func_or_class.__init__\n",
    "        is_class = True\n",
    "    else:\n",
    "        target_func = func_or_class\n",
    "        is_class = False\n",
    "\n",
    "    try:\n",
    "        sig = inspect.signature(target_func)\n",
    "        doc = inspect.getdoc(target_func)\n",
    "    except (ValueError, TypeError):\n",
    "        return [{\"Category\": category, \"Function\": name, \"Argument\": \"Error\", \"Type\": \"-\", \"Default\": \"-\"}]\n",
    "\n",
    "    arg_list = []\n",
    "    for param_name, param in sig.parameters.items():\n",
    "        if param_name == \"self\":\n",
    "            continue\n",
    "\n",
    "        # 1. 型ヒントの整理\n",
    "        type_hint = param.annotation\n",
    "        if type_hint == inspect.Parameter.empty:\n",
    "            type_str = \"Any\"\n",
    "        else:\n",
    "            type_str = str(type_hint).replace(\"typing.\", \"\").replace(\"<class '\", \"\").replace(\"'>\", \"\")\n",
    "\n",
    "        # 2. デフォルト値の整理\n",
    "        default_val = param.default\n",
    "        if default_val == inspect.Parameter.empty:\n",
    "            default_str = \"**Required**\"\n",
    "        else:\n",
    "            default_str = str(default_val)\n",
    "\n",
    "        # 3. Docstringからの情報抽出（制約や範囲のヒント）\n",
    "        doc_extract = extract_doc_info(doc, param_name)\n",
    "\n",
    "        arg_list.append({\n",
    "            \"Category\": category,\n",
    "            \"Function/Class\": name,\n",
    "            \"Argument\": param_name,\n",
    "            \"Type Hint\": type_str,\n",
    "            \"Default\": default_str,\n",
    "            \"Docstring Hint\": doc_extract, # ドキュメント内の記述\n",
    "            \"Verified Range/Options\": \"\"   # ★ここに後で手動検証結果を埋める\n",
    "        })\n",
    "    \n",
    "    return arg_list\n",
    "\n",
    "# ==========================================\n",
    "# 3. 実行とテーブル作成\n",
    "# ==========================================\n",
    "all_data = []\n",
    "\n",
    "if MainClass:\n",
    "    for cat, func_list in targets.items():\n",
    "        for func in func_list:\n",
    "            all_data.extend(analyze_arguments(cat, func))\n",
    "\n",
    "# DataFrame化\n",
    "df_api = pd.DataFrame(all_data)\n",
    "\n",
    "# 見やすく表示するためのスタイル設定\n",
    "# (長すぎるDocstringは省略表示など、実用的なフォーマット)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(f\"Total Arguments Found: {len(df_api)}\")\n",
    "display(df_api)\n",
    "\n",
    "# CSVとして保存（これをベースにExcel等で編集可能）\n",
    "csv_path = \"timesfm_api_verification_table.csv\"\n",
    "df_api.to_csv(csv_path, index=False)\n",
    "print(f\"\\nTable saved to: {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5c084b",
   "metadata": {},
   "source": [
    "# 3-4. ForecastConfig クラス定義と引数の完全一致検証\n",
    "\n",
    "`timesfm.ForecastConfig` と `timesfm.configs.ForecastConfig` が、エイリアス（同一のクラス）であるか、あるいは異なる定義であるかを検証します。\n",
    "特に「設定できる引数」に差異がないか、`inspect` モジュールを使ってシグネチャレベルで比較します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3af6bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timesfm\n",
    "import inspect\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=== ForecastConfig Class Verification ===\\n\")\n",
    "\n",
    "# 1. クラスオブジェクトの取得\n",
    "try:\n",
    "    cls_top = timesfm.ForecastConfig\n",
    "    print(\"Found: timesfm.ForecastConfig\")\n",
    "except AttributeError:\n",
    "    cls_top = None\n",
    "    print(\"Not Found: timesfm.ForecastConfig\")\n",
    "\n",
    "try:\n",
    "    cls_sub = timesfm.configs.ForecastConfig\n",
    "    print(\"Found: timesfm.configs.ForecastConfig\")\n",
    "except AttributeError:\n",
    "    cls_sub = None\n",
    "    print(\"Not Found: timesfm.configs.ForecastConfig\")\n",
    "\n",
    "# 2. 同一クラスかの判定 (Identity Check)\n",
    "if cls_top and cls_sub:\n",
    "    is_same = cls_top is cls_sub\n",
    "    print(f\"\\n[Identity Check] Are they the same object? : {is_same}\")\n",
    "    if is_same:\n",
    "        print(\">> 結論: 完全に同一のクラスです（トップレベルはエイリアスです）。引数に違いはありません。\")\n",
    "    else:\n",
    "        print(\">> 結論: 異なるクラスオブジェクトです。詳細な引数比較を行います。\")\n",
    "\n",
    "# 3. 引数(シグネチャ)の厳密比較\n",
    "if cls_top and cls_sub:\n",
    "    sig_top = inspect.signature(cls_top)\n",
    "    sig_sub = inspect.signature(cls_sub)\n",
    "    \n",
    "    print(f\"\\n[Signature Check]\")\n",
    "    print(f\"Top-level args: {len(sig_top.parameters)} items\")\n",
    "    print(f\"Sub-module args: {len(sig_sub.parameters)} items\")\n",
    "    \n",
    "    # 比較用データフレーム作成\n",
    "    params_top = {k: str(v) for k, v in sig_top.parameters.items()}\n",
    "    params_sub = {k: str(v) for k, v in sig_sub.parameters.items()}\n",
    "    \n",
    "    # 全ての引数キーを統合\n",
    "    all_keys = sorted(set(params_top.keys()) | set(params_sub.keys()))\n",
    "    \n",
    "    comparison_data = []\n",
    "    for key in all_keys:\n",
    "        val_top = params_top.get(key, \"(Not Present)\")\n",
    "        val_sub = params_sub.get(key, \"(Not Present)\")\n",
    "        match = (val_top == val_sub)\n",
    "        comparison_data.append([key, val_top, val_sub, match])\n",
    "        \n",
    "    df_comparison = pd.DataFrame(comparison_data, columns=[\"Argument\", \"timesfm.ForecastConfig\", \"timesfm.configs.ForecastConfig\", \"Match\"])\n",
    "    \n",
    "    # 不一致がある場合のみ表示、または全一致を表示\n",
    "    if df_comparison[\"Match\"].all():\n",
    "        print(\">> 引数定義は完全に一致しています。\")\n",
    "    else:\n",
    "        print(\">> 差異が見つかりました：\")\n",
    "        display(df_comparison[~df_comparison[\"Match\"]])\n",
    "\n",
    "    # 全体一覧（確認用）\n",
    "    print(\"\\n--- 引数一覧 ---\")\n",
    "    display(df_comparison)\n",
    "\n",
    "elif not cls_top:\n",
    "    print(\"\\n検証不可: timesfm.ForecastConfig が存在しません。timesfm.configs.ForecastConfig を使用してください。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b457c96",
   "metadata": {},
   "source": [
    "# 4. コンパイルと予測実行 (Compile & Forecast)\n",
    "\n",
    "検証済みの設定 (`forecast_config`) を使用して、以下のフローを実行します。\n",
    "\n",
    "1.  **コンパイル (`compile`)**: 設定に基づき、モデルの推論プロセスを最適化します（**必須**）。\n",
    "2.  **データ準備**: サンプル DataFrame (`df_sample`) を、モデルが受け付ける `List[numpy.ndarray]` 形式に変換します。\n",
    "3.  **予測 (`forecast`)**: コンパイルされたモデルで未来の値を予測します。\n",
    "4.  **結合・可視化**: 予測結果を元の DataFrame と結合し、グラフで確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed99d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. モデルのコンパイル (必須) ---\n",
    "# 前のステップで作成した推奨設定 (Case 0) を使用\n",
    "print(f\"Compiling model with config: {forecast_config} ...\")\n",
    "tfm.compile(forecast_config)\n",
    "print(\"Model compiled successfully.\")\n",
    "\n",
    "# --- 2. データ準備 (DataFrame -> List[np.array]) ---\n",
    "# TimesFM v2.5 の forecast メソッドはリスト形式の numpy 配列を受け取ります\n",
    "# unique_id ごとにデータを分割してリスト化\n",
    "input_data_map = {}\n",
    "input_arrays = []\n",
    "\n",
    "# 系列IDの順序を保持するためにリストを作成\n",
    "unique_ids = df_sample['unique_id'].unique()\n",
    "\n",
    "for uid in unique_ids:\n",
    "    # 各系列の値を抽出\n",
    "    series_values = df_sample[df_sample['unique_id'] == uid]['value'].values\n",
    "    input_arrays.append(series_values)\n",
    "    input_data_map[uid] = series_values\n",
    "\n",
    "print(f\"Prepared {len(input_arrays)} input series for forecasting.\")\n",
    "\n",
    "# --- 3. 予測実行 (Forecast) ---\n",
    "# horizon は config の max_horizon と同じか、それ以下である必要があります\n",
    "forecast_horizon = forecast_config.max_horizon\n",
    "print(f\"Forecasting next {forecast_horizon} steps...\")\n",
    "\n",
    "# 戻り値: (点予測[Batch, Horizon], その他情報)\n",
    "forecast_result = tfm.forecast(\n",
    "    inputs=input_arrays,\n",
    "    horizon=forecast_horizon\n",
    ")\n",
    "\n",
    "# 予測値の取得 (0番目の要素が点予測)\n",
    "point_forecasts = forecast_result[0]\n",
    "print(f\"Forecast shape: {point_forecasts.shape}\") # (2, 128) expected\n",
    "\n",
    "# --- 4. 結果の整理と可視化 ---\n",
    "# 予測結果をDataFrame化して可視化\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "for i, uid in enumerate(unique_ids):\n",
    "    history = input_arrays[i]\n",
    "    forecast = point_forecasts[i]\n",
    "    \n",
    "    # 日時軸の作成 (簡易的に履歴の続きとして生成)\n",
    "    last_date = df_sample[df_sample['unique_id'] == uid]['date'].max()\n",
    "    freq = \"h\" # サンプルデータの頻度\n",
    "    future_dates = pd.date_range(start=last_date + pd.Timedelta(hours=1), periods=forecast_horizon, freq=freq)\n",
    "    \n",
    "    # 履歴のプロット (直近の一部のみ表示)\n",
    "    display_len = 200\n",
    "    plt.plot(\n",
    "        df_sample[df_sample['unique_id'] == uid]['date'].iloc[-display_len:], \n",
    "        history[-display_len:], \n",
    "        label=f\"{uid} (History)\", \n",
    "        linestyle=\"--\"\n",
    "    )\n",
    "    \n",
    "    # 予測のプロット\n",
    "    plt.plot(future_dates, forecast, label=f\"{uid} (Forecast)\", linewidth=2)\n",
    "\n",
    "plt.title(\"TimesFM Forecast Result\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aba5797",
   "metadata": {},
   "source": [
    "# 5. 単変量予測の比較検証 (Univariate Forecasting)\n",
    "\n",
    "`forecast()` と `forecast_on_df()` の両方のアプローチを比較します。\n",
    "\n",
    "1.  **`forecast()`**:\n",
    "    * **概要**: NumPy配列のリストを入力とする基本API。\n",
    "    * **特徴**: 高速でオーバーヘッドが少ないが、日付やIDの管理は自分で行う必要がある。\n",
    "    * **現状**: `TimesFM_2p5_200M_torch` クラス標準実装。\n",
    "\n",
    "2.  **`forecast_on_df()` (再現実装)**:\n",
    "    * **概要**: Pandas DataFrame (Long形式) を直接受け取り、予測結果もDataFrameで返す。\n",
    "    * **特徴**: 前処理・後処理（日付生成、ID結合）を自動化し、分析フローに組み込みやすい。\n",
    "    * **現状**: v2.5のモデルクラスには直接実装されていないため、ヘルパー関数として定義して使用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94d12eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- A. 標準機能: forecast() の実行 ---\n",
    "print(\"=== A. Testing forecast() (Array Input) ===\")\n",
    "\n",
    "# 1. データの準備 (DataFrame -> List[np.ndarray])\n",
    "# 各系列の値をリストに格納\n",
    "inputs_list = []\n",
    "ids = df_sample['unique_id'].unique()\n",
    "for uid in ids:\n",
    "    series = df_sample[df_sample['unique_id'] == uid]['value'].values\n",
    "    inputs_list.append(series)\n",
    "\n",
    "# 2. 予測実行\n",
    "# コンパイル済みの設定 (max_horizon=128) を使用\n",
    "print(f\"Input type: List of {len(inputs_list)} arrays\")\n",
    "raw_forecast = tfm.forecast(inputs=inputs_list, horizon=128)\n",
    "\n",
    "# 結果は (PointForecast, ...) のタプル\n",
    "point_forecast_array = raw_forecast[0]\n",
    "print(f\"Output shape: {point_forecast_array.shape}\")\n",
    "print(\"forecast() test passed.\\n\")\n",
    "\n",
    "\n",
    "# --- B. 拡張機能: forecast_on_df() の再現と実行 ---\n",
    "print(\"=== B. Testing forecast_on_df() (DataFrame Input) ===\")\n",
    "\n",
    "def forecast_on_df(model, df, unique_id_col=\"unique_id\", value_col=\"value\", date_col=\"date\", freq=\"h\"):\n",
    "    \"\"\"\n",
    "    TimesFM v2.5向けに forecast_on_df の機能を再現するラッパー関数\n",
    "    \"\"\"\n",
    "    # 1. 前処理: データフレームから配列リストへ変換\n",
    "    unique_ids = df[unique_id_col].unique()\n",
    "    inputs = []\n",
    "    last_dates = []\n",
    "    \n",
    "    for uid in unique_ids:\n",
    "        sub_df = df[df[unique_id_col] == uid].sort_values(date_col)\n",
    "        inputs.append(sub_df[value_col].values)\n",
    "        last_dates.append(sub_df[date_col].max())\n",
    "    \n",
    "    # 2. 推論実行\n",
    "    # configから予測期間を取得\n",
    "    horizon = model.forecast_config.max_horizon\n",
    "    forecast_tuple = model.forecast(inputs=inputs, horizon=horizon)\n",
    "    forecast_values = forecast_tuple[0] # (N, Horizon)\n",
    "    \n",
    "    # 3. 後処理: 結果をDataFrame形式に復元\n",
    "    results = []\n",
    "    for idx, uid in enumerate(unique_ids):\n",
    "        # 未来の日付インデックスを作成\n",
    "        start_date = last_dates[idx] + pd.Timedelta(1, unit=freq)\n",
    "        future_dates = pd.date_range(start=start_date, periods=horizon, freq=freq)\n",
    "        \n",
    "        # 結果用DF作成\n",
    "        res_df = pd.DataFrame({\n",
    "            unique_id_col: uid,\n",
    "            date_col: future_dates,\n",
    "            \"timesfm_forecast\": forecast_values[idx]\n",
    "        })\n",
    "        results.append(res_df)\n",
    "        \n",
    "    return pd.concat(results, ignore_index=True)\n",
    "\n",
    "# 実行テスト\n",
    "df_forecast_result = forecast_on_df(\n",
    "    model=tfm, \n",
    "    df=df_sample, \n",
    "    unique_id_col=\"unique_id\", \n",
    "    value_col=\"value\", \n",
    "    date_col=\"date\",\n",
    "    freq=\"h\"\n",
    ")\n",
    "\n",
    "# 結果確認\n",
    "print(\"Forecast DataFrame Head:\")\n",
    "display(df_forecast_result.head())\n",
    "\n",
    "# 可視化 (forecast_on_df の結果を使用)\n",
    "plt.figure(figsize=(12, 5))\n",
    "for uid in ids:\n",
    "    # 実績\n",
    "    history = df_sample[df_sample['unique_id'] == uid]\n",
    "    plt.plot(history['date'].iloc[-100:], history['value'].iloc[-100:], label=f\"{uid} History\", linestyle=\"--\", alpha=0.7)\n",
    "    \n",
    "    # 予測\n",
    "    pred = df_forecast_result[df_forecast_result['unique_id'] == uid]\n",
    "    plt.plot(pred['date'], pred['timesfm_forecast'], label=f\"{uid} Forecast\", linewidth=2)\n",
    "\n",
    "plt.title(\"Forecast Result using 'forecast_on_df' wrapper\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a222de55",
   "metadata": {},
   "source": [
    "# 6. 外部変数付き予測 (再修正版: Backcast有効化)\n",
    "このノートでは `forecast_with_covariates()` を使って、以下4種類の共変量（Covariates）を与えた予測を試す。\n",
    "\n",
    "- 動的・数値（dynamic numerical）：例）プロモ有無、気温など（時刻ごとに変化）\n",
    "- 動的・カテゴリ（dynamic categorical）：例）曜日、祝日フラグなど（時刻ごとに変化）\n",
    "- 静的・数値（static numerical）：例）店舗面積、平均単価など（系列ごとに固定）\n",
    "- 静的・カテゴリ（static categorical）：例）国、地域、店舗IDなど（系列ごとに固定）\n",
    "\n",
    "重要：動的共変量は「過去(context) + 未来(horizon)」の長さが必要。\n",
    "つまり `len(cov[t]) == context_len + horizon_len`。\n",
    "\n",
    "返り値：\n",
    "- `cov_forecast`：既定では `\"xreg + timesfm\"`（XRegで補正したTimesFM予測）\n",
    "- `ols_forecast`：`\"xreg\"` 成分のみ（回帰だけの予測、比較用）\n",
    "\n",
    "※ TimesFM 2.5 は XReg による共変量サポートが復活している（`.[xreg]` が必要）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca5ba1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "print(\"sys.executable =\", sys.executable)\n",
    "\n",
    "try:\n",
    "    import jax, jaxlib\n",
    "    print(\"jax =\", jax.__version__)\n",
    "    print(\"jaxlib =\", jaxlib.__version__)\n",
    "except Exception as e:\n",
    "    print(\"JAX import failed:\", repr(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cc24f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall -y jax jaxlib\n",
    "%pip install -U pip\n",
    "%pip install -U jax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ed0549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax, jaxlib\n",
    "print(jax.__version__, jaxlib.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e96257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import inspect\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import timesfm\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "# --- 1) 設定 ---\n",
    "BATCH_SIZE  = 8\n",
    "CONTEXT_LEN = 512\n",
    "HORIZON_LEN = 128\n",
    "TOTAL_LEN   = CONTEXT_LEN + HORIZON_LEN\n",
    "\n",
    "MODEL_LOCAL_DIR = r\"C:\\moinfo\\timesfm_v2.5_local\"\n",
    "MODEL_HF_ID     = \"google/timesfm-2.5-200m-pytorch\"\n",
    "\n",
    "# --- 2) モデル読込（ローカル -> HF の順で試す） ---\n",
    "def load_timesfm_2p5():\n",
    "    try:\n",
    "        if os.path.isdir(MODEL_LOCAL_DIR):\n",
    "            print(f\"Loading model from local dir: {MODEL_LOCAL_DIR}\")\n",
    "            return timesfm.TimesFM_2p5_200M_torch.from_pretrained(MODEL_LOCAL_DIR)\n",
    "    except Exception as e:\n",
    "        print(\"Local load failed, fallback to HF:\", repr(e))\n",
    "\n",
    "    print(f\"Loading model from HF: {MODEL_HF_ID}\")\n",
    "    return timesfm.TimesFM_2p5_200M_torch.from_pretrained(MODEL_HF_ID)\n",
    "\n",
    "model = load_timesfm_2p5()\n",
    "\n",
    "# ★重要：XRegを使うなら return_backcast=True が必須\n",
    "bench_cfg = timesfm.ForecastConfig(\n",
    "    max_context=CONTEXT_LEN,\n",
    "    max_horizon=HORIZON_LEN,\n",
    "    per_core_batch_size=BATCH_SIZE,\n",
    "    normalize_inputs=True,\n",
    "    return_backcast=True,   # ← これを追加！\n",
    ")\n",
    "\n",
    "print(\"Compiling with return_backcast=True ...\")\n",
    "model.compile(bench_cfg)\n",
    "\n",
    "# --- 3) ダミー入力（ターゲット系列） ---\n",
    "inputs = [np.sin(np.linspace(0, 30, CONTEXT_LEN)).astype(np.float32) for _ in range(BATCH_SIZE)]\n",
    "\n",
    "# --- 4) 共変量を作る（最小例） ---\n",
    "t = np.arange(TOTAL_LEN, dtype=np.int32)\n",
    "\n",
    "dynamic_numerical_covariates = {\n",
    "    \"promo\": [(np.random.rand(TOTAL_LEN) < 0.1).astype(np.float32) for _ in range(BATCH_SIZE)]\n",
    "}\n",
    "\n",
    "dynamic_categorical_covariates = {\n",
    "    \"week_day\": [((t + np.random.randint(0, 7)) % 7).astype(np.int32) for _ in range(BATCH_SIZE)]\n",
    "}\n",
    "\n",
    "static_numerical_covariates = {\n",
    "    \"store_size\": [float(np.random.uniform(0.5, 2.0)) for _ in range(BATCH_SIZE)]\n",
    "}\n",
    "\n",
    "static_categorical_covariates = {\n",
    "    \"region\": [random.choice([\"JP\", \"US\", \"EU\"]) for _ in range(BATCH_SIZE)]\n",
    "}\n",
    "\n",
    "# --- 5) 互換ラッパ（引数差を吸収） ---\n",
    "def forecast_with_covariates_compat(\n",
    "    model,\n",
    "    inputs,\n",
    "    horizon,\n",
    "    dynamic_numerical_covariates=None,\n",
    "    dynamic_categorical_covariates=None,\n",
    "    static_numerical_covariates=None,\n",
    "    static_categorical_covariates=None,\n",
    "    xreg_mode=\"xreg + timesfm\",\n",
    "    ridge=0.0,\n",
    "    force_on_cpu=False,\n",
    "    normalize_xreg_target_per_input=True,\n",
    "):\n",
    "    if not hasattr(model, \"forecast_with_covariates\"):\n",
    "        raise AttributeError(\"この model に forecast_with_covariates が見つかりません。timesfm を xreg 対応で入れているか確認してください。\")\n",
    "\n",
    "    fn = model.forecast_with_covariates\n",
    "    sig = inspect.signature(fn)\n",
    "    kwargs = dict(\n",
    "        inputs=inputs,\n",
    "        dynamic_numerical_covariates=dynamic_numerical_covariates or {},\n",
    "        dynamic_categorical_covariates=dynamic_categorical_covariates or {},\n",
    "        static_numerical_covariates=static_numerical_covariates or {},\n",
    "        static_categorical_covariates=static_categorical_covariates or {},\n",
    "        xreg_mode=xreg_mode,\n",
    "        ridge=ridge,\n",
    "        force_on_cpu=force_on_cpu,\n",
    "        normalize_xreg_target_per_input=normalize_xreg_target_per_input,\n",
    "    )\n",
    "\n",
    "    if \"horizon\" in sig.parameters:\n",
    "        kwargs[\"horizon\"] = horizon\n",
    "    if \"freq\" in sig.parameters:\n",
    "        kwargs[\"freq\"] = [0] * len(inputs)\n",
    "\n",
    "    return fn(**kwargs)\n",
    "\n",
    "# --- 6) TimesFM 単体の予測 ---\n",
    "try:\n",
    "    base_point, _ = model.forecast(horizon=HORIZON_LEN, inputs=inputs)\n",
    "except TypeError:\n",
    "    base_point, _ = model.forecast(inputs=inputs, freq=[0]*len(inputs))\n",
    "\n",
    "base_point = np.asarray(base_point)\n",
    "print(\"base_point shape:\", base_point.shape)\n",
    "\n",
    "# --- 7) 共変量付き予測（XReg）---\n",
    "cov_forecast, ols_forecast = forecast_with_covariates_compat(\n",
    "    model=model,\n",
    "    inputs=inputs,\n",
    "    horizon=HORIZON_LEN,\n",
    "    dynamic_numerical_covariates=dynamic_numerical_covariates,\n",
    "    dynamic_categorical_covariates=dynamic_categorical_covariates,\n",
    "    static_numerical_covariates=static_numerical_covariates,\n",
    "    static_categorical_covariates=static_categorical_covariates,\n",
    "    xreg_mode=\"xreg + timesfm\",\n",
    "    ridge=0.0,\n",
    "    force_on_cpu=False,\n",
    "    normalize_xreg_target_per_input=True,\n",
    ")\n",
    "\n",
    "cov_forecast = np.asarray(cov_forecast)\n",
    "ols_forecast = np.asarray(ols_forecast)\n",
    "\n",
    "print(\"cov_forecast shape:\", cov_forecast.shape)\n",
    "print(\"ols_forecast shape:\", ols_forecast.shape)\n",
    "\n",
    "# --- 8) 可視化（先頭系列だけ） ---\n",
    "\n",
    "base_point = np.asarray(base_point)\n",
    "cov_forecast = np.asarray(cov_forecast)\n",
    "ols_forecast = np.asarray(ols_forecast)\n",
    "\n",
    "# ★ 未来だけ取り出す（return_backcast=True のとき必須）\n",
    "base_future = base_point[:, -HORIZON_LEN:]   # shape: (B, 128)\n",
    "\n",
    "# OLS側は (B, 128, 10) なので、代表として中央値(0.5)っぽい列を選ぶ\n",
    "# どの列が中央値かは実装依存なので、まず真ん中を取る（10本→ index=5）\n",
    "ols_median = ols_forecast[:, :, ols_forecast.shape[-1] // 2]  # shape: (B, 128)\n",
    "\n",
    "x_ctx = np.arange(CONTEXT_LEN)\n",
    "x_fut = np.arange(CONTEXT_LEN, CONTEXT_LEN + HORIZON_LEN)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(x_ctx, inputs[0], label=\"input (context)\")\n",
    "plt.plot(x_fut, base_future[0], label=\"TimesFM (base, future only)\")\n",
    "plt.plot(x_fut, cov_forecast[0], label=\"XReg + TimesFM (cov_forecast)\")\n",
    "plt.plot(x_fut, ols_median[0], label=\"XReg only (ols median)\", linestyle=\"--\")\n",
    "plt.title(\"TimesFM forecast_with_covariates: base vs covariates\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32be28b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "backcast_len = base_point.shape[1] - HORIZON_LEN\n",
    "base_backcast = base_point[:, :backcast_len]\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(np.arange(CONTEXT_LEN), inputs[0], label=\"input (context)\")\n",
    "plt.plot(np.arange(CONTEXT_LEN - backcast_len, CONTEXT_LEN), base_backcast[0], label=\"TimesFM (backcast)\")\n",
    "plt.plot(np.arange(CONTEXT_LEN, CONTEXT_LEN + HORIZON_LEN), base_future[0], label=\"TimesFM (forecast)\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602328b9",
   "metadata": {},
   "source": [
    "# 7. ベクトル抽出 (構造特定と自動フック)\n",
    "\n",
    "モデルの内部構造を表示し、特徴量抽出に最適なレイヤー（通常は最後のTransformerブロックや正規化層）を特定してフックします。\n",
    "\n",
    "**アプローチ:**\n",
    "1. `named_modules()` を走査し、モデルの階層構造をテキストで出力して確認します。\n",
    "2. 名前リストの中から、`layers` や `blocks` を含む最も深いインデックスを持つ層を自動検出し、ターゲットにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4733acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import timesfm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. モデル構造の「レントゲン撮影」 ---\n",
    "print(\"=== Inspecting Model Internal Layers ===\")\n",
    "\n",
    "# モデル内の主要なモジュール名を表示 (深さ2まで)\n",
    "# これで layers, blocks, transformer などの名前を確認します\n",
    "found_layers = []\n",
    "for name, module in tfm.model.named_modules():\n",
    "    # 名前が空（自分自身）はスキップ\n",
    "    if not name: continue\n",
    "    \n",
    "    # 全て表示すると多すぎるので、主要なコンポーネントのみ記録\n",
    "    found_layers.append(name)\n",
    "    \n",
    "    # トップレベル付近のみ表示\n",
    "    if name.count('.') <= 1:\n",
    "        print(f\" - {name}: {module.__class__.__name__}\")\n",
    "\n",
    "print(f\"\\nTotal modules found: {len(found_layers)}\")\n",
    "\n",
    "# --- 2. ターゲット層の自動選定 ---\n",
    "# 「layers.数字」のようなパターンを探し、その最大番号（最終層）を特定します\n",
    "target_name = None\n",
    "max_layer_idx = -1\n",
    "\n",
    "for name in found_layers:\n",
    "    parts = name.split('.')\n",
    "    # \"layers.19\" や \"blocks.19\" のようなパターンを検出\n",
    "    for i, part in enumerate(parts):\n",
    "        if part.isdigit() and i > 0:\n",
    "            # 数字の前の部分がコンテナ名 (layers, blocks等)\n",
    "            container_name = parts[i-1]\n",
    "            idx = int(part)\n",
    "            \n",
    "            # コンテナ名が一般的かチェック\n",
    "            if container_name in ['layers', 'blocks', 'h', 'transformer']:\n",
    "                if idx > max_layer_idx:\n",
    "                    max_layer_idx = idx\n",
    "                    # この数字までのパスをターゲット候補とする\n",
    "                    target_name = \".\".join(parts[:i+1])\n",
    "\n",
    "# もし数字付きレイヤーが見つからない場合、output_layerやnormを探す\n",
    "if target_name is None:\n",
    "    for name in found_layers:\n",
    "        if 'output' in name or 'final_norm' in name or 'ln_f' in name:\n",
    "            target_name = name\n",
    "            break\n",
    "\n",
    "print(f\"\\nTarget Layer Auto-Selected: '{target_name}'\")\n",
    "\n",
    "# --- 3. 選択した層へのフックとベクトル抽出 ---\n",
    "if target_name:\n",
    "    # 名前から実際のモジュールオブジェクトを取得\n",
    "    target_module = dict(tfm.model.named_modules())[target_name]\n",
    "    \n",
    "    embeddings_storage = {}\n",
    "    \n",
    "    # 堅牢なフック関数 (再定義)\n",
    "    def robust_hook(name):\n",
    "        def hook(model, input, output):\n",
    "            # Tensorを探す\n",
    "            if isinstance(output, torch.Tensor):\n",
    "                tensor = output\n",
    "            elif isinstance(output, (tuple, list)):\n",
    "                tensor = output[0] if len(output) > 0 and isinstance(output[0], torch.Tensor) else None\n",
    "            else:\n",
    "                tensor = None\n",
    "                \n",
    "            if tensor is not None:\n",
    "                embeddings_storage[name] = tensor.detach().cpu().numpy()\n",
    "        return hook\n",
    "\n",
    "    # フック登録\n",
    "    handle = target_module.register_forward_hook(robust_hook('feature_embedding'))\n",
    "    print(f\"Hook registered on: {target_module.__class__.__name__}\")\n",
    "\n",
    "    # 予測実行\n",
    "    print(\"Running forecast to extract features...\")\n",
    "    try:\n",
    "        _ = tfm.forecast(inputs=input_data, horizon=128)\n",
    "    except Exception as e:\n",
    "        print(f\"Forecast warning: {e}\")\n",
    "\n",
    "    # フック解除\n",
    "    handle.remove()\n",
    "\n",
    "    # --- 4. 可視化 ---\n",
    "    if 'feature_embedding' in embeddings_storage:\n",
    "        emb = embeddings_storage['feature_embedding']\n",
    "        print(f\"\\nSuccess! Extracted Embedding Shape: {emb.shape}\")\n",
    "        \n",
    "        # ヒートマップ\n",
    "        feature_map = emb[0, :, :200].T\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.imshow(feature_map, aspect='auto', cmap='magma', origin='lower')\n",
    "        plt.colorbar(label='Activation')\n",
    "        plt.title(f\"TimesFM Feature Embeddings\\nLayer: {target_name}\")\n",
    "        plt.xlabel(\"Time Patch Index\")\n",
    "        plt.ylabel(\"Feature Dimension (First 200)\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Error: Failed to capture embeddings (Tensor not found in output).\")\n",
    "\n",
    "else:\n",
    "    print(\"Error: Could not automatically identify a target layer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba195f1",
   "metadata": {},
   "source": [
    "# 7-2. 特徴量のテーブル表示 (DataFrame)\n",
    "\n",
    "モデルの内部構造解析により、`stacked_xf` という名前でTransformerブロックが積み重なっていることが分かりました。\n",
    "最も抽象度の高い特徴量を持つ **`stacked_xf.19` (最終層)** をターゲットにして再度ベクトル抽出を行い、その数値を**テーブル（DataFrame）**として整形・表示します。\n",
    "\n",
    "* **行 (Index)**: 時系列のパッチ（時間ステップ）\n",
    "* **列 (Columns)**: 特徴量の次元（0 ~ 1279）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6ec147",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import timesfm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- 1. ターゲット層 (stacked_xf.19) へのフック登録 ---\n",
    "target_layer_name = 'stacked_xf.19'  # 構造解析で見つかった最終Transformer層\n",
    "print(f\"=== Hooking into target: {target_layer_name} ===\")\n",
    "\n",
    "# モジュールの取得\n",
    "target_module = dict(tfm.model.named_modules())[target_layer_name]\n",
    "\n",
    "# データ格納用\n",
    "embeddings_storage = {}\n",
    "\n",
    "# 堅牢なフック関数\n",
    "def robust_hook(name):\n",
    "    def hook(model, input, output):\n",
    "        # Tensorを探す (Tupleの中に隠れている場合があるため)\n",
    "        if isinstance(output, torch.Tensor):\n",
    "            tensor = output\n",
    "        elif isinstance(output, (tuple, list)):\n",
    "            tensor = output[0] if len(output) > 0 and isinstance(output[0], torch.Tensor) else None\n",
    "        else:\n",
    "            tensor = None\n",
    "            \n",
    "        if tensor is not None:\n",
    "            embeddings_storage[name] = tensor.detach().cpu().numpy()\n",
    "    return hook\n",
    "\n",
    "# フック登録\n",
    "handle = target_module.register_forward_hook(robust_hook('deep_features'))\n",
    "\n",
    "# --- 2. 予測実行 (特徴量抽出) ---\n",
    "print(\"Running forecast...\")\n",
    "try:\n",
    "    # バッチサイズ1で実行\n",
    "    _ = tfm.forecast(inputs=[input_data[0]], horizon=128)\n",
    "except Exception as e:\n",
    "    # forecastの戻り値処理でエラーが出ても、フックさえ動けばOK\n",
    "    print(f\"Forecast processed (Warning: {e})\")\n",
    "\n",
    "handle.remove()\n",
    "\n",
    "# --- 3. テーブル (DataFrame) 化と表示 ---\n",
    "if 'deep_features' in embeddings_storage:\n",
    "    # Shape: (Batch, Patch_Length, Dim) -> (1, 16, 1280)\n",
    "    emb = embeddings_storage['deep_features'][0]  # バッチ先頭を取り出し\n",
    "    \n",
    "    # DataFrame作成\n",
    "    # 行: パッチ(時間), 列: 次元\n",
    "    df_features = pd.DataFrame(emb)\n",
    "    df_features.index.name = \"Time_Patch_Index\"\n",
    "    df_features.columns.name = \"Feature_Dim\"\n",
    "    \n",
    "    print(f\"\\nFeature Table Shape: {df_features.shape}\")\n",
    "    print(\"=== Feature Matrix (Head) ===\")\n",
    "    \n",
    "    # 大きすぎるので一部を表示 (最初の10列だけなど)\n",
    "    display(df_features.iloc[:, :10].head())\n",
    "    \n",
    "    # 統計情報の確認\n",
    "    print(\"\\n=== Basic Statistics of Features ===\")\n",
    "    display(df_features.describe().iloc[:, :10]) # 記述統計\n",
    "    \n",
    "    # --- (オプション) クラスタリング用データの作成 ---\n",
    "    # このテーブルを使えば、類似度計算などが可能です\n",
    "    # 例: パッチごとの相関行列\n",
    "    corr_matrix = df_features.T.corr()\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(corr_matrix, cmap='coolwarm', center=0)\n",
    "    plt.title(\"Time Patch Correlation (Similarity)\")\n",
    "    plt.xlabel(\"Patch Time\")\n",
    "    plt.ylabel(\"Patch Time\")\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"Error: Failed to capture features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865db7d2",
   "metadata": {},
   "source": [
    "# 8. 最適化と高速化の検証 (Optimization & Benchmark)\n",
    "\n",
    "TimesFM v2.5 の `compile()` による最適化効果を検証します。\n",
    "モデルの計算グラフを事前に構築・最適化することで、2回目以降の推論速度が大幅に向上する（キャッシュ効果）ことを確認します。\n",
    "\n",
    "**検証項目:**\n",
    "1.  **Compilation Time**: `tfm.compile()` にかかる時間。\n",
    "2.  **Warm-up Inference**: コンパイル後、最初の推論にかかる時間（初期化オーバーヘッドを含む）。\n",
    "3.  **Cached Inference**: 2回目以降の推論にかかる時間（ここが実運用時の速度）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806a2079",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import timesfm\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 設定\n",
    "# =========================\n",
    "BATCH_SIZE = 32\n",
    "CONTEXT_LEN = 512\n",
    "HORIZON_LEN = 128\n",
    "\n",
    "# hookで中間表現を取りたい場合だけ True（ベンチだけなら False 推奨）\n",
    "CAPTURE_ACTIVATIONS = False\n",
    "\n",
    "# 使うチェックポイント\n",
    "# - TimesFM 2.5が使える環境なら 2.5 をデフォルト\n",
    "# - 2.5が無い（=2.0系）なら 2.0 をデフォルト\n",
    "HF_REPO_ID_2P5 = \"google/timesfm-2.5-200m-pytorch\"\n",
    "HF_REPO_ID_2P0 = \"google/timesfm-2.0-500m-pytorch\"\n",
    "\n",
    "\n",
    "# =========================\n",
    "# ユーティリティ\n",
    "# =========================\n",
    "def sync_cuda():\n",
    "    \"\"\"CUDAの非同期実行で計測がズレるのを防ぐ。\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "\n",
    "def pick_first_tensor(x):\n",
    "    \"\"\"入れ子構造(tuple/list/dict)から最初の torch.Tensor を拾う。無ければ None.\"\"\"\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x\n",
    "    if isinstance(x, (tuple, list)):\n",
    "        for v in x:\n",
    "            t = pick_first_tensor(v)\n",
    "            if t is not None:\n",
    "                return t\n",
    "    if isinstance(x, dict):\n",
    "        for v in x.values():\n",
    "            t = pick_first_tensor(v)\n",
    "            if t is not None:\n",
    "                return t\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_activation_hook(name: str, storage: dict):\n",
    "    \"\"\"\n",
    "    forward hook: outputがtupleでも落ちない版。\n",
    "    あなたのエラー（'tuple' object has no attribute 'detach'）を潰す主役。\n",
    "    \"\"\"\n",
    "    def hook(_module, _inputs, output):\n",
    "        t = pick_first_tensor(output)\n",
    "        if t is None:\n",
    "            return\n",
    "        storage[name] = t.detach().cpu().numpy()\n",
    "    return hook\n",
    "\n",
    "\n",
    "def make_forecast_config(**kwargs):\n",
    "    \"\"\"\n",
    "    timesfm.ForecastConfig があればそれを使う。\n",
    "    互換性のため、受け付けない引数があれば落として再作成。\n",
    "    \"\"\"\n",
    "    ForecastConfig = getattr(timesfm, \"ForecastConfig\", None)\n",
    "    if ForecastConfig is None and hasattr(timesfm, \"configs\"):\n",
    "        ForecastConfig = getattr(timesfm.configs, \"ForecastConfig\", None)\n",
    "    if ForecastConfig is None:\n",
    "        raise RuntimeError(\"ForecastConfig が見つかりません。timesfm のバージョンを確認してください。\")\n",
    "\n",
    "    try:\n",
    "        return ForecastConfig(**kwargs)\n",
    "    except TypeError:\n",
    "        # バージョン差で受け付けないキーがある場合の保険\n",
    "        # まずは per_core_batch_size を落として再試行\n",
    "        kwargs.pop(\"per_core_batch_size\", None)\n",
    "        return ForecastConfig(**kwargs)\n",
    "\n",
    "\n",
    "def load_timesfm_model():\n",
    "    \"\"\"\n",
    "    TimesFM 2.5 API を優先してロード。\n",
    "    2.5クラスが無ければ 2.0 API へフォールバック。\n",
    "    \"\"\"\n",
    "    torch.set_float32_matmul_precision(\"high\")  # 公式例 :contentReference[oaicite:2]{index=2}\n",
    "\n",
    "    if hasattr(timesfm, \"TimesFM_2p5_200M_torch\"):\n",
    "        model = timesfm.TimesFM_2p5_200M_torch.from_pretrained(HF_REPO_ID_2P5)  # 公式例 :contentReference[oaicite:3]{index=3}\n",
    "        api_ver = \"2.5\"\n",
    "        return model, api_ver\n",
    "\n",
    "    # ---- フォールバック: TimesFM 2.0 API（TimesFm + TimesFmHparams） ----\n",
    "    if hasattr(timesfm, \"TimesFm\") and hasattr(timesfm, \"TimesFmHparams\") and hasattr(timesfm, \"TimesFmCheckpoint\"):\n",
    "        model = timesfm.TimesFm(\n",
    "            hparams=timesfm.TimesFmHparams(\n",
    "                backend=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "                per_core_batch_size=BATCH_SIZE,\n",
    "                horizon_len=HORIZON_LEN,\n",
    "                # 2.0系は context_len を持つ（max 2048 推奨） :contentReference[oaicite:4]{index=4}\n",
    "                context_len=2048,\n",
    "                num_layers=50,\n",
    "                use_positional_embedding=False,\n",
    "            ),\n",
    "            checkpoint=timesfm.TimesFmCheckpoint(huggingface_repo_id=HF_REPO_ID_2P0),\n",
    "        )\n",
    "        api_ver = \"2.0\"\n",
    "        return model, api_ver\n",
    "\n",
    "    raise RuntimeError(\"TimesFM のロード方法が見つかりません。timesfm のインストール状況を確認してください。\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# メイン\n",
    "# =========================\n",
    "def main():\n",
    "    print(\"=== Starting Optimization Benchmark ===\")\n",
    "    print(f\"torch: {torch.__version__}\")\n",
    "    print(f\"cuda available: {torch.cuda.is_available()}\")\n",
    "\n",
    "    # ダミーデータ生成（Batch, Length）\n",
    "    # 正弦波を32本用意\n",
    "    benchmark_input = [np.sin(np.linspace(0, 100, CONTEXT_LEN)).astype(np.float32) for _ in range(BATCH_SIZE)]\n",
    "\n",
    "    # モデルロード\n",
    "    tfm, api_ver = load_timesfm_model()\n",
    "    print(f\"Loaded TimesFM API version: {api_ver}\")\n",
    "\n",
    "    # eval相当（内部に torch.nn.Module を持っていれば）\n",
    "    if hasattr(tfm, \"model\") and isinstance(tfm.model, torch.nn.Module):\n",
    "        tfm.model.eval()\n",
    "\n",
    "    # forward hook（任意）\n",
    "    embeddings_storage = {}\n",
    "    hook_handles = []\n",
    "    if CAPTURE_ACTIVATIONS:\n",
    "        # 2.5系は tfm.model が本体モジュールになっていることが多い\n",
    "        target = tfm.model if hasattr(tfm, \"model\") and isinstance(tfm.model, torch.nn.Module) else None\n",
    "        if target is not None:\n",
    "            h = target.register_forward_hook(get_activation_hook(\"tfm.model:first_tensor\", embeddings_storage))\n",
    "            hook_handles.append(h)\n",
    "            print(\"Activation hook enabled (safe for tuple outputs).\")\n",
    "        else:\n",
    "            print(\"Activation hook requested but no torch.nn.Module found; skipping hooks.\")\n",
    "\n",
    "    # 2.5系：compile + ForecastConfig が使える :contentReference[oaicite:5]{index=5}\n",
    "    compile_time = 0.0\n",
    "    if hasattr(tfm, \"compile\"):\n",
    "        print(f\"Compiling model (Batch Size: {BATCH_SIZE})...\")\n",
    "        bench_config = make_forecast_config(\n",
    "            max_context=CONTEXT_LEN,\n",
    "            max_horizon=HORIZON_LEN,\n",
    "            per_core_batch_size=BATCH_SIZE,  # バージョン差で無効なら自動で落とす\n",
    "            normalize_inputs=True,\n",
    "        )\n",
    "\n",
    "        sync_cuda()\n",
    "        t0 = time.perf_counter()\n",
    "        tfm.compile(bench_config)\n",
    "        sync_cuda()\n",
    "        compile_time = time.perf_counter() - t0\n",
    "        print(f\"Compilation Time: {compile_time:.6f} seconds\")\n",
    "    else:\n",
    "        print(\"compile() not available in this TimesFM version; skipping compilation timing.\")\n",
    "\n",
    "    # 推論ベンチ\n",
    "    print(\"Running inference benchmark...\")\n",
    "\n",
    "    def run_once():\n",
    "        # 推論専用モード（autograd無効でオーバーヘッド削減） :contentReference[oaicite:6]{index=6}\n",
    "        with torch.inference_mode():\n",
    "            _ = tfm.forecast(inputs=benchmark_input, horizon=HORIZON_LEN)\n",
    "\n",
    "    # A. 1st Run (Warm-up)\n",
    "    sync_cuda()\n",
    "    t0 = time.perf_counter()\n",
    "    run_once()\n",
    "    sync_cuda()\n",
    "    warmup_time = time.perf_counter() - t0\n",
    "    print(f\"1st Run (Warm-up): {warmup_time:.6f} seconds\")\n",
    "\n",
    "    # B. 2nd Run (Cached)\n",
    "    sync_cuda()\n",
    "    t0 = time.perf_counter()\n",
    "    run_once()\n",
    "    sync_cuda()\n",
    "    optimized_time = time.perf_counter() - t0\n",
    "    print(f\"2nd Run (Cached) : {optimized_time:.6f} seconds\")\n",
    "\n",
    "    # C. Average (Stable)\n",
    "    num_loops = 10\n",
    "    total = 0.0\n",
    "    for _ in range(num_loops):\n",
    "        sync_cuda()\n",
    "        t0 = time.perf_counter()\n",
    "        run_once()\n",
    "        sync_cuda()\n",
    "        total += (time.perf_counter() - t0)\n",
    "\n",
    "    avg_time = total / num_loops\n",
    "    print(f\"Avg Run ({num_loops} loops): {avg_time:.6f} seconds\")\n",
    "\n",
    "    # 可視化\n",
    "    times = [compile_time, warmup_time, optimized_time, avg_time]\n",
    "    labels = ['Compilation', 'Warm-up\\n(1st)', 'Optimized\\n(2nd)', 'Average\\n(Stable)']\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(labels, times)\n",
    "\n",
    "    for bar in bars:\n",
    "        h = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., h, f\"{h:.6f}s\", ha=\"center\", va=\"bottom\", fontweight=\"bold\")\n",
    "\n",
    "    plt.ylabel(\"Execution Time (seconds)\")\n",
    "    plt.title(f\"TimesFM Speed Benchmark (API {api_ver})\\n(Batch Size: {BATCH_SIZE}, Horizon: {HORIZON_LEN})\")\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "    plt.show()\n",
    "\n",
    "    # 速度向上率\n",
    "    if optimized_time > 0:\n",
    "        speedup = warmup_time / optimized_time\n",
    "        print(f\"\\n>> Speedup Factor (Warm-up / Optimized): {speedup:.2f}x\")\n",
    "\n",
    "    # hook後片付け\n",
    "    for h in hook_handles:\n",
    "        h.remove()\n",
    "\n",
    "    if CAPTURE_ACTIVATIONS:\n",
    "        print(f\"Captured activations keys: {list(embeddings_storage.keys())[:5]} ...\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faa24fc",
   "metadata": {},
   "source": [
    "## TimesFM `save_pretrained()` 引数の網羅抽出と、保存結果の差分検証\n",
    "\n",
    "このノートでは、いま使っている `TimesFM` インスタンスの `save_pretrained()` について\n",
    "\n",
    "1) 実際のシグネチャ（引数一覧）を `inspect.signature()` で抽出  \n",
    "2) 主要な引数（例：`max_shard_size`, `variant`）を変えて保存  \n",
    "3) 生成されたファイル一覧・合計サイズ・再ロード可否を比較\n",
    "\n",
    "を自動で行う。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d180d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "import timesfm\n",
    "\n",
    "# ===== 設定（フルパス）=====\n",
    "MODEL_LOCAL_DIR = r\"C:\\moinfo\\timesfm_v2.5_local\"\n",
    "MODEL_HF_ID     = \"google/timesfm-2.5-200m-pytorch\"\n",
    "\n",
    "OUT_ROOT = Path(r\"C:\\moinfo\\_artifacts\\timesfm_save_pretrained_matrix_tests\")\n",
    "OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def load_model():\n",
    "    if os.path.isdir(MODEL_LOCAL_DIR):\n",
    "        print(f\"[load] local: {MODEL_LOCAL_DIR}\")\n",
    "        return timesfm.TimesFM_2p5_200M_torch.from_pretrained(MODEL_LOCAL_DIR)\n",
    "    print(f\"[load] hf: {MODEL_HF_ID}\")\n",
    "    return timesfm.TimesFM_2p5_200M_torch.from_pretrained(MODEL_HF_ID)\n",
    "\n",
    "model = load_model()\n",
    "\n",
    "# ===== ユーティリティ =====\n",
    "def sha256_file(p: Path) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with p.open(\"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(1024 * 1024), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def list_files_with_stats(d: Path):\n",
    "    rows = []\n",
    "    for p in sorted(d.rglob(\"*\")):\n",
    "        if p.is_file():\n",
    "            rows.append({\n",
    "                \"path\": str(p.relative_to(d)),\n",
    "                \"bytes\": p.stat().st_size,\n",
    "                \"sha256\": sha256_file(p),\n",
    "            })\n",
    "    return rows\n",
    "\n",
    "def save_and_report(name: str, **kwargs):\n",
    "    out_dir = OUT_ROOT / name\n",
    "    if out_dir.exists():\n",
    "        # 既存があると差分が紛れるので消す\n",
    "        for p in out_dir.rglob(\"*\"):\n",
    "            if p.is_file():\n",
    "                p.unlink()\n",
    "        for p in sorted(out_dir.rglob(\"*\"), reverse=True):\n",
    "            if p.is_dir():\n",
    "                try:\n",
    "                    p.rmdir()\n",
    "                except OSError:\n",
    "                    pass\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"\\n=== EXP: {name} ===\")\n",
    "    print(\"save_dir:\", str(out_dir))\n",
    "    print(\"kwargs   :\", kwargs)\n",
    "\n",
    "    t0 = time.time()\n",
    "    ret = model.save_pretrained(str(out_dir), **kwargs)  # ここが本体\n",
    "    dt = time.time() - t0\n",
    "\n",
    "    files = list_files_with_stats(out_dir)\n",
    "    total = sum(r[\"bytes\"] for r in files)\n",
    "    files_sorted = sorted(files, key=lambda r: r[\"bytes\"], reverse=True)\n",
    "\n",
    "    print(f\"saved_files: {len(files)} total: {total/1024/1024:.2f}MB save_time: {dt:.3f}s\")\n",
    "    print(\"top files:\")\n",
    "    for r in files_sorted[:5]:\n",
    "        print(f\"  - {r['path']}: {r['bytes']/1024/1024:.2f}MB sha256={r['sha256'][:12]}...\")\n",
    "\n",
    "    return {\"name\": name, \"dir\": str(out_dir), \"ret\": ret, \"files\": files, \"total_bytes\": total, \"save_sec\": dt}\n",
    "\n",
    "# ===== 実験メニュー =====\n",
    "# 1) baseline：何も渡さない\n",
    "exp1 = save_and_report(\"baseline_default\")\n",
    "\n",
    "# 2) README（モデルカード）だけ変える：model_card_kwargs\n",
    "# ※ docs上、ModelHubMixin 側が model card を生成するための kwargs です。:contentReference[oaicite:9]{index=9}\n",
    "exp2 = save_and_report(\n",
    "    \"model_card_kwargs_test\",\n",
    "    model_card_kwargs={\n",
    "        \"language\": [\"ja\"],\n",
    "        \"tags\": [\"moinfo-test\", \"timesfm\"],\n",
    "        \"license\": \"apache-2.0\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# 3) config.json だけ変える：config\n",
    "# ※ ここは「再ロード互換」を壊す可能性があるので、まずは“差分が出る”の確認に徹します。\n",
    "exp3 = save_and_report(\n",
    "    \"config_override_test\",\n",
    "    config={\n",
    "        \"_moinfo_note\": \"config override test\",\n",
    "        \"_saved_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    },\n",
    ")\n",
    "\n",
    "# ===== 差分比較（baseline vs others）=====\n",
    "def diff_from_baseline(base, other):\n",
    "    b = {r[\"path\"]: r for r in base[\"files\"]}\n",
    "    o = {r[\"path\"]: r for r in other[\"files\"]}\n",
    "    all_paths = sorted(set(b) | set(o))\n",
    "\n",
    "    changed = []\n",
    "    for p in all_paths:\n",
    "        if p not in b:\n",
    "            changed.append((p, \"ADDED\"))\n",
    "        elif p not in o:\n",
    "            changed.append((p, \"REMOVED\"))\n",
    "        else:\n",
    "            if b[p][\"sha256\"] != o[p][\"sha256\"] or b[p][\"bytes\"] != o[p][\"bytes\"]:\n",
    "                changed.append((p, \"CHANGED\"))\n",
    "    return changed\n",
    "\n",
    "print(\"\\n=== DIFF vs baseline ===\")\n",
    "for exp in [exp2, exp3]:\n",
    "    changed = diff_from_baseline(exp1, exp)\n",
    "    print(f\"\\n[{exp['name']}] changed_files={len(changed)}\")\n",
    "    for p, st in changed:\n",
    "        print(f\"  - {st}: {p}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5852e069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "p1 = Path(r\"C:\\moinfo\\_artifacts\\timesfm_save_pretrained_matrix_tests\\baseline_default\\README.md\")\n",
    "p2 = Path(r\"C:\\moinfo\\_artifacts\\timesfm_save_pretrained_matrix_tests\\model_card_kwargs_test\\README.md\")\n",
    "\n",
    "print(\"same?\", p1.read_bytes() == p2.read_bytes())\n",
    "print(\"baseline head:\\n\", p1.read_text(encoding=\"utf-8\", errors=\"ignore\")[:400])\n",
    "print(\"\\nkwargs head:\\n\", p2.read_text(encoding=\"utf-8\", errors=\"ignore\")[:400])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcd3f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from safetensors.torch import save_file\n",
    "\n",
    "OUT_DIR = Path(r\"C:\\moinfo\\_artifacts\\timesfm_fp16_export\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# TimesFM 2.5の内部が model ならそれを使う\n",
    "core = model.model if hasattr(model, \"model\") else None\n",
    "if core is None:\n",
    "    raise RuntimeError(\"内部 torch.nn.Module が見つかりません（model.model が無い）。\")\n",
    "\n",
    "core.eval()\n",
    "\n",
    "# fp16化した state_dict を作る\n",
    "sd = core.state_dict()\n",
    "sd_fp16 = {}\n",
    "for k, v in sd.items():\n",
    "    if torch.is_floating_point(v):\n",
    "        sd_fp16[k] = v.detach().cpu().half()\n",
    "    else:\n",
    "        sd_fp16[k] = v.detach().cpu()\n",
    "\n",
    "out_path = OUT_DIR / \"model.fp16.safetensors\"\n",
    "save_file(sd_fp16, str(out_path))\n",
    "\n",
    "print(\"saved:\", out_path, \"size(MB)=\", out_path.stat().st_size/1024/1024)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b67268",
   "metadata": {},
   "source": [
    "## state_dict を保存する（.pt と .safetensors 両対応）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d602af41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "def _get_torch_module(timesfm_obj):\n",
    "    \"\"\"TimesFMラッパ or nn.Module どちらでも受けて、内部torch.nn.Moduleを返す。\"\"\"\n",
    "    if isinstance(timesfm_obj, torch.nn.Module):\n",
    "        return timesfm_obj\n",
    "    if hasattr(timesfm_obj, \"model\") and isinstance(timesfm_obj.model, torch.nn.Module):\n",
    "        return timesfm_obj.model\n",
    "    raise TypeError(\"torch.nn.Module が見つかりません。timesfm_obj か timesfm_obj.model を確認してください。\")\n",
    "\n",
    "def save_state_dict(\n",
    "    timesfm_obj,\n",
    "    out_path: str,\n",
    "    *,\n",
    "    dtype: str | None = None,   # None / \"fp16\" / \"bf16\" / \"fp32\"\n",
    "    use_safetensors: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    推論・学習で使う「重み」を保存する。\n",
    "    dtype を指定すると、保存前に重みの型を変換して容量を削減できる（例: fp16）。\n",
    "    \"\"\"\n",
    "    out_path = Path(out_path)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    m = _get_torch_module(timesfm_obj)\n",
    "    sd = m.state_dict()\n",
    "\n",
    "    # dtype変換（容量削減・ただし出力が微妙に変わる可能性あり）\n",
    "    if dtype is not None:\n",
    "        if dtype.lower() == \"fp16\":\n",
    "            cast = torch.float16\n",
    "        elif dtype.lower() == \"bf16\":\n",
    "            cast = torch.bfloat16\n",
    "        elif dtype.lower() == \"fp32\":\n",
    "            cast = torch.float32\n",
    "        else:\n",
    "            raise ValueError(\"dtype は None / 'fp16' / 'bf16' / 'fp32' のみ対応です。\")\n",
    "\n",
    "        sd2 = {}\n",
    "        for k, v in sd.items():\n",
    "            if torch.is_floating_point(v):\n",
    "                sd2[k] = v.detach().cpu().to(cast)\n",
    "            else:\n",
    "                sd2[k] = v.detach().cpu()\n",
    "        sd = sd2\n",
    "    else:\n",
    "        # pickle/safetensorsのため、CPUに移しておくのが安全\n",
    "        sd = {k: v.detach().cpu() for k, v in sd.items()}\n",
    "\n",
    "    if use_safetensors:\n",
    "        from safetensors.torch import save_file\n",
    "        if out_path.suffix.lower() != \".safetensors\":\n",
    "            out_path = out_path.with_suffix(\".safetensors\")\n",
    "        save_file(sd, str(out_path))\n",
    "    else:\n",
    "        if out_path.suffix.lower() != \".pt\":\n",
    "            out_path = out_path.with_suffix(\".pt\")\n",
    "        torch.save(sd, str(out_path))  # PyTorch標準（pickle）\n",
    "\n",
    "    size_mb = out_path.stat().st_size / 1024 / 1024\n",
    "    print(f\"[saved] {out_path} ({size_mb:.2f} MB)\")\n",
    "    return str(out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781ec549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 例：いま使っている TimesFM インスタンスを model だとする\n",
    "# fp32（元のまま）\n",
    "save_state_dict(model, r\"C:\\moinfo\\_artifacts\\state_dict\\timesfm_2p5_fp32.pt\", dtype=None, use_safetensors=False)\n",
    "\n",
    "# fp16（容量削減）\n",
    "save_state_dict(model, r\"C:\\moinfo\\_artifacts\\state_dict\\timesfm_2p5_fp16.safetensors\", dtype=\"fp16\", use_safetensors=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50486572",
   "metadata": {},
   "source": [
    "## state_dict をロードする（strict/map_location 含む）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3588c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "def load_state_dict(\n",
    "    timesfm_obj,\n",
    "    weight_path: str,\n",
    "    *,\n",
    "    strict: bool = True,\n",
    "    map_location: str = \"cpu\",\n",
    "):\n",
    "    \"\"\"\n",
    "    保存した state_dict をロードしてモデルへ反映する。\n",
    "    strict=True：キー不一致があればエラー（再現性重視）\n",
    "    strict=False：多少違っても読み進める（改造・版違いの暫定対応）\n",
    "    \"\"\"\n",
    "    weight_path = Path(weight_path)\n",
    "    m = _get_torch_module(timesfm_obj)\n",
    "\n",
    "    if weight_path.suffix.lower() == \".safetensors\":\n",
    "        from safetensors.torch import load_file\n",
    "        sd = load_file(str(weight_path))  # safetensorsは安全に読める\n",
    "    else:\n",
    "        sd = torch.load(str(weight_path), map_location=map_location)  # PyTorch標準\n",
    "\n",
    "    missing, unexpected = m.load_state_dict(sd, strict=strict)\n",
    "    print(\"[loaded]\", str(weight_path))\n",
    "    print(\"  missing_keys   =\", len(missing))\n",
    "    print(\"  unexpected_keys=\", len(unexpected))\n",
    "    if len(missing) > 0:\n",
    "        print(\"  sample missing:\", missing[:10])\n",
    "    if len(unexpected) > 0:\n",
    "        print(\"  sample unexpected:\", unexpected[:10])\n",
    "    return missing, unexpected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fb5849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 新しく model2 を作る（あなたの既存ロード関数を使う想定）\n",
    "# model2 = load_timesfm_2p5()\n",
    "\n",
    "# fp16を読み込む（注意：ロード後に推論時のdtypeやdeviceの扱いは自分で決める）\n",
    "load_state_dict(model2, r\"C:\\moinfo\\_artifacts\\state_dict\\timesfm_2p5_fp16.safetensors\", strict=True, map_location=\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270d7004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "def save_checkpoint(\n",
    "    timesfm_obj,\n",
    "    out_path: str,\n",
    "    *,\n",
    "    optimizer=None,\n",
    "    scheduler=None,\n",
    "    scaler=None,     # AMP（自動混合精度）を使う場合\n",
    "    step: int | None = None,\n",
    "    epoch: int | None = None,\n",
    "):\n",
    "    out_path = Path(out_path)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    m = _get_torch_module(timesfm_obj)\n",
    "\n",
    "    ckpt = {\n",
    "        \"model_state_dict\": {k: v.detach().cpu() for k, v in m.state_dict().items()},\n",
    "        \"step\": step,\n",
    "        \"epoch\": epoch,\n",
    "    }\n",
    "    if optimizer is not None:\n",
    "        ckpt[\"optimizer_state_dict\"] = optimizer.state_dict()\n",
    "    if scheduler is not None:\n",
    "        ckpt[\"scheduler_state_dict\"] = scheduler.state_dict()\n",
    "    if scaler is not None:\n",
    "        ckpt[\"scaler_state_dict\"] = scaler.state_dict()\n",
    "\n",
    "    torch.save(ckpt, str(out_path))\n",
    "    print(f\"[saved checkpoint] {out_path} ({out_path.stat().st_size/1024/1024:.2f} MB)\")\n",
    "    return str(out_path)\n",
    "\n",
    "def load_checkpoint(\n",
    "    timesfm_obj,\n",
    "    ckpt_path: str,\n",
    "    *,\n",
    "    optimizer=None,\n",
    "    scheduler=None,\n",
    "    scaler=None,\n",
    "    strict: bool = True,\n",
    "    map_location: str = \"cpu\",\n",
    "):\n",
    "    ckpt = torch.load(ckpt_path, map_location=map_location)\n",
    "    m = _get_torch_module(timesfm_obj)\n",
    "\n",
    "    missing, unexpected = m.load_state_dict(ckpt[\"model_state_dict\"], strict=strict)\n",
    "\n",
    "    if optimizer is not None and \"optimizer_state_dict\" in ckpt:\n",
    "        optimizer.load_state_dict(ckpt[\"optimizer_state_dict\"])\n",
    "    if scheduler is not None and \"scheduler_state_dict\" in ckpt:\n",
    "        scheduler.load_state_dict(ckpt[\"scheduler_state_dict\"])\n",
    "    if scaler is not None and \"scaler_state_dict\" in ckpt:\n",
    "        scaler.load_state_dict(ckpt[\"scaler_state_dict\"])\n",
    "\n",
    "    print(\"[loaded checkpoint]\", ckpt_path)\n",
    "    print(\"  step =\", ckpt.get(\"step\"), \"epoch =\", ckpt.get(\"epoch\"))\n",
    "    print(\"  missing_keys =\", len(missing), \"unexpected_keys =\", len(unexpected))\n",
    "    return ckpt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a63a6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import timesfm\n",
    "\n",
    "# --- 設定（あなたのノートに合わせて） ---\n",
    "BATCH_SIZE  = 8\n",
    "CONTEXT_LEN = 512\n",
    "HORIZON_LEN = 128\n",
    "\n",
    "# 既に作ってある前提：\n",
    "# model  = load_timesfm_2p5()           # fp32（元）\n",
    "# model2 = load_timesfm_2p5(); load_state_dict(model2, fp16_path)  # fp16重みをロード済み\n",
    "\n",
    "# --- compile を強制（両方に必要） ---\n",
    "def compile_timesfm(m):\n",
    "    cfg = timesfm.ForecastConfig(\n",
    "        max_context=CONTEXT_LEN,\n",
    "        max_horizon=HORIZON_LEN,\n",
    "        per_core_batch_size=BATCH_SIZE,\n",
    "        normalize_inputs=True,\n",
    "        # XReg（forecast_with_covariates）も使うなら True のままが安全\n",
    "        return_backcast=True,\n",
    "    )\n",
    "    m.compile(cfg)\n",
    "    return cfg\n",
    "\n",
    "print(\"Compiling model (fp32) ...\")\n",
    "cfg1 = compile_timesfm(model)\n",
    "\n",
    "print(\"Compiling model2 (fp16 weights loaded) ...\")\n",
    "cfg2 = compile_timesfm(model2)\n",
    "\n",
    "# --- 入力 ---\n",
    "inputs = [np.sin(np.linspace(0, 30, CONTEXT_LEN)).astype(np.float32) for _ in range(BATCH_SIZE)]\n",
    "\n",
    "# --- 予測（互換：horizon引数 or freq引数の差分を吸収） ---\n",
    "def forecast_point_only(m):\n",
    "    try:\n",
    "        y, _ = m.forecast(horizon=HORIZON_LEN, inputs=inputs)\n",
    "    except TypeError:\n",
    "        y, _ = m.forecast(inputs=inputs, freq=[0]*len(inputs))\n",
    "    y = np.asarray(y)\n",
    "    # return_backcast=True の場合 (B, backcast+horizon) なので末尾だけ取る\n",
    "    return y[:, -HORIZON_LEN:]\n",
    "\n",
    "# --- 差分評価（fp32 vs fp16）---\n",
    "y32 = forecast_point_only(model)\n",
    "y16 = forecast_point_only(model2)\n",
    "\n",
    "abs_err = np.abs(y32 - y16)\n",
    "max_abs = float(abs_err.max())\n",
    "mae = float(abs_err.mean())\n",
    "rmse = float(np.sqrt(((y32 - y16) ** 2).mean()))\n",
    "\n",
    "den = np.maximum(np.abs(y32), 1e-6)\n",
    "rel = abs_err / den\n",
    "max_rel = float(rel.max())\n",
    "mean_rel = float(rel.mean())\n",
    "\n",
    "print(\"\\n=== fp32 vs fp16 difference (forecast horizon only) ===\")\n",
    "print(\"max_abs =\", max_abs)\n",
    "print(\"mae     =\", mae)\n",
    "print(\"rmse    =\", rmse)\n",
    "print(\"max_rel =\", max_rel)\n",
    "print(\"mean_rel=\", mean_rel)\n",
    "\n",
    "# --- 速度（簡易）---\n",
    "def bench(m, loops=10):\n",
    "    _ = forecast_point_only(m)  # warm-up\n",
    "    t0 = time.time()\n",
    "    for _ in range(loops):\n",
    "        _ = forecast_point_only(m)\n",
    "    return (time.time() - t0) / loops\n",
    "\n",
    "t32 = bench(model, loops=10)\n",
    "t16 = bench(model2, loops=10)\n",
    "\n",
    "print(\"\\n=== speed (sec / call) ===\")\n",
    "print(\"fp32:\", t32)\n",
    "print(\"fp16:\", t16)\n",
    "print(\"speedup(fp32/fp16):\", (t32 / t16) if t16 > 0 else None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6517d20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import timesfm\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# ====== 設定 ======\n",
    "BATCH_SIZE  = 8\n",
    "CONTEXT_LEN = 512\n",
    "HORIZON_LEN = 128\n",
    "\n",
    "# ====== dtype 分布の可視化 ======\n",
    "def summarize_param_dtypes(timesfm_obj):\n",
    "    core = timesfm_obj.model if hasattr(timesfm_obj, \"model\") else timesfm_obj\n",
    "    if not isinstance(core, torch.nn.Module):\n",
    "        raise TypeError(\"内部 torch.nn.Module が見つかりません（timesfm_obj.model を確認）。\")\n",
    "\n",
    "    d = {}\n",
    "    for _, v in core.state_dict().items():\n",
    "        if torch.is_floating_point(v):\n",
    "            key = str(v.dtype)\n",
    "            d[key] = d.get(key, 0) + v.numel()\n",
    "\n",
    "    total = sum(d.values()) or 1\n",
    "    print(\"float param dtype distribution (by numel):\")\n",
    "    for k in sorted(d, key=lambda x: -d[x]):\n",
    "        print(f\"  {k}: {d[k] / total * 100:.2f}%\")\n",
    "\n",
    "# ====== compile 設定 ======\n",
    "def compile_cfg():\n",
    "    return timesfm.ForecastConfig(\n",
    "        max_context=CONTEXT_LEN,\n",
    "        max_horizon=HORIZON_LEN,\n",
    "        per_core_batch_size=BATCH_SIZE,\n",
    "        normalize_inputs=True,\n",
    "        return_backcast=True,\n",
    "    )\n",
    "\n",
    "# ====== fp16 化（in-place） ======\n",
    "def to_fp16_inplace(timesfm_obj):\n",
    "    core = timesfm_obj.model if hasattr(timesfm_obj, \"model\") else timesfm_obj\n",
    "    if not isinstance(core, torch.nn.Module):\n",
    "        raise TypeError(\"内部 torch.nn.Module が見つかりません（timesfm_obj.model を確認）。\")\n",
    "    core.eval()\n",
    "    core.half()\n",
    "    return timesfm_obj\n",
    "\n",
    "# ====== 予測（末尾128だけ） ======\n",
    "inputs = [np.sin(np.linspace(0, 30, CONTEXT_LEN)).astype(np.float32) for _ in range(BATCH_SIZE)]\n",
    "\n",
    "def forecast_point_only(m):\n",
    "    y, _ = m.forecast(horizon=HORIZON_LEN, inputs=inputs)\n",
    "    y = np.asarray(y)\n",
    "    return y[:, -HORIZON_LEN:]\n",
    "\n",
    "def bench(m, loops=10):\n",
    "    _ = forecast_point_only(m)  # warm-up\n",
    "    t0 = time.time()\n",
    "    for _ in range(loops):\n",
    "        _ = forecast_point_only(m)\n",
    "    return (time.time() - t0) / loops\n",
    "\n",
    "# ====== 実行：fp16化→compile→検証 ======\n",
    "print(\"=== BEFORE fp16 cast ===\")\n",
    "summarize_param_dtypes(model2)\n",
    "\n",
    "print(\"\\nCasting model2 to fp16 ...\")\n",
    "to_fp16_inplace(model2)\n",
    "\n",
    "print(\"\\nCompiling model2 after fp16 cast ...\")\n",
    "model2.compile(compile_cfg())\n",
    "\n",
    "print(\"\\n=== AFTER fp16 cast ===\")\n",
    "summarize_param_dtypes(model2)\n",
    "\n",
    "# 速度だけ軽く測る（fp16が本当に効いてれば変化が出る可能性）\n",
    "t16 = bench(model2, loops=10)\n",
    "print(\"\\nfp16 model2 sec/call:\", t16)\n",
    "\n",
    "# 参考：GPU確認（使えるなら fp16の恩恵が出やすい）\n",
    "print(\"\\ncuda available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"gpu:\", torch.cuda.get_device_name(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee326080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if hasattr(model2, \"model\") and isinstance(model2.model, torch.nn.Module):\n",
    "    model2.model = model2.model.half()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a16fc99",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd854b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timesfm\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 1. モデルのロード (TimesFM 2.5 - 200Mパラメータ PyTorch版)\n",
    "# GPUが利用可能な場合は自動的にGPUを使用します\n",
    "print(\"Loading model...\")\n",
    "model = timesfm.TimesFM_2p5_200M_torch.from_pretrained(\n",
    "    \"google/timesfm-2.5-200m-pytorch\"\n",
    ")\n",
    "\n",
    "# 2. 予測設定のコンパイル (高速化のため)\n",
    "# max_context: 入力の最大長 (これより長い系列はトリミング)\n",
    "# max_horizon: 予測したい未来の最大ステップ数\n",
    "config = timesfm.ForecastConfig(\n",
    "    max_context=512,       # 入力系列の最大長\n",
    "    max_horizon=128,       # 予測期間の長さ\n",
    "    per_core_batch_size=1, # バッチサイズ\n",
    ")\n",
    "model.compile(config)\n",
    "\n",
    "# 3. ダミーデータの作成 (正弦波など)\n",
    "# 実際にはPandas DataFrameやNumPy配列のリストを渡します\n",
    "t = np.linspace(0, 20, 512)\n",
    "input_signal = np.sin(t) * t  # 振幅が大きくなる正弦波\n",
    "inputs = [input_signal]       # リスト形式で複数の時系列を渡せます\n",
    "\n",
    "# 4. 予測の実行\n",
    "print(\"Forecasting...\")\n",
    "# freq: 頻度 (0:高頻度/不明, 1:中頻度, 2:低頻度)。v2.5では自動推論も可。\n",
    "point_forecast, quantile_forecast = model.forecast(\n",
    "    inputs=inputs,\n",
    "    #freq=[0],  # 0は一般的な時系列(秒, 分, 時間, 日など)を指します\n",
    "    horizon=128\n",
    ")\n",
    "\n",
    "# 5. 結果の表示\n",
    "print(f\"Input shape: {inputs[0].shape}\")\n",
    "print(f\"Forecast shape: {point_forecast.shape}\")\n",
    "print(f\"First 5 predictions: {point_forecast[0, :5]}\")\n",
    "\n",
    "# (オプション) 可視化\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(np.arange(512), inputs[0], label=\"History\")\n",
    "plt.plot(np.arange(512, 512+128), point_forecast[0], label=\"Forecast\")\n",
    "plt.legend()\n",
    "plt.title(\"TimesFM Forecast Sample\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1a719a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e832374448145bfbdb1ee88c314ef6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/475 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded.\n",
      "trainable params: 4,712,448 || all params: 236,001,728 || trainable%: 1.9968\n",
      "Starting training loop...\n",
      "Epoch 1: Loss = 1.5212\n",
      "Epoch 2: Loss = 1.5057\n",
      "Epoch 3: Loss = 1.4902\n",
      "Finetuning finished.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from timesfm import TimesFM_2p5_200M_torch\n",
    "from peft import get_peft_model, LoraConfig\n",
    "\n",
    "# 1. モデルのロード\n",
    "print(\"Loading model...\")\n",
    "tfm = TimesFM_2p5_200M_torch.from_pretrained(\"google/timesfm-2.5-200m-pytorch\")\n",
    "\n",
    "# 2. モデル本体の取り出し\n",
    "# 【重要】ライブラリの仕様上、tfm.model を直接書き換えるとクラス全体に影響するため\n",
    "# 念のため構造確認だけ行い、get_peft_model はこのインスタンスに対して適用します。\n",
    "pytorch_model = tfm.model\n",
    "pytorch_model.train()\n",
    "\n",
    "# 3. LoRA設定\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=\"all-linear\",\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "# 4. LoRA適用\n",
    "# ※ここで pytorch_model が書き換わります\n",
    "model = get_peft_model(pytorch_model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# --- 以降、学習ループ ---\n",
    "# ...\n",
    "# 3. ダミーデータと学習ループの準備\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 【修正ポイント】\n",
    "# TimesFMの入力パッチサイズ (v2.5は 32)\n",
    "patch_len = 32\n",
    "\n",
    "# 入力データを (Batch, Num_Patches, Patch_Len) に変形します\n",
    "# 512ステップ / 32 = 16パッチ\n",
    "train_inputs = torch.randn(32, 512).reshape(32, -1, patch_len).to(tfm.model.device)\n",
    "train_masks = torch.zeros(32, 512).reshape(32, -1, patch_len).to(tfm.model.device)\n",
    "\n",
    "# ターゲットデータ\n",
    "# 出力次元に合わせてターゲットを用意 (簡易的な例)\n",
    "# TimesFMの出力は (Batch, Num_Patches, Output_Dim) です\n",
    "# Output_Dim = 1280 (hidden_dims と同じ)\n",
    "targets = torch.randn(32, 16, 1280).to(tfm.model.device)\n",
    "\n",
    "# 4. 学習ループ (簡易版)\n",
    "print(\"Starting training loop...\")\n",
    "for epoch in range(3):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forwardパス\n",
    "    # output: (input_embeddings, output_embeddings, output_ts, output_quantiles)\n",
    "    outputs, _ = model(train_inputs, train_masks)\n",
    "    \n",
    "    # ここでは出力埋め込み(output_embeddings)をターゲットと比較する例\n",
    "    # 実際には output_ts (予測値のlogits) など、目的に応じた出力を使用します\n",
    "    output_embeddings = outputs[1] # Shape: (32, 16, 1280)\n",
    "    \n",
    "    loss = criterion(output_embeddings, targets)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "print(\"Finetuning finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71730d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moinfo_timesfm_ext import FineTuneConfig, finetune_from_csv, make_dummy_train_csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0873989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/moinfo/data/train.csv')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from moinfo_timesfm_ext import make_dummy_train_csv\n",
    "make_dummy_train_csv(r\"C:\\moinfo\\data\\train.csv\", n_points=40000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bec5fa54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Some weights of the model checkpoint at C:\\moinfo\\timesfm_v2.5_local were not used when initializing TimesFmModelForPrediction: ['output_projection_point.hidden_layer.weight', 'output_projection_point.output_layer.weight', 'output_projection_point.residual_layer.weight', 'output_projection_quantiles.hidden_layer.weight', 'output_projection_quantiles.output_layer.weight', 'output_projection_quantiles.residual_layer.weight', 'stacked_xf.0.attn.key_ln.scale', 'stacked_xf.0.attn.out.weight', 'stacked_xf.0.attn.per_dim_scale.per_dim_scale', 'stacked_xf.0.attn.qkv_proj.weight', 'stacked_xf.0.attn.query_ln.scale', 'stacked_xf.0.ff0.weight', 'stacked_xf.0.ff1.weight', 'stacked_xf.0.post_attn_ln.scale', 'stacked_xf.0.post_ff_ln.scale', 'stacked_xf.0.pre_attn_ln.scale', 'stacked_xf.0.pre_ff_ln.scale', 'stacked_xf.1.attn.key_ln.scale', 'stacked_xf.1.attn.out.weight', 'stacked_xf.1.attn.per_dim_scale.per_dim_scale', 'stacked_xf.1.attn.qkv_proj.weight', 'stacked_xf.1.attn.query_ln.scale', 'stacked_xf.1.ff0.weight', 'stacked_xf.1.ff1.weight', 'stacked_xf.1.post_attn_ln.scale', 'stacked_xf.1.post_ff_ln.scale', 'stacked_xf.1.pre_attn_ln.scale', 'stacked_xf.1.pre_ff_ln.scale', 'stacked_xf.10.attn.key_ln.scale', 'stacked_xf.10.attn.out.weight', 'stacked_xf.10.attn.per_dim_scale.per_dim_scale', 'stacked_xf.10.attn.qkv_proj.weight', 'stacked_xf.10.attn.query_ln.scale', 'stacked_xf.10.ff0.weight', 'stacked_xf.10.ff1.weight', 'stacked_xf.10.post_attn_ln.scale', 'stacked_xf.10.post_ff_ln.scale', 'stacked_xf.10.pre_attn_ln.scale', 'stacked_xf.10.pre_ff_ln.scale', 'stacked_xf.11.attn.key_ln.scale', 'stacked_xf.11.attn.out.weight', 'stacked_xf.11.attn.per_dim_scale.per_dim_scale', 'stacked_xf.11.attn.qkv_proj.weight', 'stacked_xf.11.attn.query_ln.scale', 'stacked_xf.11.ff0.weight', 'stacked_xf.11.ff1.weight', 'stacked_xf.11.post_attn_ln.scale', 'stacked_xf.11.post_ff_ln.scale', 'stacked_xf.11.pre_attn_ln.scale', 'stacked_xf.11.pre_ff_ln.scale', 'stacked_xf.12.attn.key_ln.scale', 'stacked_xf.12.attn.out.weight', 'stacked_xf.12.attn.per_dim_scale.per_dim_scale', 'stacked_xf.12.attn.qkv_proj.weight', 'stacked_xf.12.attn.query_ln.scale', 'stacked_xf.12.ff0.weight', 'stacked_xf.12.ff1.weight', 'stacked_xf.12.post_attn_ln.scale', 'stacked_xf.12.post_ff_ln.scale', 'stacked_xf.12.pre_attn_ln.scale', 'stacked_xf.12.pre_ff_ln.scale', 'stacked_xf.13.attn.key_ln.scale', 'stacked_xf.13.attn.out.weight', 'stacked_xf.13.attn.per_dim_scale.per_dim_scale', 'stacked_xf.13.attn.qkv_proj.weight', 'stacked_xf.13.attn.query_ln.scale', 'stacked_xf.13.ff0.weight', 'stacked_xf.13.ff1.weight', 'stacked_xf.13.post_attn_ln.scale', 'stacked_xf.13.post_ff_ln.scale', 'stacked_xf.13.pre_attn_ln.scale', 'stacked_xf.13.pre_ff_ln.scale', 'stacked_xf.14.attn.key_ln.scale', 'stacked_xf.14.attn.out.weight', 'stacked_xf.14.attn.per_dim_scale.per_dim_scale', 'stacked_xf.14.attn.qkv_proj.weight', 'stacked_xf.14.attn.query_ln.scale', 'stacked_xf.14.ff0.weight', 'stacked_xf.14.ff1.weight', 'stacked_xf.14.post_attn_ln.scale', 'stacked_xf.14.post_ff_ln.scale', 'stacked_xf.14.pre_attn_ln.scale', 'stacked_xf.14.pre_ff_ln.scale', 'stacked_xf.15.attn.key_ln.scale', 'stacked_xf.15.attn.out.weight', 'stacked_xf.15.attn.per_dim_scale.per_dim_scale', 'stacked_xf.15.attn.qkv_proj.weight', 'stacked_xf.15.attn.query_ln.scale', 'stacked_xf.15.ff0.weight', 'stacked_xf.15.ff1.weight', 'stacked_xf.15.post_attn_ln.scale', 'stacked_xf.15.post_ff_ln.scale', 'stacked_xf.15.pre_attn_ln.scale', 'stacked_xf.15.pre_ff_ln.scale', 'stacked_xf.16.attn.key_ln.scale', 'stacked_xf.16.attn.out.weight', 'stacked_xf.16.attn.per_dim_scale.per_dim_scale', 'stacked_xf.16.attn.qkv_proj.weight', 'stacked_xf.16.attn.query_ln.scale', 'stacked_xf.16.ff0.weight', 'stacked_xf.16.ff1.weight', 'stacked_xf.16.post_attn_ln.scale', 'stacked_xf.16.post_ff_ln.scale', 'stacked_xf.16.pre_attn_ln.scale', 'stacked_xf.16.pre_ff_ln.scale', 'stacked_xf.17.attn.key_ln.scale', 'stacked_xf.17.attn.out.weight', 'stacked_xf.17.attn.per_dim_scale.per_dim_scale', 'stacked_xf.17.attn.qkv_proj.weight', 'stacked_xf.17.attn.query_ln.scale', 'stacked_xf.17.ff0.weight', 'stacked_xf.17.ff1.weight', 'stacked_xf.17.post_attn_ln.scale', 'stacked_xf.17.post_ff_ln.scale', 'stacked_xf.17.pre_attn_ln.scale', 'stacked_xf.17.pre_ff_ln.scale', 'stacked_xf.18.attn.key_ln.scale', 'stacked_xf.18.attn.out.weight', 'stacked_xf.18.attn.per_dim_scale.per_dim_scale', 'stacked_xf.18.attn.qkv_proj.weight', 'stacked_xf.18.attn.query_ln.scale', 'stacked_xf.18.ff0.weight', 'stacked_xf.18.ff1.weight', 'stacked_xf.18.post_attn_ln.scale', 'stacked_xf.18.post_ff_ln.scale', 'stacked_xf.18.pre_attn_ln.scale', 'stacked_xf.18.pre_ff_ln.scale', 'stacked_xf.19.attn.key_ln.scale', 'stacked_xf.19.attn.out.weight', 'stacked_xf.19.attn.per_dim_scale.per_dim_scale', 'stacked_xf.19.attn.qkv_proj.weight', 'stacked_xf.19.attn.query_ln.scale', 'stacked_xf.19.ff0.weight', 'stacked_xf.19.ff1.weight', 'stacked_xf.19.post_attn_ln.scale', 'stacked_xf.19.post_ff_ln.scale', 'stacked_xf.19.pre_attn_ln.scale', 'stacked_xf.19.pre_ff_ln.scale', 'stacked_xf.2.attn.key_ln.scale', 'stacked_xf.2.attn.out.weight', 'stacked_xf.2.attn.per_dim_scale.per_dim_scale', 'stacked_xf.2.attn.qkv_proj.weight', 'stacked_xf.2.attn.query_ln.scale', 'stacked_xf.2.ff0.weight', 'stacked_xf.2.ff1.weight', 'stacked_xf.2.post_attn_ln.scale', 'stacked_xf.2.post_ff_ln.scale', 'stacked_xf.2.pre_attn_ln.scale', 'stacked_xf.2.pre_ff_ln.scale', 'stacked_xf.3.attn.key_ln.scale', 'stacked_xf.3.attn.out.weight', 'stacked_xf.3.attn.per_dim_scale.per_dim_scale', 'stacked_xf.3.attn.qkv_proj.weight', 'stacked_xf.3.attn.query_ln.scale', 'stacked_xf.3.ff0.weight', 'stacked_xf.3.ff1.weight', 'stacked_xf.3.post_attn_ln.scale', 'stacked_xf.3.post_ff_ln.scale', 'stacked_xf.3.pre_attn_ln.scale', 'stacked_xf.3.pre_ff_ln.scale', 'stacked_xf.4.attn.key_ln.scale', 'stacked_xf.4.attn.out.weight', 'stacked_xf.4.attn.per_dim_scale.per_dim_scale', 'stacked_xf.4.attn.qkv_proj.weight', 'stacked_xf.4.attn.query_ln.scale', 'stacked_xf.4.ff0.weight', 'stacked_xf.4.ff1.weight', 'stacked_xf.4.post_attn_ln.scale', 'stacked_xf.4.post_ff_ln.scale', 'stacked_xf.4.pre_attn_ln.scale', 'stacked_xf.4.pre_ff_ln.scale', 'stacked_xf.5.attn.key_ln.scale', 'stacked_xf.5.attn.out.weight', 'stacked_xf.5.attn.per_dim_scale.per_dim_scale', 'stacked_xf.5.attn.qkv_proj.weight', 'stacked_xf.5.attn.query_ln.scale', 'stacked_xf.5.ff0.weight', 'stacked_xf.5.ff1.weight', 'stacked_xf.5.post_attn_ln.scale', 'stacked_xf.5.post_ff_ln.scale', 'stacked_xf.5.pre_attn_ln.scale', 'stacked_xf.5.pre_ff_ln.scale', 'stacked_xf.6.attn.key_ln.scale', 'stacked_xf.6.attn.out.weight', 'stacked_xf.6.attn.per_dim_scale.per_dim_scale', 'stacked_xf.6.attn.qkv_proj.weight', 'stacked_xf.6.attn.query_ln.scale', 'stacked_xf.6.ff0.weight', 'stacked_xf.6.ff1.weight', 'stacked_xf.6.post_attn_ln.scale', 'stacked_xf.6.post_ff_ln.scale', 'stacked_xf.6.pre_attn_ln.scale', 'stacked_xf.6.pre_ff_ln.scale', 'stacked_xf.7.attn.key_ln.scale', 'stacked_xf.7.attn.out.weight', 'stacked_xf.7.attn.per_dim_scale.per_dim_scale', 'stacked_xf.7.attn.qkv_proj.weight', 'stacked_xf.7.attn.query_ln.scale', 'stacked_xf.7.ff0.weight', 'stacked_xf.7.ff1.weight', 'stacked_xf.7.post_attn_ln.scale', 'stacked_xf.7.post_ff_ln.scale', 'stacked_xf.7.pre_attn_ln.scale', 'stacked_xf.7.pre_ff_ln.scale', 'stacked_xf.8.attn.key_ln.scale', 'stacked_xf.8.attn.out.weight', 'stacked_xf.8.attn.per_dim_scale.per_dim_scale', 'stacked_xf.8.attn.qkv_proj.weight', 'stacked_xf.8.attn.query_ln.scale', 'stacked_xf.8.ff0.weight', 'stacked_xf.8.ff1.weight', 'stacked_xf.8.post_attn_ln.scale', 'stacked_xf.8.post_ff_ln.scale', 'stacked_xf.8.pre_attn_ln.scale', 'stacked_xf.8.pre_ff_ln.scale', 'stacked_xf.9.attn.key_ln.scale', 'stacked_xf.9.attn.out.weight', 'stacked_xf.9.attn.per_dim_scale.per_dim_scale', 'stacked_xf.9.attn.qkv_proj.weight', 'stacked_xf.9.attn.query_ln.scale', 'stacked_xf.9.ff0.weight', 'stacked_xf.9.ff1.weight', 'stacked_xf.9.post_attn_ln.scale', 'stacked_xf.9.post_ff_ln.scale', 'stacked_xf.9.pre_attn_ln.scale', 'stacked_xf.9.pre_ff_ln.scale', 'tokenizer.hidden_layer.bias', 'tokenizer.hidden_layer.weight', 'tokenizer.output_layer.bias', 'tokenizer.output_layer.weight', 'tokenizer.residual_layer.bias', 'tokenizer.residual_layer.weight']\n",
      "- This IS expected if you are initializing TimesFmModelForPrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TimesFmModelForPrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of TimesFmModelForPrediction were not initialized from the model checkpoint at C:\\moinfo\\timesfm_v2.5_local and are newly initialized: ['decoder.freq_emb.weight', 'decoder.input_ff_layer.input_layer.bias', 'decoder.input_ff_layer.input_layer.weight', 'decoder.input_ff_layer.output_layer.bias', 'decoder.input_ff_layer.output_layer.weight', 'decoder.input_ff_layer.residual_layer.bias', 'decoder.input_ff_layer.residual_layer.weight', 'decoder.layers.0.input_layernorm.weight', 'decoder.layers.0.mlp.down_proj.bias', 'decoder.layers.0.mlp.down_proj.weight', 'decoder.layers.0.mlp.gate_proj.bias', 'decoder.layers.0.mlp.gate_proj.weight', 'decoder.layers.0.mlp.layer_norm.bias', 'decoder.layers.0.mlp.layer_norm.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.o_proj.bias', 'decoder.layers.0.self_attn.o_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.scaling', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.1.input_layernorm.weight', 'decoder.layers.1.mlp.down_proj.bias', 'decoder.layers.1.mlp.down_proj.weight', 'decoder.layers.1.mlp.gate_proj.bias', 'decoder.layers.1.mlp.gate_proj.weight', 'decoder.layers.1.mlp.layer_norm.bias', 'decoder.layers.1.mlp.layer_norm.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.o_proj.bias', 'decoder.layers.1.self_attn.o_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.scaling', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.10.input_layernorm.weight', 'decoder.layers.10.mlp.down_proj.bias', 'decoder.layers.10.mlp.down_proj.weight', 'decoder.layers.10.mlp.gate_proj.bias', 'decoder.layers.10.mlp.gate_proj.weight', 'decoder.layers.10.mlp.layer_norm.bias', 'decoder.layers.10.mlp.layer_norm.weight', 'decoder.layers.10.self_attn.k_proj.bias', 'decoder.layers.10.self_attn.k_proj.weight', 'decoder.layers.10.self_attn.o_proj.bias', 'decoder.layers.10.self_attn.o_proj.weight', 'decoder.layers.10.self_attn.q_proj.bias', 'decoder.layers.10.self_attn.q_proj.weight', 'decoder.layers.10.self_attn.scaling', 'decoder.layers.10.self_attn.v_proj.bias', 'decoder.layers.10.self_attn.v_proj.weight', 'decoder.layers.11.input_layernorm.weight', 'decoder.layers.11.mlp.down_proj.bias', 'decoder.layers.11.mlp.down_proj.weight', 'decoder.layers.11.mlp.gate_proj.bias', 'decoder.layers.11.mlp.gate_proj.weight', 'decoder.layers.11.mlp.layer_norm.bias', 'decoder.layers.11.mlp.layer_norm.weight', 'decoder.layers.11.self_attn.k_proj.bias', 'decoder.layers.11.self_attn.k_proj.weight', 'decoder.layers.11.self_attn.o_proj.bias', 'decoder.layers.11.self_attn.o_proj.weight', 'decoder.layers.11.self_attn.q_proj.bias', 'decoder.layers.11.self_attn.q_proj.weight', 'decoder.layers.11.self_attn.scaling', 'decoder.layers.11.self_attn.v_proj.bias', 'decoder.layers.11.self_attn.v_proj.weight', 'decoder.layers.12.input_layernorm.weight', 'decoder.layers.12.mlp.down_proj.bias', 'decoder.layers.12.mlp.down_proj.weight', 'decoder.layers.12.mlp.gate_proj.bias', 'decoder.layers.12.mlp.gate_proj.weight', 'decoder.layers.12.mlp.layer_norm.bias', 'decoder.layers.12.mlp.layer_norm.weight', 'decoder.layers.12.self_attn.k_proj.bias', 'decoder.layers.12.self_attn.k_proj.weight', 'decoder.layers.12.self_attn.o_proj.bias', 'decoder.layers.12.self_attn.o_proj.weight', 'decoder.layers.12.self_attn.q_proj.bias', 'decoder.layers.12.self_attn.q_proj.weight', 'decoder.layers.12.self_attn.scaling', 'decoder.layers.12.self_attn.v_proj.bias', 'decoder.layers.12.self_attn.v_proj.weight', 'decoder.layers.13.input_layernorm.weight', 'decoder.layers.13.mlp.down_proj.bias', 'decoder.layers.13.mlp.down_proj.weight', 'decoder.layers.13.mlp.gate_proj.bias', 'decoder.layers.13.mlp.gate_proj.weight', 'decoder.layers.13.mlp.layer_norm.bias', 'decoder.layers.13.mlp.layer_norm.weight', 'decoder.layers.13.self_attn.k_proj.bias', 'decoder.layers.13.self_attn.k_proj.weight', 'decoder.layers.13.self_attn.o_proj.bias', 'decoder.layers.13.self_attn.o_proj.weight', 'decoder.layers.13.self_attn.q_proj.bias', 'decoder.layers.13.self_attn.q_proj.weight', 'decoder.layers.13.self_attn.scaling', 'decoder.layers.13.self_attn.v_proj.bias', 'decoder.layers.13.self_attn.v_proj.weight', 'decoder.layers.14.input_layernorm.weight', 'decoder.layers.14.mlp.down_proj.bias', 'decoder.layers.14.mlp.down_proj.weight', 'decoder.layers.14.mlp.gate_proj.bias', 'decoder.layers.14.mlp.gate_proj.weight', 'decoder.layers.14.mlp.layer_norm.bias', 'decoder.layers.14.mlp.layer_norm.weight', 'decoder.layers.14.self_attn.k_proj.bias', 'decoder.layers.14.self_attn.k_proj.weight', 'decoder.layers.14.self_attn.o_proj.bias', 'decoder.layers.14.self_attn.o_proj.weight', 'decoder.layers.14.self_attn.q_proj.bias', 'decoder.layers.14.self_attn.q_proj.weight', 'decoder.layers.14.self_attn.scaling', 'decoder.layers.14.self_attn.v_proj.bias', 'decoder.layers.14.self_attn.v_proj.weight', 'decoder.layers.15.input_layernorm.weight', 'decoder.layers.15.mlp.down_proj.bias', 'decoder.layers.15.mlp.down_proj.weight', 'decoder.layers.15.mlp.gate_proj.bias', 'decoder.layers.15.mlp.gate_proj.weight', 'decoder.layers.15.mlp.layer_norm.bias', 'decoder.layers.15.mlp.layer_norm.weight', 'decoder.layers.15.self_attn.k_proj.bias', 'decoder.layers.15.self_attn.k_proj.weight', 'decoder.layers.15.self_attn.o_proj.bias', 'decoder.layers.15.self_attn.o_proj.weight', 'decoder.layers.15.self_attn.q_proj.bias', 'decoder.layers.15.self_attn.q_proj.weight', 'decoder.layers.15.self_attn.scaling', 'decoder.layers.15.self_attn.v_proj.bias', 'decoder.layers.15.self_attn.v_proj.weight', 'decoder.layers.16.input_layernorm.weight', 'decoder.layers.16.mlp.down_proj.bias', 'decoder.layers.16.mlp.down_proj.weight', 'decoder.layers.16.mlp.gate_proj.bias', 'decoder.layers.16.mlp.gate_proj.weight', 'decoder.layers.16.mlp.layer_norm.bias', 'decoder.layers.16.mlp.layer_norm.weight', 'decoder.layers.16.self_attn.k_proj.bias', 'decoder.layers.16.self_attn.k_proj.weight', 'decoder.layers.16.self_attn.o_proj.bias', 'decoder.layers.16.self_attn.o_proj.weight', 'decoder.layers.16.self_attn.q_proj.bias', 'decoder.layers.16.self_attn.q_proj.weight', 'decoder.layers.16.self_attn.scaling', 'decoder.layers.16.self_attn.v_proj.bias', 'decoder.layers.16.self_attn.v_proj.weight', 'decoder.layers.17.input_layernorm.weight', 'decoder.layers.17.mlp.down_proj.bias', 'decoder.layers.17.mlp.down_proj.weight', 'decoder.layers.17.mlp.gate_proj.bias', 'decoder.layers.17.mlp.gate_proj.weight', 'decoder.layers.17.mlp.layer_norm.bias', 'decoder.layers.17.mlp.layer_norm.weight', 'decoder.layers.17.self_attn.k_proj.bias', 'decoder.layers.17.self_attn.k_proj.weight', 'decoder.layers.17.self_attn.o_proj.bias', 'decoder.layers.17.self_attn.o_proj.weight', 'decoder.layers.17.self_attn.q_proj.bias', 'decoder.layers.17.self_attn.q_proj.weight', 'decoder.layers.17.self_attn.scaling', 'decoder.layers.17.self_attn.v_proj.bias', 'decoder.layers.17.self_attn.v_proj.weight', 'decoder.layers.18.input_layernorm.weight', 'decoder.layers.18.mlp.down_proj.bias', 'decoder.layers.18.mlp.down_proj.weight', 'decoder.layers.18.mlp.gate_proj.bias', 'decoder.layers.18.mlp.gate_proj.weight', 'decoder.layers.18.mlp.layer_norm.bias', 'decoder.layers.18.mlp.layer_norm.weight', 'decoder.layers.18.self_attn.k_proj.bias', 'decoder.layers.18.self_attn.k_proj.weight', 'decoder.layers.18.self_attn.o_proj.bias', 'decoder.layers.18.self_attn.o_proj.weight', 'decoder.layers.18.self_attn.q_proj.bias', 'decoder.layers.18.self_attn.q_proj.weight', 'decoder.layers.18.self_attn.scaling', 'decoder.layers.18.self_attn.v_proj.bias', 'decoder.layers.18.self_attn.v_proj.weight', 'decoder.layers.19.input_layernorm.weight', 'decoder.layers.19.mlp.down_proj.bias', 'decoder.layers.19.mlp.down_proj.weight', 'decoder.layers.19.mlp.gate_proj.bias', 'decoder.layers.19.mlp.gate_proj.weight', 'decoder.layers.19.mlp.layer_norm.bias', 'decoder.layers.19.mlp.layer_norm.weight', 'decoder.layers.19.self_attn.k_proj.bias', 'decoder.layers.19.self_attn.k_proj.weight', 'decoder.layers.19.self_attn.o_proj.bias', 'decoder.layers.19.self_attn.o_proj.weight', 'decoder.layers.19.self_attn.q_proj.bias', 'decoder.layers.19.self_attn.q_proj.weight', 'decoder.layers.19.self_attn.scaling', 'decoder.layers.19.self_attn.v_proj.bias', 'decoder.layers.19.self_attn.v_proj.weight', 'decoder.layers.2.input_layernorm.weight', 'decoder.layers.2.mlp.down_proj.bias', 'decoder.layers.2.mlp.down_proj.weight', 'decoder.layers.2.mlp.gate_proj.bias', 'decoder.layers.2.mlp.gate_proj.weight', 'decoder.layers.2.mlp.layer_norm.bias', 'decoder.layers.2.mlp.layer_norm.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.o_proj.bias', 'decoder.layers.2.self_attn.o_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.scaling', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.3.input_layernorm.weight', 'decoder.layers.3.mlp.down_proj.bias', 'decoder.layers.3.mlp.down_proj.weight', 'decoder.layers.3.mlp.gate_proj.bias', 'decoder.layers.3.mlp.gate_proj.weight', 'decoder.layers.3.mlp.layer_norm.bias', 'decoder.layers.3.mlp.layer_norm.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.o_proj.bias', 'decoder.layers.3.self_attn.o_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.scaling', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.4.input_layernorm.weight', 'decoder.layers.4.mlp.down_proj.bias', 'decoder.layers.4.mlp.down_proj.weight', 'decoder.layers.4.mlp.gate_proj.bias', 'decoder.layers.4.mlp.gate_proj.weight', 'decoder.layers.4.mlp.layer_norm.bias', 'decoder.layers.4.mlp.layer_norm.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.o_proj.bias', 'decoder.layers.4.self_attn.o_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.scaling', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.5.input_layernorm.weight', 'decoder.layers.5.mlp.down_proj.bias', 'decoder.layers.5.mlp.down_proj.weight', 'decoder.layers.5.mlp.gate_proj.bias', 'decoder.layers.5.mlp.gate_proj.weight', 'decoder.layers.5.mlp.layer_norm.bias', 'decoder.layers.5.mlp.layer_norm.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.o_proj.bias', 'decoder.layers.5.self_attn.o_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.scaling', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.6.input_layernorm.weight', 'decoder.layers.6.mlp.down_proj.bias', 'decoder.layers.6.mlp.down_proj.weight', 'decoder.layers.6.mlp.gate_proj.bias', 'decoder.layers.6.mlp.gate_proj.weight', 'decoder.layers.6.mlp.layer_norm.bias', 'decoder.layers.6.mlp.layer_norm.weight', 'decoder.layers.6.self_attn.k_proj.bias', 'decoder.layers.6.self_attn.k_proj.weight', 'decoder.layers.6.self_attn.o_proj.bias', 'decoder.layers.6.self_attn.o_proj.weight', 'decoder.layers.6.self_attn.q_proj.bias', 'decoder.layers.6.self_attn.q_proj.weight', 'decoder.layers.6.self_attn.scaling', 'decoder.layers.6.self_attn.v_proj.bias', 'decoder.layers.6.self_attn.v_proj.weight', 'decoder.layers.7.input_layernorm.weight', 'decoder.layers.7.mlp.down_proj.bias', 'decoder.layers.7.mlp.down_proj.weight', 'decoder.layers.7.mlp.gate_proj.bias', 'decoder.layers.7.mlp.gate_proj.weight', 'decoder.layers.7.mlp.layer_norm.bias', 'decoder.layers.7.mlp.layer_norm.weight', 'decoder.layers.7.self_attn.k_proj.bias', 'decoder.layers.7.self_attn.k_proj.weight', 'decoder.layers.7.self_attn.o_proj.bias', 'decoder.layers.7.self_attn.o_proj.weight', 'decoder.layers.7.self_attn.q_proj.bias', 'decoder.layers.7.self_attn.q_proj.weight', 'decoder.layers.7.self_attn.scaling', 'decoder.layers.7.self_attn.v_proj.bias', 'decoder.layers.7.self_attn.v_proj.weight', 'decoder.layers.8.input_layernorm.weight', 'decoder.layers.8.mlp.down_proj.bias', 'decoder.layers.8.mlp.down_proj.weight', 'decoder.layers.8.mlp.gate_proj.bias', 'decoder.layers.8.mlp.gate_proj.weight', 'decoder.layers.8.mlp.layer_norm.bias', 'decoder.layers.8.mlp.layer_norm.weight', 'decoder.layers.8.self_attn.k_proj.bias', 'decoder.layers.8.self_attn.k_proj.weight', 'decoder.layers.8.self_attn.o_proj.bias', 'decoder.layers.8.self_attn.o_proj.weight', 'decoder.layers.8.self_attn.q_proj.bias', 'decoder.layers.8.self_attn.q_proj.weight', 'decoder.layers.8.self_attn.scaling', 'decoder.layers.8.self_attn.v_proj.bias', 'decoder.layers.8.self_attn.v_proj.weight', 'decoder.layers.9.input_layernorm.weight', 'decoder.layers.9.mlp.down_proj.bias', 'decoder.layers.9.mlp.down_proj.weight', 'decoder.layers.9.mlp.gate_proj.bias', 'decoder.layers.9.mlp.gate_proj.weight', 'decoder.layers.9.mlp.layer_norm.bias', 'decoder.layers.9.mlp.layer_norm.weight', 'decoder.layers.9.self_attn.k_proj.bias', 'decoder.layers.9.self_attn.k_proj.weight', 'decoder.layers.9.self_attn.o_proj.bias', 'decoder.layers.9.self_attn.o_proj.weight', 'decoder.layers.9.self_attn.q_proj.bias', 'decoder.layers.9.self_attn.q_proj.weight', 'decoder.layers.9.self_attn.scaling', 'decoder.layers.9.self_attn.v_proj.bias', 'decoder.layers.9.self_attn.v_proj.weight', 'horizon_ff_layer.input_layer.bias', 'horizon_ff_layer.input_layer.weight', 'horizon_ff_layer.output_layer.bias', 'horizon_ff_layer.output_layer.weight', 'horizon_ff_layer.residual_layer.bias', 'horizon_ff_layer.residual_layer.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m現在のセルまたは前のセルでコードを実行中に、カーネル (Kernel) がクラッシュしました。\n",
      "\u001b[1;31mエラーの原因を特定するには、セル内のコードを確認してください。\n",
      "\u001b[1;31m詳細については<a href='https://aka.ms/vscodeJupyterKernelCrash'>こちら</a>をクリックします。\n",
      "\u001b[1;31m詳細については、Jupyter <a href='command:jupyter.viewOutput'>ログ</a> を参照してください。"
     ]
    }
   ],
   "source": [
    "from moinfo_timesfm_ext import FineTuneConfig, finetune_from_csv\n",
    "\n",
    "cfg = FineTuneConfig(\n",
    "    model_path=r\"C:\\moinfo\\timesfm_v2.5_local\",   # or r\"C:\\moinfo\\timesfm_hf_local\"\n",
    "    csv_path=r\"C:\\moinfo\\data\\train.csv\",\n",
    "    value_col=\"y\",\n",
    "    series_id_col=None,    # 多系列なら \"series_id\" 等\n",
    "    context_len=512,\n",
    "    horizon_len=128,\n",
    "    batch_size=8,\n",
    "    epochs=3,\n",
    "    lr=1e-4,\n",
    "    output_dir=r\"C:\\moinfo\\_artifacts\\timesfm_finetuned\",\n",
    "    device=\"auto\",\n",
    "    dtype=\"auto\",\n",
    ")\n",
    "\n",
    "out_dir = finetune_from_csv(cfg)\n",
    "out_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66011bb",
   "metadata": {},
   "source": [
    "# amazon/chronos-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a280c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully imported library: chronos\n",
      "🔍 Scanning chronos structure... (this may take a moment)\n",
      "📊 Found 90 accessible objects (classes/functions).\n",
      "\n",
      "=== 🧪 Feature Verification Report: Chronos-2 Claims ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Status</th>\n",
       "      <th>Hit Count</th>\n",
       "      <th>Keywords Used</th>\n",
       "      <th>Evidence (Sample Objects)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Multivariate (多変量)</td>\n",
       "      <td>✅ Confirmed</td>\n",
       "      <td>3</td>\n",
       "      <td>multivariate, group_attention, jointly predict, co-evolving, ndim</td>\n",
       "      <td>left_pad_and_stack_1D (ndim)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Covariates (共変量/外部変数)</td>\n",
       "      <td>✅ Confirmed</td>\n",
       "      <td>11</td>\n",
       "      <td>covariate, exogenous, future_features, past_features, feat_dynamic, static_features</td>\n",
       "      <td>Chronos2Dataset (covariate), Chronos2Model (covariate)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Probabilistic/Quantile (確率的予測)</td>\n",
       "      <td>✅ Confirmed</td>\n",
       "      <td>36</td>\n",
       "      <td>quantile, probabilistic, prediction_interval, forecast_distribution, sample</td>\n",
       "      <td>ForecastType (sample), Chronos2Pipeline (quantile), Chronos2ForecastingConfig (quantile), Chrono...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Zero-Shot / Pretrained (事前学習)</td>\n",
       "      <td>✅ Confirmed</td>\n",
       "      <td>34</td>\n",
       "      <td>pretrained, zero-shot, pipeline, from_pretrained</td>\n",
       "      <td>Chronos2Model (pretrained), Chronos2Pipeline (pipeline), ChronosBoltModelForForecasting (from_pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Handling Missing Data (欠損値)</td>\n",
       "      <td>✅ Confirmed</td>\n",
       "      <td>29</td>\n",
       "      <td>mask, missing, nan, pad</td>\n",
       "      <td>validate_and_prepare_single_dict_task (nan), TimeCrossAttention (mask), Chronos2CoreConfig (miss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Arbitrary Dimensions (任意次元)</td>\n",
       "      <td>✅ Confirmed</td>\n",
       "      <td>6</td>\n",
       "      <td>arbitrary, dimension, flexible</td>\n",
       "      <td>InstanceNorm (dimension), interpolate_quantiles (dimension), weighted_quantile (dimension)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Category       Status  Hit Count  \\\n",
       "0              Multivariate (多変量)  ✅ Confirmed          3   \n",
       "1           Covariates (共変量/外部変数)  ✅ Confirmed         11   \n",
       "2  Probabilistic/Quantile (確率的予測)  ✅ Confirmed         36   \n",
       "3   Zero-Shot / Pretrained (事前学習)  ✅ Confirmed         34   \n",
       "4     Handling Missing Data (欠損値)  ✅ Confirmed         29   \n",
       "5     Arbitrary Dimensions (任意次元)  ✅ Confirmed          6   \n",
       "\n",
       "                                                                         Keywords Used  \\\n",
       "0                    multivariate, group_attention, jointly predict, co-evolving, ndim   \n",
       "1  covariate, exogenous, future_features, past_features, feat_dynamic, static_features   \n",
       "2          quantile, probabilistic, prediction_interval, forecast_distribution, sample   \n",
       "3                                     pretrained, zero-shot, pipeline, from_pretrained   \n",
       "4                                                              mask, missing, nan, pad   \n",
       "5                                                       arbitrary, dimension, flexible   \n",
       "\n",
       "                                                                             Evidence (Sample Objects)  \n",
       "0                                                                         left_pad_and_stack_1D (ndim)  \n",
       "1                                               Chronos2Dataset (covariate), Chronos2Model (covariate)  \n",
       "2  ForecastType (sample), Chronos2Pipeline (quantile), Chronos2ForecastingConfig (quantile), Chrono...  \n",
       "3  Chronos2Model (pretrained), Chronos2Pipeline (pipeline), ChronosBoltModelForForecasting (from_pr...  \n",
       "4  validate_and_prepare_single_dict_task (nan), TimeCrossAttention (mask), Chronos2CoreConfig (miss...  \n",
       "5           InstanceNorm (dimension), interpolate_quantiles (dimension), weighted_quantile (dimension)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import inspect\n",
    "import importlib\n",
    "import pkgutil\n",
    "import pandas as pd\n",
    "import re\n",
    "import types\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "class LibraryFeatureInspector:\n",
    "    \"\"\"\n",
    "    指定されたライブラリを探索し、特定の機能（キーワード）に関連する\n",
    "    クラス、メソッド、ドキュメントを抽出してファクトチェックを行うクラス。\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, library_name: str):\n",
    "        self.library_name = library_name\n",
    "        self.found_items = []\n",
    "        try:\n",
    "            self.module = importlib.import_module(library_name)\n",
    "            print(f\"✅ Successfully imported library: {library_name}\")\n",
    "        except ImportError:\n",
    "            print(f\"❌ Library '{library_name}' not found. Please install it first (e.g., !pip install chronos-forecasting).\")\n",
    "            self.module = None\n",
    "\n",
    "    def _recursive_inspect(self, module: Any, visited: set, depth: int = 0, max_depth: int = 3):\n",
    "        \"\"\"モジュールを再帰的に探索してクラスと関数を収集\"\"\"\n",
    "        if module in visited or depth > max_depth:\n",
    "            return\n",
    "        visited.add(module)\n",
    "\n",
    "        # モジュール内の属性を走査\n",
    "        for name, obj in inspect.getmembers(module):\n",
    "            if name.startswith(\"_\"): continue\n",
    "\n",
    "            # クラスまたは関数の場合\n",
    "            if inspect.isclass(obj) or inspect.isfunction(obj):\n",
    "                if hasattr(obj, '__module__') and obj.__module__ and obj.__module__.startswith(self.library_name):\n",
    "                    doc = inspect.getdoc(obj) or \"\"\n",
    "                    # ソースコードの取得（可能な場合）\n",
    "                    try:\n",
    "                        source = inspect.getsource(obj)\n",
    "                    except (OSError, TypeError):\n",
    "                        source = \"\"\n",
    "                    \n",
    "                    self.found_items.append({\n",
    "                        \"name\": name,\n",
    "                        \"type\": \"class\" if inspect.isclass(obj) else \"function\",\n",
    "                        \"module\": obj.__module__,\n",
    "                        \"docstring\": doc,\n",
    "                        \"source_preview\": source[:500] # 最初の500文字を確認用に保持\n",
    "                    })\n",
    "            \n",
    "            # サブモジュールの探索\n",
    "            elif isinstance(obj, types.ModuleType):\n",
    "                if hasattr(obj, '__name__') and obj.__name__.startswith(self.library_name):\n",
    "                    self._recursive_inspect(obj, visited, depth + 1, max_depth)\n",
    "\n",
    "    def scan_library(self):\n",
    "        \"\"\"ライブラリ全体をスキャン\"\"\"\n",
    "        if not self.module: return\n",
    "        print(f\"🔍 Scanning {self.library_name} structure... (this may take a moment)\")\n",
    "        visited = set()\n",
    "        self._recursive_inspect(self.module, visited)\n",
    "        print(f\"📊 Found {len(self.found_items)} accessible objects (classes/functions).\")\n",
    "\n",
    "    def verify_features(self, feature_queries: Dict[str, List[str]]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        特定の機能キーワードがドキュメントやコード内に存在するか検証する\n",
    "        \n",
    "        Args:\n",
    "            feature_queries: { \"機能カテゴリ\": [\"検索キーワード1\", \"検索キーワード2\"] }\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        df_items = pd.DataFrame(self.found_items)\n",
    "        if df_items.empty:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        for category, keywords in feature_queries.items():\n",
    "            found_count = 0\n",
    "            evidence = []\n",
    "            \n",
    "            for keyword in keywords:\n",
    "                # ドキュメントまたはソースコードにキーワードが含まれるか検索 (Case Insensitive)\n",
    "                mask = (\n",
    "                    df_items[\"docstring\"].str.contains(keyword, case=False, na=False) | \n",
    "                    df_items[\"name\"].str.contains(keyword, case=False, na=False) |\n",
    "                    df_items[\"source_preview\"].str.contains(keyword, case=False, na=False)\n",
    "                )\n",
    "                matched = df_items[mask]\n",
    "                \n",
    "                if not matched.empty:\n",
    "                    found_count += len(matched)\n",
    "                    # 証拠として上位3つのオブジェクト名を記録\n",
    "                    top_matches = matched[\"name\"].head(3).tolist()\n",
    "                    evidence.extend([f\"{m} ({keyword})\" for m in top_matches])\n",
    "\n",
    "            # 結果の判定\n",
    "            status = \"✅ Confirmed\" if found_count > 0 else \"❓ Not Found\"\n",
    "            confidence = \"High\" if found_count > 5 else (\"Low\" if found_count > 0 else \"None\")\n",
    "            \n",
    "            results.append({\n",
    "                \"Category\": category,\n",
    "                \"Status\": status,\n",
    "                \"Hit Count\": found_count,\n",
    "                \"Keywords Used\": \", \".join(keywords),\n",
    "                \"Evidence (Sample Objects)\": \", \".join(list(set(evidence))[:5]) # 重複排除して表示\n",
    "            })\n",
    "            \n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "# --- 実行設定 ---\n",
    "\n",
    "# 1. 解析対象のライブラリを指定 ('chronos' または 比較のために 'timesfm' も可)\n",
    "# ※ Chronos-2のパッケージ名は通常 'chronos' です\n",
    "TARGET_LIBRARY = \"chronos\" \n",
    "\n",
    "# 2. 検証したい機能（ファクトチェック対象）と検索キーワードの定義\n",
    "feature_definitions = {\n",
    "    \"Multivariate (多変量)\": [\n",
    "        \"multivariate\", \"group_attention\", \"jointly predict\", \"co-evolving\", \"ndim\"\n",
    "    ],\n",
    "    \"Covariates (共変量/外部変数)\": [\n",
    "        \"covariate\", \"exogenous\", \"future_features\", \"past_features\", \"feat_dynamic\", \"static_features\"\n",
    "    ],\n",
    "    \"Probabilistic/Quantile (確率的予測)\": [\n",
    "        \"quantile\", \"probabilistic\", \"prediction_interval\", \"forecast_distribution\", \"sample\"\n",
    "    ],\n",
    "    \"Zero-Shot / Pretrained (事前学習)\": [\n",
    "        \"pretrained\", \"zero-shot\", \"pipeline\", \"from_pretrained\"\n",
    "    ],\n",
    "    \"Handling Missing Data (欠損値)\": [\n",
    "        \"mask\", \"missing\", \"nan\", \"pad\"\n",
    "    ],\n",
    "    \"Arbitrary Dimensions (任意次元)\": [\n",
    "        \"arbitrary\", \"dimension\", \"flexible\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# --- 実行プロセス ---\n",
    "inspector = LibraryFeatureInspector(TARGET_LIBRARY)\n",
    "\n",
    "# ライブラリがインストールされている場合のみ実行\n",
    "if inspector.module:\n",
    "    inspector.scan_library()\n",
    "    df_verification = inspector.verify_features(feature_definitions)\n",
    "    \n",
    "    print(\"\\n=== 🧪 Feature Verification Report: Chronos-2 Claims ===\")\n",
    "    display(df_verification) # Jupyter環境での表示用\n",
    "    \n",
    "    # 必要に応じてCSV保存\n",
    "    # df_verification.to_csv(\"chronos_feature_verification.csv\", index=False)\n",
    "else:\n",
    "    print(\"\\nℹ️ Chronos-2を検証するには、以下のコマンドでインストールしてください:\")\n",
    "    print(\"!pip install chronos-forecasting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b937d78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Exploring library: chronos\n",
      "\n",
      "=== 📦 Chronos-2 Full Capability Catalog ===\n",
      "\n",
      "🔹 Core Models & Pipelines:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Module</th>\n",
       "      <th>Name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Signature</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chronos.base</td>\n",
       "      <td>BaseChronosPipeline</td>\n",
       "      <td>Class</td>\n",
       "      <td>(inner_model: 'PreTrainedModel')</td>\n",
       "      <td>(No description available)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>chronos.chronos2</td>\n",
       "      <td>Chronos2CoreConfig</td>\n",
       "      <td>Class</td>\n",
       "      <td>(d_model: int = 512, d_kv: int = 64, d_ff: int = 2048, num_layers: int = 6, num_heads: int = 8, ...</td>\n",
       "      <td>HF transformers-style pretrained model config for Chronos-2.0, based on T5Config.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>chronos.chronos2</td>\n",
       "      <td>Chronos2Dataset</td>\n",
       "      <td>Class</td>\n",
       "      <td>(inputs: Sequence[Mapping[str, Union[torch.Tensor, numpy.ndarray, Mapping[str, torch.Tensor | nu...</td>\n",
       "      <td>A dataset wrapper for Chronos-2 models.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>chronos.chronos2.model</td>\n",
       "      <td>Chronos2Encoder</td>\n",
       "      <td>Class</td>\n",
       "      <td>(config: chronos.chronos2.config.Chronos2CoreConfig)</td>\n",
       "      <td>Base class for all neural network modules.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>chronos.chronos2.model</td>\n",
       "      <td>Chronos2EncoderBlock</td>\n",
       "      <td>Class</td>\n",
       "      <td>(config: chronos.chronos2.config.Chronos2CoreConfig)</td>\n",
       "      <td>Base class for all neural network modules.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>chronos.chronos2.model</td>\n",
       "      <td>Chronos2EncoderBlockOutput</td>\n",
       "      <td>Class</td>\n",
       "      <td>(hidden_states: torch.Tensor | None = None, time_self_attn_weights: torch.Tensor | None = None, ...</td>\n",
       "      <td>Chronos2EncoderBlockOutput(hidden_states: torch.Tensor | None = None, time_self_attn_weights: to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>chronos.chronos2.model</td>\n",
       "      <td>Chronos2EncoderOutput</td>\n",
       "      <td>Class</td>\n",
       "      <td>(last_hidden_state: torch.Tensor | None = None, all_time_self_attn_weights: tuple[torch.Tensor, ...</td>\n",
       "      <td>Chronos2EncoderOutput(last_hidden_state: torch.Tensor | None = None, all_time_self_attn_weights:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>chronos.chronos2</td>\n",
       "      <td>Chronos2ForecastingConfig</td>\n",
       "      <td>Class</td>\n",
       "      <td>(context_length: int, output_patch_size: int, input_patch_size: int, input_patch_stride: int, qu...</td>\n",
       "      <td>Chronos2ForecastingConfig(context_length: int, output_patch_size: int, input_patch_size: int, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>chronos.chronos2.layers</td>\n",
       "      <td>Chronos2LayerNorm</td>\n",
       "      <td>Class</td>\n",
       "      <td>(hidden_size: int, eps: float = 1e-06)</td>\n",
       "      <td>Base class for all neural network modules.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>chronos.chronos2</td>\n",
       "      <td>Chronos2Model</td>\n",
       "      <td>Class</td>\n",
       "      <td>(config: chronos.chronos2.config.Chronos2CoreConfig)</td>\n",
       "      <td>Base class for all models.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>chronos.chronos2.model</td>\n",
       "      <td>Chronos2Output</td>\n",
       "      <td>Class</td>\n",
       "      <td>(loss: torch.Tensor | None = None, quantile_preds: torch.Tensor | None = None, enc_time_self_att...</td>\n",
       "      <td>Chronos2Output(loss: torch.Tensor | None = None, quantile_preds: torch.Tensor | None = None, enc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>chronos.chronos2</td>\n",
       "      <td>Chronos2Pipeline</td>\n",
       "      <td>Class</td>\n",
       "      <td>(model: chronos.chronos2.model.Chronos2Model)</td>\n",
       "      <td>(No description available)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>chronos.chronos2.trainer</td>\n",
       "      <td>Chronos2Trainer</td>\n",
       "      <td>Class</td>\n",
       "      <td>(model: Union[transformers.modeling_utils.PreTrainedModel, torch.nn.modules.module.Module, NoneT...</td>\n",
       "      <td>A custom trainer based on transformers Trainer. We need to override the dataloader getters becau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>chronos.chronos_bolt</td>\n",
       "      <td>ChronosBoltConfig</td>\n",
       "      <td>Class</td>\n",
       "      <td>(context_length: int, prediction_length: int, input_patch_size: int, input_patch_stride: int, qu...</td>\n",
       "      <td>ChronosBoltConfig(context_length: int, prediction_length: int, input_patch_size: int, input_patc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>chronos.chronos_bolt</td>\n",
       "      <td>ChronosBoltModelForForecasting</td>\n",
       "      <td>Class</td>\n",
       "      <td>(config: transformers.models.t5.configuration_t5.T5Config)</td>\n",
       "      <td>This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>chronos.chronos_bolt</td>\n",
       "      <td>ChronosBoltOutput</td>\n",
       "      <td>Class</td>\n",
       "      <td>(loss: Optional[torch.Tensor] = None, quantile_preds: Optional[torch.Tensor] = None, attentions:...</td>\n",
       "      <td>ChronosBoltOutput(loss: Optional[torch.Tensor] = None, quantile_preds: Optional[torch.Tensor] = ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>chronos.chronos_bolt</td>\n",
       "      <td>ChronosBoltPipeline</td>\n",
       "      <td>Class</td>\n",
       "      <td>(model: chronos.chronos_bolt.ChronosBoltModelForForecasting)</td>\n",
       "      <td>(No description available)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>chronos.chronos</td>\n",
       "      <td>ChronosConfig</td>\n",
       "      <td>Class</td>\n",
       "      <td>(tokenizer_class: str, tokenizer_kwargs: Dict[str, Any], context_length: int, prediction_length:...</td>\n",
       "      <td>This class holds all the configuration parameters to be used</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>chronos.chronos</td>\n",
       "      <td>ChronosModel</td>\n",
       "      <td>Class</td>\n",
       "      <td>(config: chronos.chronos.ChronosConfig, model: transformers.modeling_utils.PreTrainedModel) -&gt; None</td>\n",
       "      <td>A ``ChronosModel`` wraps a ``PreTrainedModel`` object from ``transformers``</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>chronos.chronos</td>\n",
       "      <td>ChronosPipeline</td>\n",
       "      <td>Class</td>\n",
       "      <td>(tokenizer, model)</td>\n",
       "      <td>A ``ChronosPipeline`` uses the given tokenizer and model to forecast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>chronos.chronos</td>\n",
       "      <td>ChronosTokenizer</td>\n",
       "      <td>Class</td>\n",
       "      <td>()</td>\n",
       "      <td>A ``ChronosTokenizer`` defines how time series are mapped into token IDs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chronos.base</td>\n",
       "      <td>PipelineRegistry</td>\n",
       "      <td>Class</td>\n",
       "      <td>(name, bases, attrs)</td>\n",
       "      <td>type(object) -&gt; the object's type</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chronos.boto_utils</td>\n",
       "      <td>cache_model_from_s3</td>\n",
       "      <td>Function</td>\n",
       "      <td>(s3_uri: str, force_download: bool = False, boto3_session: boto3.session.Session | None = None)</td>\n",
       "      <td>(No description available)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>chronos.boto_utils</td>\n",
       "      <td>download_model_files_from_cloudfront</td>\n",
       "      <td>Function</td>\n",
       "      <td>(cloudfront_url: str, bucket: str, prefix: str, local_path: pathlib.Path, force_download: bool =...</td>\n",
       "      <td>(No description available)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>chronos.boto_utils</td>\n",
       "      <td>download_model_files_from_s3</td>\n",
       "      <td>Function</td>\n",
       "      <td>(bucket: str, prefix: str, local_path: pathlib.Path, force_download: bool = False, boto3_session...</td>\n",
       "      <td>(No description available)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Module                                  Name      Type  \\\n",
       "0               chronos.base                   BaseChronosPipeline     Class   \n",
       "15          chronos.chronos2                    Chronos2CoreConfig     Class   \n",
       "16          chronos.chronos2                       Chronos2Dataset     Class   \n",
       "42    chronos.chronos2.model                       Chronos2Encoder     Class   \n",
       "43    chronos.chronos2.model                  Chronos2EncoderBlock     Class   \n",
       "44    chronos.chronos2.model            Chronos2EncoderBlockOutput     Class   \n",
       "45    chronos.chronos2.model                 Chronos2EncoderOutput     Class   \n",
       "17          chronos.chronos2             Chronos2ForecastingConfig     Class   \n",
       "31   chronos.chronos2.layers                     Chronos2LayerNorm     Class   \n",
       "18          chronos.chronos2                         Chronos2Model     Class   \n",
       "49    chronos.chronos2.model                        Chronos2Output     Class   \n",
       "19          chronos.chronos2                      Chronos2Pipeline     Class   \n",
       "67  chronos.chronos2.trainer                       Chronos2Trainer     Class   \n",
       "71      chronos.chronos_bolt                     ChronosBoltConfig     Class   \n",
       "72      chronos.chronos_bolt        ChronosBoltModelForForecasting     Class   \n",
       "73      chronos.chronos_bolt                     ChronosBoltOutput     Class   \n",
       "74      chronos.chronos_bolt                   ChronosBoltPipeline     Class   \n",
       "8            chronos.chronos                         ChronosConfig     Class   \n",
       "9            chronos.chronos                          ChronosModel     Class   \n",
       "10           chronos.chronos                       ChronosPipeline     Class   \n",
       "11           chronos.chronos                      ChronosTokenizer     Class   \n",
       "2               chronos.base                      PipelineRegistry     Class   \n",
       "4         chronos.boto_utils                   cache_model_from_s3  Function   \n",
       "5         chronos.boto_utils  download_model_files_from_cloudfront  Function   \n",
       "6         chronos.boto_utils          download_model_files_from_s3  Function   \n",
       "\n",
       "                                                                                              Signature  \\\n",
       "0                                                                      (inner_model: 'PreTrainedModel')   \n",
       "15  (d_model: int = 512, d_kv: int = 64, d_ff: int = 2048, num_layers: int = 6, num_heads: int = 8, ...   \n",
       "16  (inputs: Sequence[Mapping[str, Union[torch.Tensor, numpy.ndarray, Mapping[str, torch.Tensor | nu...   \n",
       "42                                                 (config: chronos.chronos2.config.Chronos2CoreConfig)   \n",
       "43                                                 (config: chronos.chronos2.config.Chronos2CoreConfig)   \n",
       "44  (hidden_states: torch.Tensor | None = None, time_self_attn_weights: torch.Tensor | None = None, ...   \n",
       "45  (last_hidden_state: torch.Tensor | None = None, all_time_self_attn_weights: tuple[torch.Tensor, ...   \n",
       "17  (context_length: int, output_patch_size: int, input_patch_size: int, input_patch_stride: int, qu...   \n",
       "31                                                               (hidden_size: int, eps: float = 1e-06)   \n",
       "18                                                 (config: chronos.chronos2.config.Chronos2CoreConfig)   \n",
       "49  (loss: torch.Tensor | None = None, quantile_preds: torch.Tensor | None = None, enc_time_self_att...   \n",
       "19                                                        (model: chronos.chronos2.model.Chronos2Model)   \n",
       "67  (model: Union[transformers.modeling_utils.PreTrainedModel, torch.nn.modules.module.Module, NoneT...   \n",
       "71  (context_length: int, prediction_length: int, input_patch_size: int, input_patch_stride: int, qu...   \n",
       "72                                           (config: transformers.models.t5.configuration_t5.T5Config)   \n",
       "73  (loss: Optional[torch.Tensor] = None, quantile_preds: Optional[torch.Tensor] = None, attentions:...   \n",
       "74                                         (model: chronos.chronos_bolt.ChronosBoltModelForForecasting)   \n",
       "8   (tokenizer_class: str, tokenizer_kwargs: Dict[str, Any], context_length: int, prediction_length:...   \n",
       "9   (config: chronos.chronos.ChronosConfig, model: transformers.modeling_utils.PreTrainedModel) -> None   \n",
       "10                                                                                   (tokenizer, model)   \n",
       "11                                                                                                   ()   \n",
       "2                                                                                  (name, bases, attrs)   \n",
       "4       (s3_uri: str, force_download: bool = False, boto3_session: boto3.session.Session | None = None)   \n",
       "5   (cloudfront_url: str, bucket: str, prefix: str, local_path: pathlib.Path, force_download: bool =...   \n",
       "6   (bucket: str, prefix: str, local_path: pathlib.Path, force_download: bool = False, boto3_session...   \n",
       "\n",
       "                                                                                            Description  \n",
       "0                                                                            (No description available)  \n",
       "15                    HF transformers-style pretrained model config for Chronos-2.0, based on T5Config.  \n",
       "16                                                              A dataset wrapper for Chronos-2 models.  \n",
       "42                                                           Base class for all neural network modules.  \n",
       "43                                                           Base class for all neural network modules.  \n",
       "44  Chronos2EncoderBlockOutput(hidden_states: torch.Tensor | None = None, time_self_attn_weights: to...  \n",
       "45  Chronos2EncoderOutput(last_hidden_state: torch.Tensor | None = None, all_time_self_attn_weights:...  \n",
       "17  Chronos2ForecastingConfig(context_length: int, output_patch_size: int, input_patch_size: int, in...  \n",
       "31                                                           Base class for all neural network modules.  \n",
       "18                                                                           Base class for all models.  \n",
       "49  Chronos2Output(loss: torch.Tensor | None = None, quantile_preds: torch.Tensor | None = None, enc...  \n",
       "19                                                                           (No description available)  \n",
       "67  A custom trainer based on transformers Trainer. We need to override the dataloader getters becau...  \n",
       "71  ChronosBoltConfig(context_length: int, prediction_length: int, input_patch_size: int, input_patc...  \n",
       "72  This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic...  \n",
       "73  ChronosBoltOutput(loss: Optional[torch.Tensor] = None, quantile_preds: Optional[torch.Tensor] = ...  \n",
       "74                                                                           (No description available)  \n",
       "8                                          This class holds all the configuration parameters to be used  \n",
       "9                           A ``ChronosModel`` wraps a ``PreTrainedModel`` object from ``transformers``  \n",
       "10                                 A ``ChronosPipeline`` uses the given tokenizer and model to forecast  \n",
       "11                             A ``ChronosTokenizer`` defines how time series are mapped into token IDs  \n",
       "2                                                                     type(object) -> the object's type  \n",
       "4                                                                            (No description available)  \n",
       "5                                                                            (No description available)  \n",
       "6                                                                            (No description available)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Configuration & Settings:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Module</th>\n",
       "      <th>Name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Signature</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>chronos.chronos2</td>\n",
       "      <td>Chronos2CoreConfig</td>\n",
       "      <td>Class</td>\n",
       "      <td>(d_model: int = 512, d_kv: int = 64, d_ff: int = 2048, num_layers: int = 6, num_heads: int = 8, ...</td>\n",
       "      <td>HF transformers-style pretrained model config for Chronos-2.0, based on T5Config.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>chronos.chronos2</td>\n",
       "      <td>Chronos2ForecastingConfig</td>\n",
       "      <td>Class</td>\n",
       "      <td>(context_length: int, output_patch_size: int, input_patch_size: int, input_patch_stride: int, qu...</td>\n",
       "      <td>Chronos2ForecastingConfig(context_length: int, output_patch_size: int, input_patch_size: int, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>chronos.chronos_bolt</td>\n",
       "      <td>ChronosBoltConfig</td>\n",
       "      <td>Class</td>\n",
       "      <td>(context_length: int, prediction_length: int, input_patch_size: int, input_patch_stride: int, qu...</td>\n",
       "      <td>ChronosBoltConfig(context_length: int, prediction_length: int, input_patch_size: int, input_patc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>chronos.chronos</td>\n",
       "      <td>ChronosConfig</td>\n",
       "      <td>Class</td>\n",
       "      <td>(tokenizer_class: str, tokenizer_kwargs: Dict[str, Any], context_length: int, prediction_length:...</td>\n",
       "      <td>This class holds all the configuration parameters to be used</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Module                       Name   Type  \\\n",
       "15      chronos.chronos2         Chronos2CoreConfig  Class   \n",
       "17      chronos.chronos2  Chronos2ForecastingConfig  Class   \n",
       "71  chronos.chronos_bolt          ChronosBoltConfig  Class   \n",
       "8        chronos.chronos              ChronosConfig  Class   \n",
       "\n",
       "                                                                                              Signature  \\\n",
       "15  (d_model: int = 512, d_kv: int = 64, d_ff: int = 2048, num_layers: int = 6, num_heads: int = 8, ...   \n",
       "17  (context_length: int, output_patch_size: int, input_patch_size: int, input_patch_stride: int, qu...   \n",
       "71  (context_length: int, prediction_length: int, input_patch_size: int, input_patch_stride: int, qu...   \n",
       "8   (tokenizer_class: str, tokenizer_kwargs: Dict[str, Any], context_length: int, prediction_length:...   \n",
       "\n",
       "                                                                                            Description  \n",
       "15                    HF transformers-style pretrained model config for Chronos-2.0, based on T5Config.  \n",
       "17  Chronos2ForecastingConfig(context_length: int, output_patch_size: int, input_patch_size: int, in...  \n",
       "71  ChronosBoltConfig(context_length: int, prediction_length: int, input_patch_size: int, input_patc...  \n",
       "8                                          This class holds all the configuration parameters to be used  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Utilities & Data Processing:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Module</th>\n",
       "      <th>Name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Signature</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>chronos.chronos2.layers</td>\n",
       "      <td>AttentionOutput</td>\n",
       "      <td>Class</td>\n",
       "      <td>(hidden_states: torch.Tensor | None = None, attn_weights: torch.Tensor | None = None) -&gt; None</td>\n",
       "      <td>AttentionOutput(hidden_states: torch.Tensor | None = None, attn_weights: torch.Tensor | None = N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>chronos.chronos2.dataset</td>\n",
       "      <td>DatasetMode</td>\n",
       "      <td>Class</td>\n",
       "      <td>(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)</td>\n",
       "      <td>str(object='') -&gt; str</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>chronos.chronos2.trainer</td>\n",
       "      <td>EvaluateAndSaveFinalStepCallback</td>\n",
       "      <td>Class</td>\n",
       "      <td>()</td>\n",
       "      <td>Callback to evaluate and save the model at last training step.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>chronos.chronos2.layers</td>\n",
       "      <td>FeedForward</td>\n",
       "      <td>Class</td>\n",
       "      <td>(config: chronos.chronos2.config.Chronos2CoreConfig)</td>\n",
       "      <td>Base class for all neural network modules.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chronos.base</td>\n",
       "      <td>ForecastType</td>\n",
       "      <td>Class</td>\n",
       "      <td>(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)</td>\n",
       "      <td>Create a collection of name/value pairs.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>chronos.chronos2.layers</td>\n",
       "      <td>GroupSelfAttention</td>\n",
       "      <td>Class</td>\n",
       "      <td>(config: chronos.chronos2.config.Chronos2CoreConfig)</td>\n",
       "      <td>Self-attention applied along the batch axis masked by the group attention mask</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>chronos.chronos2.model</td>\n",
       "      <td>InstanceNorm</td>\n",
       "      <td>Class</td>\n",
       "      <td>(eps: float = 1e-05, use_arcsinh: bool = False) -&gt; None</td>\n",
       "      <td>Apply standardization along the last dimension and optionally apply arcsinh after standardization.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>chronos.chronos2.layers</td>\n",
       "      <td>MHA</td>\n",
       "      <td>Class</td>\n",
       "      <td>(config: chronos.chronos2.config.Chronos2CoreConfig, use_rope: bool = True)</td>\n",
       "      <td>Multi-head Attention Layer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>chronos.chronos2.layers</td>\n",
       "      <td>MLP</td>\n",
       "      <td>Class</td>\n",
       "      <td>(config: chronos.chronos2.config.Chronos2CoreConfig)</td>\n",
       "      <td>Base class for all neural network modules.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>chronos.chronos</td>\n",
       "      <td>MeanScaleUniformBins</td>\n",
       "      <td>Class</td>\n",
       "      <td>(low_limit: float, high_limit: float, config: chronos.chronos.ChronosConfig) -&gt; None</td>\n",
       "      <td>A ``ChronosTokenizer`` defines how time series are mapped into token IDs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>chronos.chronos2.model</td>\n",
       "      <td>Patch</td>\n",
       "      <td>Class</td>\n",
       "      <td>(patch_size: int, patch_stride: int) -&gt; None</td>\n",
       "      <td>Base class for all neural network modules.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>chronos.chronos2.layers</td>\n",
       "      <td>ResidualBlock</td>\n",
       "      <td>Class</td>\n",
       "      <td>(in_dim: int, h_dim: int, out_dim: int, act_fn_name: str, dropout_p: float = 0.0, use_layer_norm...</td>\n",
       "      <td>A generic residual block which can be used for input and output embedding layers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>chronos.chronos2.layers</td>\n",
       "      <td>RoPE</td>\n",
       "      <td>Class</td>\n",
       "      <td>(dim: int, base: float = 10000)</td>\n",
       "      <td>Applies rotary position embeddings (RoPE) to input tensors.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>chronos.chronos2.layers</td>\n",
       "      <td>TimeCrossAttention</td>\n",
       "      <td>Class</td>\n",
       "      <td>(config: chronos.chronos2.config.Chronos2CoreConfig)</td>\n",
       "      <td>Base class for all neural network modules.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>chronos.chronos2.layers</td>\n",
       "      <td>TimeSelfAttention</td>\n",
       "      <td>Class</td>\n",
       "      <td>(config: chronos.chronos2.config.Chronos2CoreConfig)</td>\n",
       "      <td>Base class for all neural network modules.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>chronos.chronos2.pipeline</td>\n",
       "      <td>convert_df_input_to_list_of_dicts_input</td>\n",
       "      <td>Function</td>\n",
       "      <td>(df: 'pd.DataFrame', future_df: 'pd.DataFrame | None', target_columns: list[str], prediction_len...</td>\n",
       "      <td>Convert from dataframe input format to a list of dictionaries input format.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>chronos.chronos2.dataset</td>\n",
       "      <td>convert_fev_window_to_list_of_dicts_input</td>\n",
       "      <td>Function</td>\n",
       "      <td>(window: 'fev.EvaluationWindow', as_univariate: bool) -&gt; tuple[list[dict[str, numpy.ndarray | di...</td>\n",
       "      <td>(No description available)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>chronos.chronos2.dataset</td>\n",
       "      <td>convert_list_of_tensors_input_to_list_of_dicts_input</td>\n",
       "      <td>Function</td>\n",
       "      <td>(list_of_tensors: Sequence[torch.Tensor | numpy.ndarray]) -&gt; list[dict[str, torch.Tensor]]</td>\n",
       "      <td>Convert a list of tensors input format to a list of dictionaries input format.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>chronos.chronos2.dataset</td>\n",
       "      <td>convert_tensor_input_to_list_of_dicts_input</td>\n",
       "      <td>Function</td>\n",
       "      <td>(tensor: torch.Tensor | numpy.ndarray) -&gt; list[dict[str, torch.Tensor]]</td>\n",
       "      <td>Convert a tensor input format to a list of dictionaries input format.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>chronos.chronos2.pipeline</td>\n",
       "      <td>interpolate_quantiles</td>\n",
       "      <td>Function</td>\n",
       "      <td>(query_quantile_levels: torch.Tensor | list[float], original_quantile_levels: torch.Tensor | lis...</td>\n",
       "      <td>Interpolates quantile values at specified query levels using linear interpolation using original</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>chronos.chronos2.dataset</td>\n",
       "      <td>left_pad_and_cat_2D</td>\n",
       "      <td>Function</td>\n",
       "      <td>(tensors: list[torch.Tensor]) -&gt; torch.Tensor</td>\n",
       "      <td>Left pads tensors in the list to the length of the longest tensor along the second axis, then co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chronos.base</td>\n",
       "      <td>left_pad_and_stack_1D</td>\n",
       "      <td>Function</td>\n",
       "      <td>(tensors: List[torch.Tensor]) -&gt; torch.Tensor</td>\n",
       "      <td>(No description available)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>chronos.chronos2.trainer</td>\n",
       "      <td>seed_worker</td>\n",
       "      <td>Function</td>\n",
       "      <td>(worker_id: int)</td>\n",
       "      <td>(No description available)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>chronos.chronos2.dataset</td>\n",
       "      <td>validate_and_prepare_single_dict_task</td>\n",
       "      <td>Function</td>\n",
       "      <td>(task: Mapping[str, Union[torch.Tensor, numpy.ndarray, Mapping[str, torch.Tensor | numpy.ndarray...</td>\n",
       "      <td>Validates and prepares a single dictionary task for Chronos2Model.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>chronos.df_utils</td>\n",
       "      <td>validate_df_inputs</td>\n",
       "      <td>Function</td>\n",
       "      <td>(df: 'pd.DataFrame', future_df: 'pd.DataFrame | None', target_columns: list[str], prediction_len...</td>\n",
       "      <td>Validates and prepares dataframe inputs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>chronos.chronos2.pipeline</td>\n",
       "      <td>weighted_quantile</td>\n",
       "      <td>Function</td>\n",
       "      <td>(query_quantile_levels: torch.Tensor | list[float], sample_weights: torch.Tensor | list[float], ...</td>\n",
       "      <td>Computes quantiles from a distribution specified by `samples` and their corresponding probabilit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Module  \\\n",
       "29    chronos.chronos2.layers   \n",
       "23   chronos.chronos2.dataset   \n",
       "68   chronos.chronos2.trainer   \n",
       "32    chronos.chronos2.layers   \n",
       "1                chronos.base   \n",
       "33    chronos.chronos2.layers   \n",
       "52     chronos.chronos2.model   \n",
       "34    chronos.chronos2.layers   \n",
       "35    chronos.chronos2.layers   \n",
       "13            chronos.chronos   \n",
       "55     chronos.chronos2.model   \n",
       "36    chronos.chronos2.layers   \n",
       "37    chronos.chronos2.layers   \n",
       "38    chronos.chronos2.layers   \n",
       "39    chronos.chronos2.layers   \n",
       "64  chronos.chronos2.pipeline   \n",
       "24   chronos.chronos2.dataset   \n",
       "25   chronos.chronos2.dataset   \n",
       "26   chronos.chronos2.dataset   \n",
       "65  chronos.chronos2.pipeline   \n",
       "27   chronos.chronos2.dataset   \n",
       "3                chronos.base   \n",
       "69   chronos.chronos2.trainer   \n",
       "28   chronos.chronos2.dataset   \n",
       "80           chronos.df_utils   \n",
       "66  chronos.chronos2.pipeline   \n",
       "\n",
       "                                                    Name      Type  \\\n",
       "29                                       AttentionOutput     Class   \n",
       "23                                           DatasetMode     Class   \n",
       "68                      EvaluateAndSaveFinalStepCallback     Class   \n",
       "32                                           FeedForward     Class   \n",
       "1                                           ForecastType     Class   \n",
       "33                                    GroupSelfAttention     Class   \n",
       "52                                          InstanceNorm     Class   \n",
       "34                                                   MHA     Class   \n",
       "35                                                   MLP     Class   \n",
       "13                                  MeanScaleUniformBins     Class   \n",
       "55                                                 Patch     Class   \n",
       "36                                         ResidualBlock     Class   \n",
       "37                                                  RoPE     Class   \n",
       "38                                    TimeCrossAttention     Class   \n",
       "39                                     TimeSelfAttention     Class   \n",
       "64               convert_df_input_to_list_of_dicts_input  Function   \n",
       "24             convert_fev_window_to_list_of_dicts_input  Function   \n",
       "25  convert_list_of_tensors_input_to_list_of_dicts_input  Function   \n",
       "26           convert_tensor_input_to_list_of_dicts_input  Function   \n",
       "65                                 interpolate_quantiles  Function   \n",
       "27                                   left_pad_and_cat_2D  Function   \n",
       "3                                  left_pad_and_stack_1D  Function   \n",
       "69                                           seed_worker  Function   \n",
       "28                 validate_and_prepare_single_dict_task  Function   \n",
       "80                                    validate_df_inputs  Function   \n",
       "66                                     weighted_quantile  Function   \n",
       "\n",
       "                                                                                              Signature  \\\n",
       "29        (hidden_states: torch.Tensor | None = None, attn_weights: torch.Tensor | None = None) -> None   \n",
       "23                (value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)   \n",
       "68                                                                                                   ()   \n",
       "32                                                 (config: chronos.chronos2.config.Chronos2CoreConfig)   \n",
       "1                 (value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)   \n",
       "33                                                 (config: chronos.chronos2.config.Chronos2CoreConfig)   \n",
       "52                                              (eps: float = 1e-05, use_arcsinh: bool = False) -> None   \n",
       "34                          (config: chronos.chronos2.config.Chronos2CoreConfig, use_rope: bool = True)   \n",
       "35                                                 (config: chronos.chronos2.config.Chronos2CoreConfig)   \n",
       "13                 (low_limit: float, high_limit: float, config: chronos.chronos.ChronosConfig) -> None   \n",
       "55                                                         (patch_size: int, patch_stride: int) -> None   \n",
       "36  (in_dim: int, h_dim: int, out_dim: int, act_fn_name: str, dropout_p: float = 0.0, use_layer_norm...   \n",
       "37                                                                      (dim: int, base: float = 10000)   \n",
       "38                                                 (config: chronos.chronos2.config.Chronos2CoreConfig)   \n",
       "39                                                 (config: chronos.chronos2.config.Chronos2CoreConfig)   \n",
       "64  (df: 'pd.DataFrame', future_df: 'pd.DataFrame | None', target_columns: list[str], prediction_len...   \n",
       "24  (window: 'fev.EvaluationWindow', as_univariate: bool) -> tuple[list[dict[str, numpy.ndarray | di...   \n",
       "25           (list_of_tensors: Sequence[torch.Tensor | numpy.ndarray]) -> list[dict[str, torch.Tensor]]   \n",
       "26                              (tensor: torch.Tensor | numpy.ndarray) -> list[dict[str, torch.Tensor]]   \n",
       "65  (query_quantile_levels: torch.Tensor | list[float], original_quantile_levels: torch.Tensor | lis...   \n",
       "27                                                        (tensors: list[torch.Tensor]) -> torch.Tensor   \n",
       "3                                                         (tensors: List[torch.Tensor]) -> torch.Tensor   \n",
       "69                                                                                     (worker_id: int)   \n",
       "28  (task: Mapping[str, Union[torch.Tensor, numpy.ndarray, Mapping[str, torch.Tensor | numpy.ndarray...   \n",
       "80  (df: 'pd.DataFrame', future_df: 'pd.DataFrame | None', target_columns: list[str], prediction_len...   \n",
       "66  (query_quantile_levels: torch.Tensor | list[float], sample_weights: torch.Tensor | list[float], ...   \n",
       "\n",
       "                                                                                            Description  \n",
       "29  AttentionOutput(hidden_states: torch.Tensor | None = None, attn_weights: torch.Tensor | None = N...  \n",
       "23                                                                                str(object='') -> str  \n",
       "68                                       Callback to evaluate and save the model at last training step.  \n",
       "32                                                           Base class for all neural network modules.  \n",
       "1                                                              Create a collection of name/value pairs.  \n",
       "33                       Self-attention applied along the batch axis masked by the group attention mask  \n",
       "52   Apply standardization along the last dimension and optionally apply arcsinh after standardization.  \n",
       "34                                                                           Multi-head Attention Layer  \n",
       "35                                                           Base class for all neural network modules.  \n",
       "13                             A ``ChronosTokenizer`` defines how time series are mapped into token IDs  \n",
       "55                                                           Base class for all neural network modules.  \n",
       "36                     A generic residual block which can be used for input and output embedding layers  \n",
       "37                                          Applies rotary position embeddings (RoPE) to input tensors.  \n",
       "38                                                           Base class for all neural network modules.  \n",
       "39                                                           Base class for all neural network modules.  \n",
       "64                          Convert from dataframe input format to a list of dictionaries input format.  \n",
       "24                                                                           (No description available)  \n",
       "25                       Convert a list of tensors input format to a list of dictionaries input format.  \n",
       "26                                Convert a tensor input format to a list of dictionaries input format.  \n",
       "65     Interpolates quantile values at specified query levels using linear interpolation using original  \n",
       "27  Left pads tensors in the list to the length of the longest tensor along the second axis, then co...  \n",
       "3                                                                            (No description available)  \n",
       "69                                                                           (No description available)  \n",
       "28                                   Validates and prepares a single dictionary task for Chronos2Model.  \n",
       "80                                                              Validates and prepares dataframe inputs  \n",
       "66  Computes quantiles from a distribution specified by `samples` and their corresponding probabilit...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚡ Check for Chronos-Bolt (Fast Inference):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Module</th>\n",
       "      <th>Name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Signature</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>chronos.chronos_bolt</td>\n",
       "      <td>ChronosBoltConfig</td>\n",
       "      <td>Class</td>\n",
       "      <td>(context_length: int, prediction_length: int, input_patch_size: int, input_patch_stride: int, qu...</td>\n",
       "      <td>ChronosBoltConfig(context_length: int, prediction_length: int, input_patch_size: int, input_patc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>chronos.chronos_bolt</td>\n",
       "      <td>ChronosBoltModelForForecasting</td>\n",
       "      <td>Class</td>\n",
       "      <td>(config: transformers.models.t5.configuration_t5.T5Config)</td>\n",
       "      <td>This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>chronos.chronos_bolt</td>\n",
       "      <td>ChronosBoltOutput</td>\n",
       "      <td>Class</td>\n",
       "      <td>(loss: Optional[torch.Tensor] = None, quantile_preds: Optional[torch.Tensor] = None, attentions:...</td>\n",
       "      <td>ChronosBoltOutput(loss: Optional[torch.Tensor] = None, quantile_preds: Optional[torch.Tensor] = ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>chronos.chronos_bolt</td>\n",
       "      <td>ChronosBoltPipeline</td>\n",
       "      <td>Class</td>\n",
       "      <td>(model: chronos.chronos_bolt.ChronosBoltModelForForecasting)</td>\n",
       "      <td>(No description available)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Module                            Name   Type  \\\n",
       "71  chronos.chronos_bolt               ChronosBoltConfig  Class   \n",
       "72  chronos.chronos_bolt  ChronosBoltModelForForecasting  Class   \n",
       "73  chronos.chronos_bolt               ChronosBoltOutput  Class   \n",
       "74  chronos.chronos_bolt             ChronosBoltPipeline  Class   \n",
       "\n",
       "                                                                                              Signature  \\\n",
       "71  (context_length: int, prediction_length: int, input_patch_size: int, input_patch_stride: int, qu...   \n",
       "72                                           (config: transformers.models.t5.configuration_t5.T5Config)   \n",
       "73  (loss: Optional[torch.Tensor] = None, quantile_preds: Optional[torch.Tensor] = None, attentions:...   \n",
       "74                                         (model: chronos.chronos_bolt.ChronosBoltModelForForecasting)   \n",
       "\n",
       "                                                                                            Description  \n",
       "71  ChronosBoltConfig(context_length: int, prediction_length: int, input_patch_size: int, input_patc...  \n",
       "72  This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic...  \n",
       "73  ChronosBoltOutput(loss: Optional[torch.Tensor] = None, quantile_preds: Optional[torch.Tensor] = ...  \n",
       "74                                                                           (No description available)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pkgutil\n",
    "import importlib\n",
    "import inspect\n",
    "import pandas as pd\n",
    "import chronos\n",
    "\n",
    "def discover_all_capabilities(library_module):\n",
    "    \"\"\"\n",
    "    ライブラリ内の全ての公開クラス・関数を探索し、\n",
    "    その「機能（Docstring）」を一覧化して隠れた機能を発掘する。\n",
    "    \"\"\"\n",
    "    catalog = []\n",
    "    \n",
    "    # パッケージのパスを取得\n",
    "    package_path = library_module.__path__\n",
    "    prefix = library_module.__name__ + \".\"\n",
    "\n",
    "    print(f\"🚀 Exploring library: {library_module.__name__}\")\n",
    "\n",
    "    # 再帰的にモジュールを探索\n",
    "    for _, name, ispkg in pkgutil.walk_packages(package_path, prefix):\n",
    "        try:\n",
    "            module = importlib.import_module(name)\n",
    "            \n",
    "            # モジュール内のメンバーを検査\n",
    "            for member_name, obj in inspect.getmembers(module):\n",
    "                # プライベートメンバと他所からのインポートを除外\n",
    "                if member_name.startswith(\"_\"): continue\n",
    "                \n",
    "                # クラスまたは関数のみ対象\n",
    "                if inspect.isclass(obj) or inspect.isfunction(obj):\n",
    "                    # そのライブラリ等で定義されたものに限定（外部ライブラリの除外）\n",
    "                    if hasattr(obj, '__module__') and obj.__module__ and obj.__module__.startswith(library_module.__name__):\n",
    "                        \n",
    "                        # Docstringの取得と整形\n",
    "                        doc = inspect.getdoc(obj)\n",
    "                        summary = doc.split('\\n')[0] if doc else \"(No description available)\"\n",
    "                        \n",
    "                        # 引数（シグネチャ）の取得 - パラメータから機能を推測するため\n",
    "                        try:\n",
    "                            sig = str(inspect.signature(obj))\n",
    "                        except (ValueError, TypeError):\n",
    "                            sig = \"(...)\"\n",
    "\n",
    "                        catalog.append({\n",
    "                            \"Module\": name,\n",
    "                            \"Name\": member_name,\n",
    "                            \"Type\": \"Class\" if inspect.isclass(obj) else \"Function\",\n",
    "                            \"Signature\": sig[:100], # 長すぎる場合はカット\n",
    "                            \"Description\": summary[:150] # 長すぎる場合はカット\n",
    "                        })\n",
    "        except ImportError:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            # エラーは無視して続行\n",
    "            pass\n",
    "\n",
    "    df = pd.DataFrame(catalog)\n",
    "    # 重複を除去（異なるモジュールでimportされている場合があるため）\n",
    "    df = df.drop_duplicates(subset=[\"Name\", \"Type\"])\n",
    "    return df\n",
    "\n",
    "# --- 実行 ---\n",
    "df_catalog = discover_all_capabilities(chronos)\n",
    "\n",
    "# 機能カテゴリを推測して整理表示\n",
    "print(\"\\n=== 📦 Chronos-2 Full Capability Catalog ===\")\n",
    "\n",
    "# 1. Pipeline / Model 関連（中核機能）\n",
    "print(\"\\n🔹 Core Models & Pipelines:\")\n",
    "display(df_catalog[df_catalog[\"Name\"].str.contains(\"Model|Pipeline|Chronos\", case=False, regex=True)].sort_values(\"Name\"))\n",
    "\n",
    "# 2. Config / Settings 関連（設定項目から隠し機能が見えることが多い）\n",
    "print(\"\\n🔹 Configuration & Settings:\")\n",
    "display(df_catalog[df_catalog[\"Name\"].str.contains(\"Config\", case=False)].sort_values(\"Name\"))\n",
    "\n",
    "# 3. Utilities / Data 関連（データ処理、評価、変換など）\n",
    "print(\"\\n🔹 Utilities & Data Processing:\")\n",
    "# 上記以外を抽出\n",
    "others = df_catalog[\n",
    "    ~df_catalog[\"Name\"].str.contains(\"Model|Pipeline|Chronos|Config\", case=False, regex=True)\n",
    "].sort_values(\"Name\")\n",
    "display(others)\n",
    "\n",
    "# 4. \"Bolt\" (高速化モデル) 関連の確認\n",
    "print(\"\\n⚡ Check for Chronos-Bolt (Fast Inference):\")\n",
    "bolt_features = df_catalog[df_catalog[\"Name\"].str.contains(\"Bolt\", case=False)]\n",
    "if not bolt_features.empty:\n",
    "    display(bolt_features)\n",
    "else:\n",
    "    print(\"No explicit 'Bolt' components found in the API structure.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4a032d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Exploring library: chronos...\n",
      "\n",
      "✅ CSV Output Generated: chronos_capabilities.csv\n",
      "📊 Total items found: 51\n",
      "📂 Location: c:\\moinfo\\chronos_capabilities.csv\n",
      "\n",
      "--- Preview (Top 5) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Name</th>\n",
       "      <th>Module</th>\n",
       "      <th>Signature</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Class</td>\n",
       "      <td>BaseChronosPipeline</td>\n",
       "      <td>chronos.base</td>\n",
       "      <td>(inner_model: 'PreTrainedModel')</td>\n",
       "      <td>(No description available)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Class</td>\n",
       "      <td>ForecastType</td>\n",
       "      <td>chronos.base</td>\n",
       "      <td>(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)</td>\n",
       "      <td>Create a collection of name/value pairs.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Class</td>\n",
       "      <td>PipelineRegistry</td>\n",
       "      <td>chronos.base</td>\n",
       "      <td>(name, bases, attrs)</td>\n",
       "      <td>type(object) -&gt; the object's type</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Function</td>\n",
       "      <td>left_pad_and_stack_1D</td>\n",
       "      <td>chronos.base</td>\n",
       "      <td>(tensors: List[torch.Tensor]) -&gt; torch.Tensor</td>\n",
       "      <td>(No description available)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Function</td>\n",
       "      <td>cache_model_from_s3</td>\n",
       "      <td>chronos.boto_utils</td>\n",
       "      <td>(s3_uri: str, force_download: bool = False, boto3_session: boto3.session.Session | None = None)</td>\n",
       "      <td>(No description available)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Category                   Name              Module  \\\n",
       "0     Class    BaseChronosPipeline        chronos.base   \n",
       "1     Class           ForecastType        chronos.base   \n",
       "2     Class       PipelineRegistry        chronos.base   \n",
       "3  Function  left_pad_and_stack_1D        chronos.base   \n",
       "4  Function    cache_model_from_s3  chronos.boto_utils   \n",
       "\n",
       "                                                                                         Signature  \\\n",
       "0                                                                 (inner_model: 'PreTrainedModel')   \n",
       "1            (value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)   \n",
       "2                                                                             (name, bases, attrs)   \n",
       "3                                                    (tensors: List[torch.Tensor]) -> torch.Tensor   \n",
       "4  (s3_uri: str, force_download: bool = False, boto3_session: boto3.session.Session | None = None)   \n",
       "\n",
       "                                Description  \n",
       "0                (No description available)  \n",
       "1  Create a collection of name/value pairs.  \n",
       "2         type(object) -> the object's type  \n",
       "3                (No description available)  \n",
       "4                (No description available)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pkgutil\n",
    "import importlib\n",
    "import inspect\n",
    "import pandas as pd\n",
    "import chronos\n",
    "import os\n",
    "\n",
    "def discover_all_capabilities(library_module):\n",
    "    \"\"\"\n",
    "    ライブラリ内の全ての公開クラス・関数を探索し、\n",
    "    その「機能（Docstring）」を一覧化して隠れた機能を発掘する。\n",
    "    \"\"\"\n",
    "    catalog = []\n",
    "    \n",
    "    # パッケージのパスを取得\n",
    "    # 注意: インストール環境によっては __path__ がリストの場合があるため対応\n",
    "    package_path = library_module.__path__\n",
    "    prefix = library_module.__name__ + \".\"\n",
    "\n",
    "    print(f\"🚀 Exploring library: {library_module.__name__}...\")\n",
    "\n",
    "    # 再帰的にモジュールを探索\n",
    "    for _, name, ispkg in pkgutil.walk_packages(package_path, prefix):\n",
    "        try:\n",
    "            module = importlib.import_module(name)\n",
    "            \n",
    "            # モジュール内のメンバーを検査\n",
    "            for member_name, obj in inspect.getmembers(module):\n",
    "                # プライベートメンバと他所からのインポートを除外\n",
    "                if member_name.startswith(\"_\"): continue\n",
    "                \n",
    "                # クラスまたは関数のみ対象\n",
    "                if inspect.isclass(obj) or inspect.isfunction(obj):\n",
    "                    # そのライブラリ等で定義されたものに限定（外部ライブラリの除外）\n",
    "                    if hasattr(obj, '__module__') and obj.__module__ and obj.__module__.startswith(library_module.__name__):\n",
    "                        \n",
    "                        # Docstringの取得と整形\n",
    "                        doc = inspect.getdoc(obj)\n",
    "                        summary = doc.split('\\n')[0] if doc else \"(No description available)\"\n",
    "                        \n",
    "                        # シグネチャ（引数定義）の取得\n",
    "                        try:\n",
    "                            sig = str(inspect.signature(obj))\n",
    "                        except (ValueError, TypeError):\n",
    "                            sig = \"(...)\"\n",
    "\n",
    "                        catalog.append({\n",
    "                            \"Category\": \"Class\" if inspect.isclass(obj) else \"Function\",\n",
    "                            \"Name\": member_name,\n",
    "                            \"Module\": name,\n",
    "                            \"Signature\": sig,  # 引数構成（機能の推測に有用）\n",
    "                            \"Description\": summary # 機能概要\n",
    "                        })\n",
    "        except ImportError:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            # 読み込みエラーは無視\n",
    "            pass\n",
    "\n",
    "    df = pd.DataFrame(catalog)\n",
    "    if not df.empty:\n",
    "        # 重複を除去（異なるモジュールでimportされている場合があるため）\n",
    "        df = df.drop_duplicates(subset=[\"Name\", \"Category\"])\n",
    "        # 見やすいようにソート（モジュール順 -> 名前順）\n",
    "        df = df.sort_values(by=[\"Module\", \"Name\"])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# --- 実行 & CSV保存 ---\n",
    "\n",
    "# 1. 探索実行\n",
    "df_catalog = discover_all_capabilities(chronos)\n",
    "\n",
    "if not df_catalog.empty:\n",
    "    # 2. CSVファイル名\n",
    "    output_filename = \"chronos_capabilities.csv\"\n",
    "    \n",
    "    # 3. CSV出力 (utf-8-sig はExcelでの文字化けを防ぎます)\n",
    "    df_catalog.to_csv(output_filename, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"\\n✅ CSV Output Generated: {output_filename}\")\n",
    "    print(f\"📊 Total items found: {len(df_catalog)}\")\n",
    "    print(f\"📂 Location: {os.path.abspath(output_filename)}\")\n",
    "    \n",
    "    # (オプション) 上位5行を表示して確認\n",
    "    print(\"\\n--- Preview (Top 5) ---\")\n",
    "    display(df_catalog.head())\n",
    "else:\n",
    "    print(\"❌ No capabilities found. Please check if 'chronos' is installed correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e8afde",
   "metadata": {},
   "source": [
    "# 📚 Chronos-2 機能カタログの階層化と可視化\n",
    "\n",
    "取得した `df_catalog` (機能一覧) を分析し、ライブラリの構造を「カスケード（階層）構造」で整理します。\n",
    "これにより、「どの機能がどのコンポーネントに属しているか」を体系的に把握します。\n",
    "\n",
    "### 分類階層の定義\n",
    "* **Level 1 (Component)**: 製品ライン（Chronos-2 / Bolt / Base / Utils）\n",
    "* **Level 2 (Role)**: 役割（Pipeline / Model / Config / Data / Layers）\n",
    "* **Level 3 (Detail)**: 具体的な機能詳細（推論、学習、Attention機構、入出力など）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10c56818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded catalog from chronos_capabilities.csv\n",
      "\n",
      "🌳 Chronos-2 Library Structure Tree\n",
      "============================================================\n",
      "\n",
      "📦 ☁️ AWS Integration\n",
      "  ├─ 📂 S3 / IO\n",
      "  │    ├─ cache_model_from_s3                 : (No description available)\n",
      "  │    ├─ download_model_files_from_cloudfront : (No description available)\n",
      "  │    ├─ download_model_files_from_s3        : (No description available)\n",
      "\n",
      "📦 ⚡ Chronos-Bolt (Speed)\n",
      "  ├─ 📂 0. Configuration\n",
      "  │    ├─ ChronosBoltConfig                   : ChronosBoltConfig(context_length: int, prediction_length: in...\n",
      "  ├─ 📂 1. Inference Pipeline\n",
      "  │    ├─ ChronosBoltPipeline                 : (No description available)\n",
      "  ├─ 📂 2. Model Architecture\n",
      "  │    ├─ ChronosBoltModelForForecasting      : This model inherits from [`PreTrainedModel`]. Check the supe...\n",
      "  ├─ 📂 4. Output Structures\n",
      "  │    ├─ ChronosBoltOutput                   : ChronosBoltOutput(loss: Optional[torch.Tensor] = None, quant...\n",
      "\n",
      "📦 🏗️ Base Infrastructure\n",
      "  ├─ 📂 1. Inference Pipeline\n",
      "  │    ├─ BaseChronosPipeline                 : (No description available)\n",
      "  │    ├─ PipelineRegistry                    : type(object) -> the object's type\n",
      "  ├─ 📂 Misc / Components\n",
      "  │    ├─ ForecastType                        : Create a collection of name/value pairs.\n",
      "  │    ├─ left_pad_and_stack_1D               : (No description available)\n",
      "\n",
      "📦 📊 Data Engineering\n",
      "  ├─ 📂 3. Data Processing\n",
      "  │    ├─ validate_df_inputs                  : Validates and prepares dataframe inputs\n",
      "\n",
      "📦 🚀 Chronos-2 (Core)\n",
      "  ├─ 📂 0. Configuration\n",
      "  │    ├─ Chronos2CoreConfig                  : HF transformers-style pretrained model config for Chronos-2....\n",
      "  │    ├─ Chronos2ForecastingConfig           : Chronos2ForecastingConfig(context_length: int, output_patch_...\n",
      "  ├─ 📂 1. Inference Pipeline\n",
      "  │    ├─ Chronos2Pipeline                    : (No description available)\n",
      "  ├─ 📂 2. Model Architecture\n",
      "  │    ├─ Chronos2Encoder                     : Base class for all neural network modules.\n",
      "  │    ├─ Chronos2EncoderBlock                : Base class for all neural network modules.\n",
      "  │    ├─ Chronos2EncoderBlockOutput          : Chronos2EncoderBlockOutput(hidden_states: torch.Tensor | Non...\n",
      "  │    ├─ Chronos2EncoderOutput               : Chronos2EncoderOutput(last_hidden_state: torch.Tensor | None...\n",
      "  │    ├─ Chronos2Model                       : Base class for all models.\n",
      "  ├─ 📂 3. Data Processing\n",
      "  │    ├─ Chronos2Dataset                     : A dataset wrapper for Chronos-2 models.\n",
      "  │    ├─ DatasetMode                         : str(object='') -> str\n",
      "  │    ├─ convert_df_input_to_list_of_dicts_input : Convert from dataframe input format to a list of dictionarie...\n",
      "  │    ├─ convert_fev_window_to_list_of_dicts_input : (No description available)\n",
      "  │    ├─ convert_list_of_tensors_input_to_list_of_dicts_input : Convert a list of tensors input format to a list of dictiona...\n",
      "  │    ├─ convert_tensor_input_to_list_of_dicts_input : Convert a tensor input format to a list of dictionaries inpu...\n",
      "  │    ├─ validate_and_prepare_single_dict_task : Validates and prepares a single dictionary task for Chronos2...\n",
      "  ├─ 📂 4. Output Structures\n",
      "  │    ├─ AttentionOutput                     : AttentionOutput(hidden_states: torch.Tensor | None = None, a...\n",
      "  │    ├─ Chronos2Output                      : Chronos2Output(loss: torch.Tensor | None = None, quantile_pr...\n",
      "  ├─ 📂 Attention Mechanism\n",
      "  │    ├─ GroupSelfAttention                  : Self-attention applied along the batch axis masked by the gr...\n",
      "  │    ├─ TimeCrossAttention                  : Base class for all neural network modules.\n",
      "  │    ├─ TimeSelfAttention                   : Base class for all neural network modules.\n",
      "  ├─ 📂 Misc / Components\n",
      "  │    ├─ Chronos2LayerNorm                   : Base class for all neural network modules.\n",
      "  │    ├─ Chronos2Trainer                     : A custom trainer based on transformers Trainer. We need to o...\n",
      "  │    ├─ EvaluateAndSaveFinalStepCallback    : Callback to evaluate and save the model at last training ste...\n",
      "  │    ├─ FeedForward                         : Base class for all neural network modules.\n",
      "  │    ├─ InstanceNorm                        : Apply standardization along the last dimension and optionall...\n",
      "  │    ├─ MHA                                 : Multi-head Attention Layer\n",
      "  │    ├─ MLP                                 : Base class for all neural network modules.\n",
      "  │    ├─ Patch                               : Base class for all neural network modules.\n",
      "  │    ├─ ResidualBlock                       : A generic residual block which can be used for input and out...\n",
      "  │    ├─ RoPE                                : Applies rotary position embeddings (RoPE) to input tensors.\n",
      "  │    ├─ interpolate_quantiles               : Interpolates quantile values at specified query levels using...\n",
      "  │    ├─ left_pad_and_cat_2D                 : Left pads tensors in the list to the length of the longest t...\n",
      "  │    ├─ seed_worker                         : (No description available)\n",
      "  │    ├─ weighted_quantile                   : Computes quantiles from a distribution specified by `samples...\n",
      "\n",
      "📦 🛠️ Utilities & Common\n",
      "  ├─ 📂 0. Configuration\n",
      "  │    ├─ ChronosConfig                       : This class holds all the configuration parameters to be used\n",
      "  ├─ 📂 1. Inference Pipeline\n",
      "  │    ├─ ChronosPipeline                     : A ``ChronosPipeline`` uses the given tokenizer and model to ...\n",
      "  ├─ 📂 2. Model Architecture\n",
      "  │    ├─ ChronosModel                        : A ``ChronosModel`` wraps a ``PreTrainedModel`` object from `...\n",
      "  ├─ 📂 Misc / Components\n",
      "  │    ├─ ChronosTokenizer                    : A ``ChronosTokenizer`` defines how time series are mapped in...\n",
      "  │    ├─ MeanScaleUniformBins                : A ``ChronosTokenizer`` defines how time series are mapped in...\n",
      "\n",
      "📊 Detailed Interactive Table\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_6bfc5 th {\n",
       "  text-align: left;\n",
       "  background-color: #f0f0f0;\n",
       "}\n",
       "#T_6bfc5_row0_col0, #T_6bfc5_row0_col1, #T_6bfc5_row0_col2, #T_6bfc5_row0_col3, #T_6bfc5_row0_col4, #T_6bfc5_row1_col0, #T_6bfc5_row1_col1, #T_6bfc5_row1_col2, #T_6bfc5_row1_col3, #T_6bfc5_row1_col4, #T_6bfc5_row2_col0, #T_6bfc5_row2_col1, #T_6bfc5_row2_col2, #T_6bfc5_row2_col3, #T_6bfc5_row2_col4, #T_6bfc5_row3_col0, #T_6bfc5_row3_col1, #T_6bfc5_row3_col2, #T_6bfc5_row3_col3, #T_6bfc5_row3_col4, #T_6bfc5_row4_col0, #T_6bfc5_row4_col1, #T_6bfc5_row4_col2, #T_6bfc5_row4_col3, #T_6bfc5_row4_col4, #T_6bfc5_row5_col0, #T_6bfc5_row5_col1, #T_6bfc5_row5_col2, #T_6bfc5_row5_col3, #T_6bfc5_row5_col4, #T_6bfc5_row6_col0, #T_6bfc5_row6_col1, #T_6bfc5_row6_col2, #T_6bfc5_row6_col3, #T_6bfc5_row6_col4, #T_6bfc5_row7_col0, #T_6bfc5_row7_col1, #T_6bfc5_row7_col2, #T_6bfc5_row7_col3, #T_6bfc5_row7_col4, #T_6bfc5_row8_col0, #T_6bfc5_row8_col1, #T_6bfc5_row8_col2, #T_6bfc5_row8_col3, #T_6bfc5_row8_col4, #T_6bfc5_row9_col0, #T_6bfc5_row9_col1, #T_6bfc5_row9_col2, #T_6bfc5_row9_col3, #T_6bfc5_row9_col4, #T_6bfc5_row10_col0, #T_6bfc5_row10_col1, #T_6bfc5_row10_col2, #T_6bfc5_row10_col3, #T_6bfc5_row10_col4, #T_6bfc5_row11_col0, #T_6bfc5_row11_col1, #T_6bfc5_row11_col2, #T_6bfc5_row11_col3, #T_6bfc5_row11_col4, #T_6bfc5_row12_col0, #T_6bfc5_row12_col1, #T_6bfc5_row12_col2, #T_6bfc5_row12_col3, #T_6bfc5_row12_col4, #T_6bfc5_row13_col0, #T_6bfc5_row13_col1, #T_6bfc5_row13_col2, #T_6bfc5_row13_col3, #T_6bfc5_row13_col4, #T_6bfc5_row14_col0, #T_6bfc5_row14_col1, #T_6bfc5_row14_col2, #T_6bfc5_row14_col3, #T_6bfc5_row14_col4, #T_6bfc5_row15_col0, #T_6bfc5_row15_col1, #T_6bfc5_row15_col2, #T_6bfc5_row15_col3, #T_6bfc5_row15_col4, #T_6bfc5_row16_col0, #T_6bfc5_row16_col1, #T_6bfc5_row16_col2, #T_6bfc5_row16_col3, #T_6bfc5_row16_col4, #T_6bfc5_row17_col0, #T_6bfc5_row17_col1, #T_6bfc5_row17_col2, #T_6bfc5_row17_col3, #T_6bfc5_row17_col4, #T_6bfc5_row18_col0, #T_6bfc5_row18_col1, #T_6bfc5_row18_col2, #T_6bfc5_row18_col3, #T_6bfc5_row18_col4, #T_6bfc5_row19_col0, #T_6bfc5_row19_col1, #T_6bfc5_row19_col2, #T_6bfc5_row19_col3, #T_6bfc5_row19_col4, #T_6bfc5_row20_col0, #T_6bfc5_row20_col1, #T_6bfc5_row20_col2, #T_6bfc5_row20_col3, #T_6bfc5_row20_col4, #T_6bfc5_row21_col0, #T_6bfc5_row21_col1, #T_6bfc5_row21_col2, #T_6bfc5_row21_col3, #T_6bfc5_row21_col4, #T_6bfc5_row22_col0, #T_6bfc5_row22_col1, #T_6bfc5_row22_col2, #T_6bfc5_row22_col3, #T_6bfc5_row22_col4, #T_6bfc5_row23_col0, #T_6bfc5_row23_col1, #T_6bfc5_row23_col2, #T_6bfc5_row23_col3, #T_6bfc5_row23_col4, #T_6bfc5_row24_col0, #T_6bfc5_row24_col1, #T_6bfc5_row24_col2, #T_6bfc5_row24_col3, #T_6bfc5_row24_col4, #T_6bfc5_row25_col0, #T_6bfc5_row25_col1, #T_6bfc5_row25_col2, #T_6bfc5_row25_col3, #T_6bfc5_row25_col4, #T_6bfc5_row26_col0, #T_6bfc5_row26_col1, #T_6bfc5_row26_col2, #T_6bfc5_row26_col3, #T_6bfc5_row26_col4, #T_6bfc5_row27_col0, #T_6bfc5_row27_col1, #T_6bfc5_row27_col2, #T_6bfc5_row27_col3, #T_6bfc5_row27_col4, #T_6bfc5_row28_col0, #T_6bfc5_row28_col1, #T_6bfc5_row28_col2, #T_6bfc5_row28_col3, #T_6bfc5_row28_col4, #T_6bfc5_row29_col0, #T_6bfc5_row29_col1, #T_6bfc5_row29_col2, #T_6bfc5_row29_col3, #T_6bfc5_row29_col4, #T_6bfc5_row30_col0, #T_6bfc5_row30_col1, #T_6bfc5_row30_col2, #T_6bfc5_row30_col3, #T_6bfc5_row30_col4, #T_6bfc5_row31_col0, #T_6bfc5_row31_col1, #T_6bfc5_row31_col2, #T_6bfc5_row31_col3, #T_6bfc5_row31_col4, #T_6bfc5_row32_col0, #T_6bfc5_row32_col1, #T_6bfc5_row32_col2, #T_6bfc5_row32_col3, #T_6bfc5_row32_col4, #T_6bfc5_row33_col0, #T_6bfc5_row33_col1, #T_6bfc5_row33_col2, #T_6bfc5_row33_col3, #T_6bfc5_row33_col4, #T_6bfc5_row34_col0, #T_6bfc5_row34_col1, #T_6bfc5_row34_col2, #T_6bfc5_row34_col3, #T_6bfc5_row34_col4, #T_6bfc5_row35_col0, #T_6bfc5_row35_col1, #T_6bfc5_row35_col2, #T_6bfc5_row35_col3, #T_6bfc5_row35_col4, #T_6bfc5_row36_col0, #T_6bfc5_row36_col1, #T_6bfc5_row36_col2, #T_6bfc5_row36_col3, #T_6bfc5_row36_col4, #T_6bfc5_row37_col0, #T_6bfc5_row37_col1, #T_6bfc5_row37_col2, #T_6bfc5_row37_col3, #T_6bfc5_row37_col4, #T_6bfc5_row38_col0, #T_6bfc5_row38_col1, #T_6bfc5_row38_col2, #T_6bfc5_row38_col3, #T_6bfc5_row38_col4, #T_6bfc5_row39_col0, #T_6bfc5_row39_col1, #T_6bfc5_row39_col2, #T_6bfc5_row39_col3, #T_6bfc5_row39_col4, #T_6bfc5_row40_col0, #T_6bfc5_row40_col1, #T_6bfc5_row40_col2, #T_6bfc5_row40_col3, #T_6bfc5_row40_col4, #T_6bfc5_row41_col0, #T_6bfc5_row41_col1, #T_6bfc5_row41_col2, #T_6bfc5_row41_col3, #T_6bfc5_row41_col4, #T_6bfc5_row42_col0, #T_6bfc5_row42_col1, #T_6bfc5_row42_col2, #T_6bfc5_row42_col3, #T_6bfc5_row42_col4, #T_6bfc5_row43_col0, #T_6bfc5_row43_col1, #T_6bfc5_row43_col2, #T_6bfc5_row43_col3, #T_6bfc5_row43_col4, #T_6bfc5_row44_col0, #T_6bfc5_row44_col1, #T_6bfc5_row44_col2, #T_6bfc5_row44_col3, #T_6bfc5_row44_col4, #T_6bfc5_row45_col0, #T_6bfc5_row45_col1, #T_6bfc5_row45_col2, #T_6bfc5_row45_col3, #T_6bfc5_row45_col4, #T_6bfc5_row46_col0, #T_6bfc5_row46_col1, #T_6bfc5_row46_col2, #T_6bfc5_row46_col3, #T_6bfc5_row46_col4, #T_6bfc5_row47_col0, #T_6bfc5_row47_col1, #T_6bfc5_row47_col2, #T_6bfc5_row47_col3, #T_6bfc5_row47_col4, #T_6bfc5_row48_col0, #T_6bfc5_row48_col1, #T_6bfc5_row48_col2, #T_6bfc5_row48_col3, #T_6bfc5_row48_col4, #T_6bfc5_row49_col0, #T_6bfc5_row49_col1, #T_6bfc5_row49_col2, #T_6bfc5_row49_col3, #T_6bfc5_row49_col4, #T_6bfc5_row50_col0, #T_6bfc5_row50_col1, #T_6bfc5_row50_col2, #T_6bfc5_row50_col3, #T_6bfc5_row50_col4 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_6bfc5\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_6bfc5_level0_col0\" class=\"col_heading level0 col0\" >L1_Component</th>\n",
       "      <th id=\"T_6bfc5_level0_col1\" class=\"col_heading level0 col1\" >L2_Role</th>\n",
       "      <th id=\"T_6bfc5_level0_col2\" class=\"col_heading level0 col2\" >L3_Detail</th>\n",
       "      <th id=\"T_6bfc5_level0_col3\" class=\"col_heading level0 col3\" >Name</th>\n",
       "      <th id=\"T_6bfc5_level0_col4\" class=\"col_heading level0 col4\" >Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row0_col0\" class=\"data row0 col0\" >☁️ AWS Integration</td>\n",
       "      <td id=\"T_6bfc5_row0_col1\" class=\"data row0 col1\" >S3 / IO</td>\n",
       "      <td id=\"T_6bfc5_row0_col2\" class=\"data row0 col2\" >Component Implementation</td>\n",
       "      <td id=\"T_6bfc5_row0_col3\" class=\"data row0 col3\" >cache_model_from_s3</td>\n",
       "      <td id=\"T_6bfc5_row0_col4\" class=\"data row0 col4\" >(No description available)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row1_col0\" class=\"data row1 col0\" >☁️ AWS Integration</td>\n",
       "      <td id=\"T_6bfc5_row1_col1\" class=\"data row1 col1\" >S3 / IO</td>\n",
       "      <td id=\"T_6bfc5_row1_col2\" class=\"data row1 col2\" >Component Implementation</td>\n",
       "      <td id=\"T_6bfc5_row1_col3\" class=\"data row1 col3\" >download_model_files_from_cloudfront</td>\n",
       "      <td id=\"T_6bfc5_row1_col4\" class=\"data row1 col4\" >(No description available)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row2_col0\" class=\"data row2 col0\" >☁️ AWS Integration</td>\n",
       "      <td id=\"T_6bfc5_row2_col1\" class=\"data row2 col1\" >S3 / IO</td>\n",
       "      <td id=\"T_6bfc5_row2_col2\" class=\"data row2 col2\" >Component Implementation</td>\n",
       "      <td id=\"T_6bfc5_row2_col3\" class=\"data row2 col3\" >download_model_files_from_s3</td>\n",
       "      <td id=\"T_6bfc5_row2_col4\" class=\"data row2 col4\" >(No description available)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row3_col0\" class=\"data row3 col0\" >⚡ Chronos-Bolt (Speed)</td>\n",
       "      <td id=\"T_6bfc5_row3_col1\" class=\"data row3 col1\" >0. Configuration</td>\n",
       "      <td id=\"T_6bfc5_row3_col2\" class=\"data row3 col2\" >Hyperparameters</td>\n",
       "      <td id=\"T_6bfc5_row3_col3\" class=\"data row3 col3\" >ChronosBoltConfig</td>\n",
       "      <td id=\"T_6bfc5_row3_col4\" class=\"data row3 col4\" >ChronosBoltConfig(context_length: int, prediction_length: int, input_patch_size: int, input_patch_stride: int, quantiles: List[float], use_reg_token: bool = False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row4_col0\" class=\"data row4 col0\" >⚡ Chronos-Bolt (Speed)</td>\n",
       "      <td id=\"T_6bfc5_row4_col1\" class=\"data row4 col1\" >1. Inference Pipeline</td>\n",
       "      <td id=\"T_6bfc5_row4_col2\" class=\"data row4 col2\" >End-to-End Workflow</td>\n",
       "      <td id=\"T_6bfc5_row4_col3\" class=\"data row4 col3\" >ChronosBoltPipeline</td>\n",
       "      <td id=\"T_6bfc5_row4_col4\" class=\"data row4 col4\" >(No description available)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row5_col0\" class=\"data row5 col0\" >⚡ Chronos-Bolt (Speed)</td>\n",
       "      <td id=\"T_6bfc5_row5_col1\" class=\"data row5 col1\" >2. Model Architecture</td>\n",
       "      <td id=\"T_6bfc5_row5_col2\" class=\"data row5 col2\" >Component Implementation</td>\n",
       "      <td id=\"T_6bfc5_row5_col3\" class=\"data row5 col3\" >ChronosBoltModelForForecasting</td>\n",
       "      <td id=\"T_6bfc5_row5_col4\" class=\"data row5 col4\" >This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row6_col0\" class=\"data row6 col0\" >⚡ Chronos-Bolt (Speed)</td>\n",
       "      <td id=\"T_6bfc5_row6_col1\" class=\"data row6 col1\" >4. Output Structures</td>\n",
       "      <td id=\"T_6bfc5_row6_col2\" class=\"data row6 col2\" >Component Implementation</td>\n",
       "      <td id=\"T_6bfc5_row6_col3\" class=\"data row6 col3\" >ChronosBoltOutput</td>\n",
       "      <td id=\"T_6bfc5_row6_col4\" class=\"data row6 col4\" >ChronosBoltOutput(loss: Optional[torch.Tensor] = None, quantile_preds: Optional[torch.Tensor] = None, attentions: Optional[torch.Tensor] = None, cross_attentions: Optional[torch.Tensor] = None)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row7_col0\" class=\"data row7 col0\" >🏗️ Base Infrastructure</td>\n",
       "      <td id=\"T_6bfc5_row7_col1\" class=\"data row7 col1\" >1. Inference Pipeline</td>\n",
       "      <td id=\"T_6bfc5_row7_col2\" class=\"data row7 col2\" >End-to-End Workflow</td>\n",
       "      <td id=\"T_6bfc5_row7_col3\" class=\"data row7 col3\" >BaseChronosPipeline</td>\n",
       "      <td id=\"T_6bfc5_row7_col4\" class=\"data row7 col4\" >(No description available)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row8_col0\" class=\"data row8 col0\" >🏗️ Base Infrastructure</td>\n",
       "      <td id=\"T_6bfc5_row8_col1\" class=\"data row8 col1\" >1. Inference Pipeline</td>\n",
       "      <td id=\"T_6bfc5_row8_col2\" class=\"data row8 col2\" >End-to-End Workflow</td>\n",
       "      <td id=\"T_6bfc5_row8_col3\" class=\"data row8 col3\" >PipelineRegistry</td>\n",
       "      <td id=\"T_6bfc5_row8_col4\" class=\"data row8 col4\" >type(object) -> the object's type</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row9_col0\" class=\"data row9 col0\" >🏗️ Base Infrastructure</td>\n",
       "      <td id=\"T_6bfc5_row9_col1\" class=\"data row9 col1\" >Misc / Components</td>\n",
       "      <td id=\"T_6bfc5_row9_col2\" class=\"data row9 col2\" >Component Implementation</td>\n",
       "      <td id=\"T_6bfc5_row9_col3\" class=\"data row9 col3\" >ForecastType</td>\n",
       "      <td id=\"T_6bfc5_row9_col4\" class=\"data row9 col4\" >Create a collection of name/value pairs.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row10_col0\" class=\"data row10 col0\" >🏗️ Base Infrastructure</td>\n",
       "      <td id=\"T_6bfc5_row10_col1\" class=\"data row10 col1\" >Misc / Components</td>\n",
       "      <td id=\"T_6bfc5_row10_col2\" class=\"data row10 col2\" >Component Implementation</td>\n",
       "      <td id=\"T_6bfc5_row10_col3\" class=\"data row10 col3\" >left_pad_and_stack_1D</td>\n",
       "      <td id=\"T_6bfc5_row10_col4\" class=\"data row10 col4\" >(No description available)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row11_col0\" class=\"data row11 col0\" >📊 Data Engineering</td>\n",
       "      <td id=\"T_6bfc5_row11_col1\" class=\"data row11 col1\" >3. Data Processing</td>\n",
       "      <td id=\"T_6bfc5_row11_col2\" class=\"data row11 col2\" >Component Implementation</td>\n",
       "      <td id=\"T_6bfc5_row11_col3\" class=\"data row11 col3\" >validate_df_inputs</td>\n",
       "      <td id=\"T_6bfc5_row11_col4\" class=\"data row11 col4\" >Validates and prepares dataframe inputs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row12_col0\" class=\"data row12 col0\" >🚀 Chronos-2 (Core)</td>\n",
       "      <td id=\"T_6bfc5_row12_col1\" class=\"data row12 col1\" >0. Configuration</td>\n",
       "      <td id=\"T_6bfc5_row12_col2\" class=\"data row12 col2\" >Hyperparameters</td>\n",
       "      <td id=\"T_6bfc5_row12_col3\" class=\"data row12 col3\" >Chronos2CoreConfig</td>\n",
       "      <td id=\"T_6bfc5_row12_col4\" class=\"data row12 col4\" >HF transformers-style pretrained model config for Chronos-2.0, based on T5Config.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row13_col0\" class=\"data row13 col0\" >🚀 Chronos-2 (Core)</td>\n",
       "      <td id=\"T_6bfc5_row13_col1\" class=\"data row13 col1\" >0. Configuration</td>\n",
       "      <td id=\"T_6bfc5_row13_col2\" class=\"data row13 col2\" >Hyperparameters</td>\n",
       "      <td id=\"T_6bfc5_row13_col3\" class=\"data row13 col3\" >Chronos2ForecastingConfig</td>\n",
       "      <td id=\"T_6bfc5_row13_col4\" class=\"data row13 col4\" >Chronos2ForecastingConfig(context_length: int, output_patch_size: int, input_patch_size: int, input_patch_stride: int, quantiles: List[float], use_reg_token: bool = False, use_arcsinh: bool = False, max_output_patches: int = 1, time_encoding_scale: int | None = None)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row14_col0\" class=\"data row14 col0\" >🚀 Chronos-2 (Core)</td>\n",
       "      <td id=\"T_6bfc5_row14_col1\" class=\"data row14 col1\" >1. Inference Pipeline</td>\n",
       "      <td id=\"T_6bfc5_row14_col2\" class=\"data row14 col2\" >End-to-End Workflow</td>\n",
       "      <td id=\"T_6bfc5_row14_col3\" class=\"data row14 col3\" >Chronos2Pipeline</td>\n",
       "      <td id=\"T_6bfc5_row14_col4\" class=\"data row14 col4\" >(No description available)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row15_col0\" class=\"data row15 col0\" >🚀 Chronos-2 (Core)</td>\n",
       "      <td id=\"T_6bfc5_row15_col1\" class=\"data row15 col1\" >2. Model Architecture</td>\n",
       "      <td id=\"T_6bfc5_row15_col2\" class=\"data row15 col2\" >Component Implementation</td>\n",
       "      <td id=\"T_6bfc5_row15_col3\" class=\"data row15 col3\" >Chronos2Encoder</td>\n",
       "      <td id=\"T_6bfc5_row15_col4\" class=\"data row15 col4\" >Base class for all neural network modules.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row16_col0\" class=\"data row16 col0\" >🚀 Chronos-2 (Core)</td>\n",
       "      <td id=\"T_6bfc5_row16_col1\" class=\"data row16 col1\" >2. Model Architecture</td>\n",
       "      <td id=\"T_6bfc5_row16_col2\" class=\"data row16 col2\" >Component Implementation</td>\n",
       "      <td id=\"T_6bfc5_row16_col3\" class=\"data row16 col3\" >Chronos2EncoderBlock</td>\n",
       "      <td id=\"T_6bfc5_row16_col4\" class=\"data row16 col4\" >Base class for all neural network modules.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row17_col0\" class=\"data row17 col0\" >🚀 Chronos-2 (Core)</td>\n",
       "      <td id=\"T_6bfc5_row17_col1\" class=\"data row17 col1\" >2. Model Architecture</td>\n",
       "      <td id=\"T_6bfc5_row17_col2\" class=\"data row17 col2\" >Component Implementation</td>\n",
       "      <td id=\"T_6bfc5_row17_col3\" class=\"data row17 col3\" >Chronos2EncoderBlockOutput</td>\n",
       "      <td id=\"T_6bfc5_row17_col4\" class=\"data row17 col4\" >Chronos2EncoderBlockOutput(hidden_states: torch.Tensor | None = None, time_self_attn_weights: torch.Tensor | None = None, group_self_attn_weights: torch.Tensor | None = None)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row18_col0\" class=\"data row18 col0\" >🚀 Chronos-2 (Core)</td>\n",
       "      <td id=\"T_6bfc5_row18_col1\" class=\"data row18 col1\" >2. Model Architecture</td>\n",
       "      <td id=\"T_6bfc5_row18_col2\" class=\"data row18 col2\" >Component Implementation</td>\n",
       "      <td id=\"T_6bfc5_row18_col3\" class=\"data row18 col3\" >Chronos2EncoderOutput</td>\n",
       "      <td id=\"T_6bfc5_row18_col4\" class=\"data row18 col4\" >Chronos2EncoderOutput(last_hidden_state: torch.Tensor | None = None, all_time_self_attn_weights: tuple[torch.Tensor, ...] | None = None, all_group_self_attn_weights: tuple[torch.Tensor, ...] | None = None)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row19_col0\" class=\"data row19 col0\" >🚀 Chronos-2 (Core)</td>\n",
       "      <td id=\"T_6bfc5_row19_col1\" class=\"data row19 col1\" >2. Model Architecture</td>\n",
       "      <td id=\"T_6bfc5_row19_col2\" class=\"data row19 col2\" >Component Implementation</td>\n",
       "      <td id=\"T_6bfc5_row19_col3\" class=\"data row19 col3\" >Chronos2Model</td>\n",
       "      <td id=\"T_6bfc5_row19_col4\" class=\"data row19 col4\" >Base class for all models.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row20_col0\" class=\"data row20 col0\" >🚀 Chronos-2 (Core)</td>\n",
       "      <td id=\"T_6bfc5_row20_col1\" class=\"data row20 col1\" >3. Data Processing</td>\n",
       "      <td id=\"T_6bfc5_row20_col2\" class=\"data row20 col2\" >Component Implementation</td>\n",
       "      <td id=\"T_6bfc5_row20_col3\" class=\"data row20 col3\" >Chronos2Dataset</td>\n",
       "      <td id=\"T_6bfc5_row20_col4\" class=\"data row20 col4\" >A dataset wrapper for Chronos-2 models.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row21_col0\" class=\"data row21 col0\" >🚀 Chronos-2 (Core)</td>\n",
       "      <td id=\"T_6bfc5_row21_col1\" class=\"data row21 col1\" >3. Data Processing</td>\n",
       "      <td id=\"T_6bfc5_row21_col2\" class=\"data row21 col2\" >Component Implementation</td>\n",
       "      <td id=\"T_6bfc5_row21_col3\" class=\"data row21 col3\" >DatasetMode</td>\n",
       "      <td id=\"T_6bfc5_row21_col4\" class=\"data row21 col4\" >str(object='') -> str</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row22_col0\" class=\"data row22 col0\" >🚀 Chronos-2 (Core)</td>\n",
       "      <td id=\"T_6bfc5_row22_col1\" class=\"data row22 col1\" >3. Data Processing</td>\n",
       "      <td id=\"T_6bfc5_row22_col2\" class=\"data row22 col2\" >Component Implementation</td>\n",
       "      <td id=\"T_6bfc5_row22_col3\" class=\"data row22 col3\" >convert_df_input_to_list_of_dicts_input</td>\n",
       "      <td id=\"T_6bfc5_row22_col4\" class=\"data row22 col4\" >Convert from dataframe input format to a list of dictionaries input format.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row23_col0\" class=\"data row23 col0\" >🚀 Chronos-2 (Core)</td>\n",
       "      <td id=\"T_6bfc5_row23_col1\" class=\"data row23 col1\" >3. Data Processing</td>\n",
       "      <td id=\"T_6bfc5_row23_col2\" class=\"data row23 col2\" >Component Implementation</td>\n",
       "      <td id=\"T_6bfc5_row23_col3\" class=\"data row23 col3\" >convert_fev_window_to_list_of_dicts_input</td>\n",
       "      <td id=\"T_6bfc5_row23_col4\" class=\"data row23 col4\" >(No description available)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row24_col0\" class=\"data row24 col0\" >🚀 Chronos-2 (Core)</td>\n",
       "      <td id=\"T_6bfc5_row24_col1\" class=\"data row24 col1\" >3. Data Processing</td>\n",
       "      <td id=\"T_6bfc5_row24_col2\" class=\"data row24 col2\" >Component Implementation</td>\n",
       "      <td id=\"T_6bfc5_row24_col3\" class=\"data row24 col3\" >convert_list_of_tensors_input_to_list_of_dicts_input</td>\n",
       "      <td id=\"T_6bfc5_row24_col4\" class=\"data row24 col4\" >Convert a list of tensors input format to a list of dictionaries input format.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row25_col0\" class=\"data row25 col0\" >🚀 Chronos-2 (Core)</td>\n",
       "      <td id=\"T_6bfc5_row25_col1\" class=\"data row25 col1\" >3. Data Processing</td>\n",
       "      <td id=\"T_6bfc5_row25_col2\" class=\"data row25 col2\" >Component Implementation</td>\n",
       "      <td id=\"T_6bfc5_row25_col3\" class=\"data row25 col3\" >convert_tensor_input_to_list_of_dicts_input</td>\n",
       "      <td id=\"T_6bfc5_row25_col4\" class=\"data row25 col4\" >Convert a tensor input format to a list of dictionaries input format.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row26_col0\" class=\"data row26 col0\" >🚀 Chronos-2 (Core)</td>\n",
       "      <td id=\"T_6bfc5_row26_col1\" class=\"data row26 col1\" >3. Data Processing</td>\n",
       "      <td id=\"T_6bfc5_row26_col2\" class=\"data row26 col2\" >Component Implementation</td>\n",
       "      <td id=\"T_6bfc5_row26_col3\" class=\"data row26 col3\" >validate_and_prepare_single_dict_task</td>\n",
       "      <td id=\"T_6bfc5_row26_col4\" class=\"data row26 col4\" >Validates and prepares a single dictionary task for Chronos2Model.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row27_col0\" class=\"data row27 col0\" >🚀 Chronos-2 (Core)</td>\n",
       "      <td id=\"T_6bfc5_row27_col1\" class=\"data row27 col1\" >4. Output Structures</td>\n",
       "      <td id=\"T_6bfc5_row27_col2\" class=\"data row27 col2\" >Component Implementation</td>\n",
       "      <td id=\"T_6bfc5_row27_col3\" class=\"data row27 col3\" >AttentionOutput</td>\n",
       "      <td id=\"T_6bfc5_row27_col4\" class=\"data row27 col4\" >AttentionOutput(hidden_states: torch.Tensor | None = None, attn_weights: torch.Tensor | None = None)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row28_col0\" class=\"data row28 col0\" >🚀 Chronos-2 (Core)</td>\n",
       "      <td id=\"T_6bfc5_row28_col1\" class=\"data row28 col1\" >4. Output Structures</td>\n",
       "      <td id=\"T_6bfc5_row28_col2\" class=\"data row28 col2\" >Component Implementation</td>\n",
       "      <td id=\"T_6bfc5_row28_col3\" class=\"data row28 col3\" >Chronos2Output</td>\n",
       "      <td id=\"T_6bfc5_row28_col4\" class=\"data row28 col4\" >Chronos2Output(loss: torch.Tensor | None = None, quantile_preds: torch.Tensor | None = None, enc_time_self_attn_weights: tuple[torch.Tensor, ...] | None = None, enc_group_self_attn_weights: tuple[torch.Tensor, ...] | None = None)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row29_col0\" class=\"data row29 col0\" >🚀 Chronos-2 (Core)</td>\n",
       "      <td id=\"T_6bfc5_row29_col1\" class=\"data row29 col1\" >Attention Mechanism</td>\n",
       "      <td id=\"T_6bfc5_row29_col2\" class=\"data row29 col2\" >Multivariate (Group Interaction)</td>\n",
       "      <td id=\"T_6bfc5_row29_col3\" class=\"data row29 col3\" >GroupSelfAttention</td>\n",
       "      <td id=\"T_6bfc5_row29_col4\" class=\"data row29 col4\" >Self-attention applied along the batch axis masked by the group attention mask</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row30_col0\" class=\"data row30 col0\" >🚀 Chronos-2 (Core)</td>\n",
       "      <td id=\"T_6bfc5_row30_col1\" class=\"data row30 col1\" >Attention Mechanism</td>\n",
       "      <td id=\"T_6bfc5_row30_col2\" class=\"data row30 col2\" >Covariate Support</td>\n",
       "      <td id=\"T_6bfc5_row30_col3\" class=\"data row30 col3\" >TimeCrossAttention</td>\n",
       "      <td id=\"T_6bfc5_row30_col4\" class=\"data row30 col4\" >Base class for all neural network modules.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row31_col0\" class=\"data row31 col0\" >🚀 Chronos-2 (Core)</td>\n",
       "      <td id=\"T_6bfc5_row31_col1\" class=\"data row31 col1\" >Attention Mechanism</td>\n",
       "      <td id=\"T_6bfc5_row31_col2\" class=\"data row31 col2\" >Component Implementation</td>\n",
       "      <td id=\"T_6bfc5_row31_col3\" class=\"data row31 col3\" >TimeSelfAttention</td>\n",
       "      <td id=\"T_6bfc5_row31_col4\" class=\"data row31 col4\" >Base class for all neural network modules.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row32_col0\" class=\"data row32 col0\" >🚀 Chronos-2 (Core)</td>\n",
       "      <td id=\"T_6bfc5_row32_col1\" class=\"data row32 col1\" >Misc / Components</td>\n",
       "      <td id=\"T_6bfc5_row32_col2\" class=\"data row32 col2\" >Component Implementation</td>\n",
       "      <td id=\"T_6bfc5_row32_col3\" class=\"data row32 col3\" >Chronos2LayerNorm</td>\n",
       "      <td id=\"T_6bfc5_row32_col4\" class=\"data row32 col4\" >Base class for all neural network modules.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row33_col0\" class=\"data row33 col0\" >🚀 Chronos-2 (Core)</td>\n",
       "      <td id=\"T_6bfc5_row33_col1\" class=\"data row33 col1\" >Misc / Components</td>\n",
       "      <td id=\"T_6bfc5_row33_col2\" class=\"data row33 col2\" >Component Implementation</td>\n",
       "      <td id=\"T_6bfc5_row33_col3\" class=\"data row33 col3\" >Chronos2Trainer</td>\n",
       "      <td id=\"T_6bfc5_row33_col4\" class=\"data row33 col4\" >A custom trainer based on transformers Trainer. We need to override the dataloader getters because we handle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row34_col0\" class=\"data row34 col0\" >🚀 Chronos-2 (Core)</td>\n",
       "      <td id=\"T_6bfc5_row34_col1\" class=\"data row34 col1\" >Misc / Components</td>\n",
       "      <td id=\"T_6bfc5_row34_col2\" class=\"data row34 col2\" >Component Implementation</td>\n",
       "      <td id=\"T_6bfc5_row34_col3\" class=\"data row34 col3\" >EvaluateAndSaveFinalStepCallback</td>\n",
       "      <td id=\"T_6bfc5_row34_col4\" class=\"data row34 col4\" >Callback to evaluate and save the model at last training step.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row35_col0\" class=\"data row35 col0\" >🚀 Chronos-2 (Core)</td>\n",
       "      <td id=\"T_6bfc5_row35_col1\" class=\"data row35 col1\" >Misc / Components</td>\n",
       "      <td id=\"T_6bfc5_row35_col2\" class=\"data row35 col2\" >Component Implementation</td>\n",
       "      <td id=\"T_6bfc5_row35_col3\" class=\"data row35 col3\" >FeedForward</td>\n",
       "      <td id=\"T_6bfc5_row35_col4\" class=\"data row35 col4\" >Base class for all neural network modules.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row36_col0\" class=\"data row36 col0\" >🚀 Chronos-2 (Core)</td>\n",
       "      <td id=\"T_6bfc5_row36_col1\" class=\"data row36 col1\" >Misc / Components</td>\n",
       "      <td id=\"T_6bfc5_row36_col2\" class=\"data row36 col2\" >Component Implementation</td>\n",
       "      <td id=\"T_6bfc5_row36_col3\" class=\"data row36 col3\" >InstanceNorm</td>\n",
       "      <td id=\"T_6bfc5_row36_col4\" class=\"data row36 col4\" >Apply standardization along the last dimension and optionally apply arcsinh after standardization.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row37_col0\" class=\"data row37 col0\" >🚀 Chronos-2 (Core)</td>\n",
       "      <td id=\"T_6bfc5_row37_col1\" class=\"data row37 col1\" >Misc / Components</td>\n",
       "      <td id=\"T_6bfc5_row37_col2\" class=\"data row37 col2\" >Component Implementation</td>\n",
       "      <td id=\"T_6bfc5_row37_col3\" class=\"data row37 col3\" >MHA</td>\n",
       "      <td id=\"T_6bfc5_row37_col4\" class=\"data row37 col4\" >Multi-head Attention Layer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row38_col0\" class=\"data row38 col0\" >🚀 Chronos-2 (Core)</td>\n",
       "      <td id=\"T_6bfc5_row38_col1\" class=\"data row38 col1\" >Misc / Components</td>\n",
       "      <td id=\"T_6bfc5_row38_col2\" class=\"data row38 col2\" >Component Implementation</td>\n",
       "      <td id=\"T_6bfc5_row38_col3\" class=\"data row38 col3\" >MLP</td>\n",
       "      <td id=\"T_6bfc5_row38_col4\" class=\"data row38 col4\" >Base class for all neural network modules.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row39_col0\" class=\"data row39 col0\" >🚀 Chronos-2 (Core)</td>\n",
       "      <td id=\"T_6bfc5_row39_col1\" class=\"data row39 col1\" >Misc / Components</td>\n",
       "      <td id=\"T_6bfc5_row39_col2\" class=\"data row39 col2\" >Component Implementation</td>\n",
       "      <td id=\"T_6bfc5_row39_col3\" class=\"data row39 col3\" >Patch</td>\n",
       "      <td id=\"T_6bfc5_row39_col4\" class=\"data row39 col4\" >Base class for all neural network modules.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row40_col0\" class=\"data row40 col0\" >🚀 Chronos-2 (Core)</td>\n",
       "      <td id=\"T_6bfc5_row40_col1\" class=\"data row40 col1\" >Misc / Components</td>\n",
       "      <td id=\"T_6bfc5_row40_col2\" class=\"data row40 col2\" >Component Implementation</td>\n",
       "      <td id=\"T_6bfc5_row40_col3\" class=\"data row40 col3\" >ResidualBlock</td>\n",
       "      <td id=\"T_6bfc5_row40_col4\" class=\"data row40 col4\" >A generic residual block which can be used for input and output embedding layers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row41_col0\" class=\"data row41 col0\" >🚀 Chronos-2 (Core)</td>\n",
       "      <td id=\"T_6bfc5_row41_col1\" class=\"data row41 col1\" >Misc / Components</td>\n",
       "      <td id=\"T_6bfc5_row41_col2\" class=\"data row41 col2\" >Component Implementation</td>\n",
       "      <td id=\"T_6bfc5_row41_col3\" class=\"data row41 col3\" >RoPE</td>\n",
       "      <td id=\"T_6bfc5_row41_col4\" class=\"data row41 col4\" >Applies rotary position embeddings (RoPE) to input tensors.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row42_col0\" class=\"data row42 col0\" >🚀 Chronos-2 (Core)</td>\n",
       "      <td id=\"T_6bfc5_row42_col1\" class=\"data row42 col1\" >Misc / Components</td>\n",
       "      <td id=\"T_6bfc5_row42_col2\" class=\"data row42 col2\" >Probabilistic / Uncertainty</td>\n",
       "      <td id=\"T_6bfc5_row42_col3\" class=\"data row42 col3\" >interpolate_quantiles</td>\n",
       "      <td id=\"T_6bfc5_row42_col4\" class=\"data row42 col4\" >Interpolates quantile values at specified query levels using linear interpolation using original</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row43_col0\" class=\"data row43 col0\" >🚀 Chronos-2 (Core)</td>\n",
       "      <td id=\"T_6bfc5_row43_col1\" class=\"data row43 col1\" >Misc / Components</td>\n",
       "      <td id=\"T_6bfc5_row43_col2\" class=\"data row43 col2\" >Component Implementation</td>\n",
       "      <td id=\"T_6bfc5_row43_col3\" class=\"data row43 col3\" >left_pad_and_cat_2D</td>\n",
       "      <td id=\"T_6bfc5_row43_col4\" class=\"data row43 col4\" >Left pads tensors in the list to the length of the longest tensor along the second axis, then concats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row44_col0\" class=\"data row44 col0\" >🚀 Chronos-2 (Core)</td>\n",
       "      <td id=\"T_6bfc5_row44_col1\" class=\"data row44 col1\" >Misc / Components</td>\n",
       "      <td id=\"T_6bfc5_row44_col2\" class=\"data row44 col2\" >Component Implementation</td>\n",
       "      <td id=\"T_6bfc5_row44_col3\" class=\"data row44 col3\" >seed_worker</td>\n",
       "      <td id=\"T_6bfc5_row44_col4\" class=\"data row44 col4\" >(No description available)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row45_col0\" class=\"data row45 col0\" >🚀 Chronos-2 (Core)</td>\n",
       "      <td id=\"T_6bfc5_row45_col1\" class=\"data row45 col1\" >Misc / Components</td>\n",
       "      <td id=\"T_6bfc5_row45_col2\" class=\"data row45 col2\" >Component Implementation</td>\n",
       "      <td id=\"T_6bfc5_row45_col3\" class=\"data row45 col3\" >weighted_quantile</td>\n",
       "      <td id=\"T_6bfc5_row45_col4\" class=\"data row45 col4\" >Computes quantiles from a distribution specified by `samples` and their corresponding probability mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row46_col0\" class=\"data row46 col0\" >🛠️ Utilities & Common</td>\n",
       "      <td id=\"T_6bfc5_row46_col1\" class=\"data row46 col1\" >0. Configuration</td>\n",
       "      <td id=\"T_6bfc5_row46_col2\" class=\"data row46 col2\" >Hyperparameters</td>\n",
       "      <td id=\"T_6bfc5_row46_col3\" class=\"data row46 col3\" >ChronosConfig</td>\n",
       "      <td id=\"T_6bfc5_row46_col4\" class=\"data row46 col4\" >This class holds all the configuration parameters to be used</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row47_col0\" class=\"data row47 col0\" >🛠️ Utilities & Common</td>\n",
       "      <td id=\"T_6bfc5_row47_col1\" class=\"data row47 col1\" >1. Inference Pipeline</td>\n",
       "      <td id=\"T_6bfc5_row47_col2\" class=\"data row47 col2\" >End-to-End Workflow</td>\n",
       "      <td id=\"T_6bfc5_row47_col3\" class=\"data row47 col3\" >ChronosPipeline</td>\n",
       "      <td id=\"T_6bfc5_row47_col4\" class=\"data row47 col4\" >A ``ChronosPipeline`` uses the given tokenizer and model to forecast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row48_col0\" class=\"data row48 col0\" >🛠️ Utilities & Common</td>\n",
       "      <td id=\"T_6bfc5_row48_col1\" class=\"data row48 col1\" >2. Model Architecture</td>\n",
       "      <td id=\"T_6bfc5_row48_col2\" class=\"data row48 col2\" >Component Implementation</td>\n",
       "      <td id=\"T_6bfc5_row48_col3\" class=\"data row48 col3\" >ChronosModel</td>\n",
       "      <td id=\"T_6bfc5_row48_col4\" class=\"data row48 col4\" >A ``ChronosModel`` wraps a ``PreTrainedModel`` object from ``transformers``</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row49_col0\" class=\"data row49 col0\" >🛠️ Utilities & Common</td>\n",
       "      <td id=\"T_6bfc5_row49_col1\" class=\"data row49 col1\" >Misc / Components</td>\n",
       "      <td id=\"T_6bfc5_row49_col2\" class=\"data row49 col2\" >Component Implementation</td>\n",
       "      <td id=\"T_6bfc5_row49_col3\" class=\"data row49 col3\" >ChronosTokenizer</td>\n",
       "      <td id=\"T_6bfc5_row49_col4\" class=\"data row49 col4\" >A ``ChronosTokenizer`` defines how time series are mapped into token IDs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_6bfc5_row50_col0\" class=\"data row50 col0\" >🛠️ Utilities & Common</td>\n",
       "      <td id=\"T_6bfc5_row50_col1\" class=\"data row50 col1\" >Misc / Components</td>\n",
       "      <td id=\"T_6bfc5_row50_col2\" class=\"data row50 col2\" >Component Implementation</td>\n",
       "      <td id=\"T_6bfc5_row50_col3\" class=\"data row50 col3\" >MeanScaleUniformBins</td>\n",
       "      <td id=\"T_6bfc5_row50_col4\" class=\"data row50 col4\" >A ``ChronosTokenizer`` defines how time series are mapped into token IDs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2b2025d7150>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- 1. データの準備 ---\n",
    "# 前のステップで作成したCSVがあれば読み込み、なければメモリ上のdf_catalogを使用\n",
    "csv_path = \"chronos_capabilities.csv\"\n",
    "if os.path.exists(csv_path):\n",
    "    df_catalog = pd.read_csv(csv_path)\n",
    "    print(f\"✅ Loaded catalog from {csv_path}\")\n",
    "else:\n",
    "    # df_catalogがメモリにない場合のフォールバック（通常は前のセルから引き継ぎます）\n",
    "    print(\"⚠️ 'df_catalog' variable or CSV file not found. Please run the discovery step first.\")\n",
    "\n",
    "# --- 2. カスケード分類ロジックの実装 ---\n",
    "def categorize_cascade(row):\n",
    "    name = str(row['Name'])\n",
    "    module = str(row['Module'])\n",
    "    desc = str(row['Description']).lower()\n",
    "    \n",
    "    # --- Level 1: Major Component (製品ライン・大分類) ---\n",
    "    if 'Bolt' in name:\n",
    "        l1 = \"⚡ Chronos-Bolt (Speed)\"\n",
    "    elif 'Chronos2' in name or 'chronos2' in module:\n",
    "        l1 = \"🚀 Chronos-2 (Core)\"\n",
    "    elif 'boto' in module:\n",
    "        l1 = \"☁️ AWS Integration\"\n",
    "    elif 'base' in module:\n",
    "        l1 = \"🏗️ Base Infrastructure\"\n",
    "    elif 'df_utils' in module or 'dataset' in module:\n",
    "        l1 = \"📊 Data Engineering\"\n",
    "    elif any(x in name for x in ['Attention', 'Layer', 'Norm', 'Embedding', 'ResNet']):\n",
    "        l1 = \"🧠 Model Internals\"\n",
    "    else:\n",
    "        l1 = \"🛠️ Utilities & Common\"\n",
    "\n",
    "    # --- Level 2: Role (役割・中分類) ---\n",
    "    if 'Pipeline' in name:\n",
    "        l2 = \"1. Inference Pipeline\"\n",
    "    elif 'Config' in name:\n",
    "        l2 = \"0. Configuration\"\n",
    "    elif 'Model' in name or 'Encoder' in name:\n",
    "        l2 = \"2. Model Architecture\"\n",
    "    elif 'Dataset' in name or 'convert' in name or 'validate' in name:\n",
    "        l2 = \"3. Data Processing\"\n",
    "    elif 'Output' in name:\n",
    "        l2 = \"4. Output Structures\"\n",
    "    elif 'Attention' in name:\n",
    "        l2 = \"Attention Mechanism\"\n",
    "    elif 'download' in name or 'cache' in name:\n",
    "        l2 = \"S3 / IO\"\n",
    "    else:\n",
    "        l2 = \"Misc / Components\"\n",
    "\n",
    "    # --- Level 3: Detail (詳細・小分類) ---\n",
    "    if 'Multivariate' in desc or 'group' in name.lower():\n",
    "        l3 = \"Multivariate (Group Interaction)\"\n",
    "    elif 'Quantile' in desc or 'quantiles' in name.lower() or 'probabilistic' in desc:\n",
    "        l3 = \"Probabilistic / Uncertainty\"\n",
    "    elif 'Covariate' in desc or 'cross' in name.lower():\n",
    "        l3 = \"Covariate Support\"\n",
    "    elif 'Pipeline' in name:\n",
    "        l3 = \"End-to-End Workflow\"\n",
    "    elif 'Config' in name:\n",
    "        l3 = \"Hyperparameters\"\n",
    "    else:\n",
    "        # シグネチャや名前から推測\n",
    "        l3 = \"Component Implementation\"\n",
    "\n",
    "    return pd.Series([l1, l2, l3], index=['L1_Component', 'L2_Role', 'L3_Detail'])\n",
    "\n",
    "# --- 3. 適用と整形 ---\n",
    "# 分類を適用\n",
    "cascade_df = df_catalog.apply(categorize_cascade, axis=1)\n",
    "df_tree = pd.concat([df_catalog, cascade_df], axis=1)\n",
    "\n",
    "# 見やすい順序にソート\n",
    "df_tree = df_tree.sort_values(by=['L1_Component', 'L2_Role', 'Name'])\n",
    "\n",
    "# 必要な列だけに絞り込み\n",
    "display_cols = ['L1_Component', 'L2_Role', 'L3_Detail', 'Name', 'Description']\n",
    "df_final = df_tree[display_cols]\n",
    "\n",
    "# --- 4. 視覚化 (Tree View) ---\n",
    "def print_tree_view(df):\n",
    "    \"\"\"テキスト形式でツリー構造を表示する\"\"\"\n",
    "    print(\"\\n🌳 Chronos-2 Library Structure Tree\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    current_l1 = \"\"\n",
    "    current_l2 = \"\"\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        # L1 (Component) Change\n",
    "        if row['L1_Component'] != current_l1:\n",
    "            print(f\"\\n📦 {row['L1_Component']}\")\n",
    "            current_l1 = row['L1_Component']\n",
    "            current_l2 = \"\" # Reset L2\n",
    "            \n",
    "        # L2 (Role) Change\n",
    "        if row['L2_Role'] != current_l2:\n",
    "            print(f\"  ├─ 📂 {row['L2_Role']}\")\n",
    "            current_l2 = row['L2_Role']\n",
    "            \n",
    "        # Leaf (Item)\n",
    "        # 詳細説明が長い場合は切り詰め\n",
    "        desc_short = (row['Description'][:60] + '...') if len(str(row['Description'])) > 60 else row['Description']\n",
    "        print(f\"  │    ├─ {row['Name']:<35} : {desc_short}\")\n",
    "\n",
    "# ツリー表示の実行\n",
    "print_tree_view(df_final)\n",
    "\n",
    "# --- 5. Pandas Stylerによるテーブル表示 (Notebook用) ---\n",
    "# インタラクティブなテーブルとして表示\n",
    "print(\"\\n📊 Detailed Interactive Table\")\n",
    "(df_final.style\n",
    "    .set_properties(**{'text-align': 'left'})\n",
    "    .set_table_styles([\n",
    "        {'selector': 'th', 'props': [('text-align', 'left'), ('background-color', '#f0f0f0')]}\n",
    "    ])\n",
    "    .hide(axis='index')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efed5b71",
   "metadata": {},
   "source": [
    "# 📅 Chronos-2 ライフサイクル（イベント）別機能マップ\n",
    "\n",
    "ライブラリの全機能を、機械学習プロジェクトの**実行プロセス（イベント）**ごとに再分類します。\n",
    "これにより、「今、データ準備フェーズだからこの関数を使う」といった実践的なナビゲーションが可能になります。\n",
    "\n",
    "### 🏷️ イベント分類の定義\n",
    "1.  **🏁 Setup & Config (設定・準備)**:\n",
    "    * ハイパーパラメータ設定、環境構築、S3からのダウンロードなど。\n",
    "2.  **🧹 Data Preparation (データ前処理)**:\n",
    "    * DataFrameの検証、テンソル変換、パディング、データセット作成。\n",
    "3.  **🧠 Model Loading (モデル読込・構築)**:\n",
    "    * モデルアーキテクチャの定義、事前学習済みモデルのロード。\n",
    "4.  **🏋️ Training & Tuning (学習・ファインチューニング)**:\n",
    "    * Trainer、損失計算、学習ループ関連（追加学習を行う場合）。\n",
    "5.  **🚀 Inference & Forecast (推論・予測)**:\n",
    "    * パイプライン実行、未来予測、分位点（Quantile）計算。\n",
    "6.  **🧩 Internal Processing (内部計算メカニズム)**:\n",
    "    * Attention機構、正規化、エンコーダーなど、推論の裏側で動く部品。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46ab93a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📅 Chronos-2 Workflow Event Map\n",
      "======================================================================\n",
      "\n",
      "1. 🏁 Setup & Config\n",
      "--------------------------------------------------\n",
      "  🔹 Chronos2CoreConfig                       : HF transformers-style pretrained model config for Chronos-2.0, based o...\n",
      "  🔹 Chronos2ForecastingConfig                : Chronos2ForecastingConfig(context_length: int, output_patch_size: int,...\n",
      "  🔹 ChronosBoltConfig                        : ChronosBoltConfig(context_length: int, prediction_length: int, input_p...\n",
      "  🔹 ChronosConfig                            : This class holds all the configuration parameters to be used\n",
      "  🔹 PipelineRegistry                         : type(object) -> the object's type\n",
      "\n",
      "1. 🏁 Setup & Config (Cloud/IO)\n",
      "--------------------------------------------------\n",
      "  🔹 cache_model_from_s3                      : (No description available)\n",
      "  🔹 download_model_files_from_cloudfront     : (No description available)\n",
      "  🔹 download_model_files_from_s3             : (No description available)\n",
      "\n",
      "2. 🧹 Data Preparation\n",
      "--------------------------------------------------\n",
      "  🔹 Chronos2Dataset                          : A dataset wrapper for Chronos-2 models.\n",
      "  🔹 DatasetMode                              : str(object='') -> str\n",
      "  🔹 convert_fev_window_to_list_of_dicts_input : (No description available)\n",
      "  🔹 convert_list_of_tensors_input_to_list_of_dicts_input : Convert a list of tensors input format to a list of dictionaries input...\n",
      "  🔹 convert_tensor_input_to_list_of_dicts_input : Convert a tensor input format to a list of dictionaries input format.\n",
      "  🔹 left_pad_and_cat_2D                      : Left pads tensors in the list to the length of the longest tensor alon...\n",
      "  🔹 validate_and_prepare_single_dict_task    : Validates and prepares a single dictionary task for Chronos2Model.\n",
      "\n",
      "2. 🧹 Data Preparation (Tokenization)\n",
      "--------------------------------------------------\n",
      "  🔹 ChronosTokenizer                         : A ``ChronosTokenizer`` defines how time series are mapped into token I...\n",
      "  🔹 MeanScaleUniformBins                     : A ``ChronosTokenizer`` defines how time series are mapped into token I...\n",
      "\n",
      "2. 🧹 Data Preparation (Utils)\n",
      "--------------------------------------------------\n",
      "  🔹 convert_df_input_to_list_of_dicts_input  : Convert from dataframe input format to a list of dictionaries input fo...\n",
      "  🔹 left_pad_and_stack_1D                    : (No description available)\n",
      "  🔹 validate_df_inputs                       : Validates and prepares dataframe inputs\n",
      "\n",
      "4. 🏋️ Training & Tuning\n",
      "--------------------------------------------------\n",
      "  🔹 Chronos2Trainer                          : A custom trainer based on transformers Trainer. We need to override th...\n",
      "  🔹 EvaluateAndSaveFinalStepCallback         : Callback to evaluate and save the model at last training step.\n",
      "\n",
      "5. 🚀 Inference (High-Level)\n",
      "--------------------------------------------------\n",
      "  🔹 BaseChronosPipeline                      : (No description available)\n",
      "  🔹 Chronos2Pipeline                         : (No description available)\n",
      "  🔹 ChronosBoltPipeline                      : (No description available)\n",
      "  🔹 ChronosPipeline                          : A ``ChronosPipeline`` uses the given tokenizer and model to forecast\n",
      "\n",
      "5. 🚀 Inference (Result)\n",
      "--------------------------------------------------\n",
      "  🔹 AttentionOutput                          : AttentionOutput(hidden_states: torch.Tensor | None = None, attn_weight...\n",
      "  🔹 Chronos2EncoderBlockOutput               : Chronos2EncoderBlockOutput(hidden_states: torch.Tensor | None = None, ...\n",
      "  🔹 Chronos2EncoderOutput                    : Chronos2EncoderOutput(last_hidden_state: torch.Tensor | None = None, a...\n",
      "  🔹 Chronos2Output                           : Chronos2Output(loss: torch.Tensor | None = None, quantile_preds: torch...\n",
      "  🔹 ChronosBoltModelForForecasting           : This model inherits from [`PreTrainedModel`]. Check the superclass doc...\n",
      "  🔹 ChronosBoltOutput                        : ChronosBoltOutput(loss: Optional[torch.Tensor] = None, quantile_preds:...\n",
      "  🔹 ForecastType                             : Create a collection of name/value pairs.\n",
      "  🔹 interpolate_quantiles                    : Interpolates quantile values at specified query levels using linear in...\n",
      "\n",
      "6. 🧩 Internal Layers (Low-Level)\n",
      "--------------------------------------------------\n",
      "  🔹 Chronos2Encoder                          : Base class for all neural network modules.\n",
      "  🔹 Chronos2EncoderBlock                     : Base class for all neural network modules.\n",
      "  🔹 Chronos2LayerNorm                        : Base class for all neural network modules.\n",
      "  🔹 GroupSelfAttention                       : Self-attention applied along the batch axis masked by the group attent...\n",
      "  🔹 InstanceNorm                             : Apply standardization along the last dimension and optionally apply ar...\n",
      "  🔹 MHA                                      : Multi-head Attention Layer\n",
      "  🔹 MLP                                      : Base class for all neural network modules.\n",
      "  🔹 ResidualBlock                            : A generic residual block which can be used for input and output embedd...\n",
      "  🔹 RoPE                                     : Applies rotary position embeddings (RoPE) to input tensors.\n",
      "  🔹 TimeCrossAttention                       : Base class for all neural network modules.\n",
      "  🔹 TimeSelfAttention                        : Base class for all neural network modules.\n",
      "\n",
      "7. 🛠️ Misc Utilities\n",
      "--------------------------------------------------\n",
      "  🔹 Chronos2Model                            : Base class for all models.\n",
      "  🔹 ChronosModel                             : A ``ChronosModel`` wraps a ``PreTrainedModel`` object from ``transform...\n",
      "  🔹 FeedForward                              : Base class for all neural network modules.\n",
      "  🔹 Patch                                    : Base class for all neural network modules.\n",
      "  🔹 seed_worker                              : (No description available)\n",
      "  🔹 weighted_quantile                        : Computes quantiles from a distribution specified by `samples` and thei...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- 1. データ読み込み ---\n",
    "csv_path = \"chronos_capabilities.csv\"\n",
    "if os.path.exists(csv_path):\n",
    "    df_catalog = pd.read_csv(csv_path)\n",
    "else:\n",
    "    # CSVがない場合は前のステップの変数を利用（エラーハンドリング）\n",
    "    print(\"⚠️ CSV file not found. Using 'df_catalog' from memory if available.\")\n",
    "\n",
    "# --- 2. イベントベース分類ロジック ---\n",
    "def classify_by_event(row):\n",
    "    name = str(row['Name'])\n",
    "    module = str(row['Module'])\n",
    "    desc = str(row['Description']).lower()\n",
    "    \n",
    "    # Prioritize specific keywords to map to lifecycle events\n",
    "    \n",
    "    # 1. 🏁 Setup & Config (設定・環境・ダウンロード)\n",
    "    if 'Config' in name or 'Registry' in name:\n",
    "        return \"1. 🏁 Setup & Config\"\n",
    "    if 'download' in name or 'cache' in name or 'boto' in module:\n",
    "        return \"1. 🏁 Setup & Config (Cloud/IO)\"\n",
    "\n",
    "    # 2. 🧹 Data Preparation (データ準備・前処理)\n",
    "    if 'Dataset' in name or 'dataset' in module:\n",
    "        return \"2. 🧹 Data Preparation\"\n",
    "    if 'convert' in name or 'validate' in name or 'pad' in name:\n",
    "        return \"2. 🧹 Data Preparation (Utils)\"\n",
    "    if 'Tokenizer' in name or 'Bin' in name:\n",
    "        return \"2. 🧹 Data Preparation (Tokenization)\"\n",
    "\n",
    "    # 3. 🧠 Model Loading (モデル定義・構築)\n",
    "    if ('Model' in name or 'Chronos' in name) and 'Pipeline' not in name and 'Config' not in name and 'Trainer' not in name:\n",
    "         # Model class definition itself\n",
    "         if 'Architecture' in desc or 'PreTrainedModel' in desc or 'Base class' in desc:\n",
    "             return \"3. 🧠 Model Definition\"\n",
    "\n",
    "    # 4. 🏋️ Training & Tuning (学習)\n",
    "    if 'Trainer' in name or 'Loss' in name or 'Callback' in name:\n",
    "        return \"4. 🏋️ Training & Tuning\"\n",
    "    \n",
    "    # 5. 🚀 Inference & Forecast (推論・予測)\n",
    "    if 'Pipeline' in name:\n",
    "        return \"5. 🚀 Inference (High-Level)\"\n",
    "    if 'Forecast' in name or 'predict' in name or 'quantiles' in name or 'Output' in name:\n",
    "        return \"5. 🚀 Inference (Result)\"\n",
    "\n",
    "    # 6. 🧩 Internal Processing (内部計算・レイヤー)\n",
    "    if any(x in name for x in ['Attention', 'Layer', 'Norm', 'Embedding', 'Block', 'Encoder', 'Decoder', 'MHA', 'MLP', 'RoPE']):\n",
    "        return \"6. 🧩 Internal Layers (Low-Level)\"\n",
    "\n",
    "    return \"7. 🛠️ Misc Utilities\"\n",
    "\n",
    "# --- 3. 適用と整形 ---\n",
    "df_event = df_catalog.copy()\n",
    "df_event['Event_Phase'] = df_event.apply(classify_by_event, axis=1)\n",
    "\n",
    "# Sort strictly by Event Phase then Name\n",
    "df_event = df_event.sort_values(by=['Event_Phase', 'Name'])\n",
    "\n",
    "# --- 4. 可視化 (Event Map) ---\n",
    "def print_event_map(df):\n",
    "    print(\"\\n📅 Chronos-2 Workflow Event Map\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    current_phase = \"\"\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        if row['Event_Phase'] != current_phase:\n",
    "            print(f\"\\n{row['Event_Phase']}\")\n",
    "            print(\"-\" * 50)\n",
    "            current_phase = row['Event_Phase']\n",
    "            \n",
    "        # 機能名と短い説明を表示\n",
    "        desc = row['Description'] if pd.notna(row['Description']) else \"\"\n",
    "        desc = (desc[:70] + '...') if len(desc) > 70 else desc\n",
    "        print(f\"  🔹 {row['Name']:<40} : {desc}\")\n",
    "\n",
    "print_event_map(df_event)\n",
    "\n",
    "# --- 5. 保存 (オプション) ---\n",
    "# df_event.to_csv(\"chronos_event_map.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3973ffd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting environment setup at: c:\\moinfo\n",
      "--------------------------------------------------\n",
      "✅ Created Directory: chronos_hf_local\n",
      "✅ Created Directory: chronos_bolt_local\n",
      "✅ Created Directory: libs/chronos/00_raw\n",
      "✅ Created Directory: libs/chronos/01_sandbox\n",
      "✅ Created Directory: libs/chronos/02_src/moinfo_chronos_ext/adapter\n",
      "✅ Created Directory: libs/chronos/02_src/moinfo_chronos_ext/pipeline\n",
      "✅ Created Directory: libs/chronos/02_src/moinfo_chronos_ext/utils\n",
      "✅ Created Directory: libs/chronos/03_scripts\n",
      "✅ Created Directory: libs/chronos/04_outputs/figs\n",
      "✅ Created Directory: libs/chronos/04_outputs/logs\n",
      "✅ Created Directory: libs/chronos/04_outputs/tables\n",
      "✅ Created Directory: libs/chronos/04_outputs/api\n",
      "✅ Created Directory: libs/chronos/05_reports\n",
      "✅ Created Directory: libs/chronos/06_tests\n",
      "✅ Created Directory: libs/chronos/07_configs\n",
      "✅ Created Directory: libs/chronos/08_logs\n",
      "✅ Created Directory: libs/chronos/09_docs\n",
      "📄 Created File:      chronos_hf_local/.keep\n",
      "📄 Created File:      chronos_bolt_local/.keep\n",
      "📄 Created File:      libs/chronos/02_src/moinfo_chronos_ext/__init__.py\n",
      "📄 Created File:      libs/chronos/02_src/moinfo_chronos_ext/adapter/__init__.py\n",
      "📄 Created File:      libs/chronos/02_src/moinfo_chronos_ext/pipeline/__init__.py\n",
      "📄 Created File:      libs/chronos/02_src/moinfo_chronos_ext/utils/__init__.py\n",
      "📄 Created File:      libs/chronos/README.md\n",
      "📄 Created File:      libs/chronos/07_configs/default_config.yaml\n",
      "--------------------------------------------------\n",
      "🎉 Setup completed successfully.\n",
      "   Next Step: Run 'pip install chronos-forecasting' if not installed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def create_project_structure(base_path: str):\n",
    "    \"\"\"\n",
    "    Chronos-2開発環境用のディレクトリ構造を、既存のTimesFM環境と整合性を保ちつつ構築します。\n",
    "    \n",
    "    Args:\n",
    "        base_path (str): プロジェクトのルートパス (例: C:/moinfo)\n",
    "    \"\"\"\n",
    "    root = Path(base_path)\n",
    "    \n",
    "    # 1. 作成するディレクトリの定義 (階層構造)\n",
    "    directories = [\n",
    "        # モデル格納用 (Root直下)\n",
    "        \"chronos_hf_local\",\n",
    "        \"chronos_bolt_local\",\n",
    "        \n",
    "        # ライブラリ開発領域\n",
    "        \"libs/chronos/00_raw\",\n",
    "        \"libs/chronos/01_sandbox\",\n",
    "        \"libs/chronos/02_src/moinfo_chronos_ext/adapter\",\n",
    "        \"libs/chronos/02_src/moinfo_chronos_ext/pipeline\",\n",
    "        \"libs/chronos/02_src/moinfo_chronos_ext/utils\",\n",
    "        \"libs/chronos/03_scripts\",\n",
    "        \"libs/chronos/04_outputs/figs\",\n",
    "        \"libs/chronos/04_outputs/logs\",\n",
    "        \"libs/chronos/04_outputs/tables\",\n",
    "        \"libs/chronos/04_outputs/api\",\n",
    "        \"libs/chronos/05_reports\",\n",
    "        \"libs/chronos/06_tests\",\n",
    "        \"libs/chronos/07_configs\",\n",
    "        \"libs/chronos/08_logs\",\n",
    "        \"libs/chronos/09_docs\",\n",
    "    ]\n",
    "\n",
    "    # 2. 空ファイル作成定義 (Pythonパッケージ化など)\n",
    "    files_to_create = {\n",
    "        # Keepファイル (Git管理用)\n",
    "        \"chronos_hf_local/.keep\": \"\",\n",
    "        \"chronos_bolt_local/.keep\": \"\",\n",
    "        \n",
    "        # Python Package Init\n",
    "        \"libs/chronos/02_src/moinfo_chronos_ext/__init__.py\": \n",
    "            '\"\"\"moinfo_chronos_ext package initialized.\"\"\"\\n__version__ = \"0.1.0\"',\n",
    "        \"libs/chronos/02_src/moinfo_chronos_ext/adapter/__init__.py\": \"\",\n",
    "        \"libs/chronos/02_src/moinfo_chronos_ext/pipeline/__init__.py\": \"\",\n",
    "        \"libs/chronos/02_src/moinfo_chronos_ext/utils/__init__.py\": \"\",\n",
    "        \n",
    "        # README templates\n",
    "        \"libs/chronos/README.md\": \n",
    "            \"# Chronos-2 Forecasting Workspace\\n\\nAmazon Chronos-2 & Bolt models integration workspace.\\n\",\n",
    "        \"libs/chronos/07_configs/default_config.yaml\": \n",
    "            \"# Default configuration for Chronos Pipeline\\nmodel_id: 'amazon/chronos-t5-small'\\ndevice: 'cpu'\\n\"\n",
    "    }\n",
    "\n",
    "    print(f\"🚀 Starting environment setup at: {root.absolute()}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # 3. ディレクトリ作成実行\n",
    "    for dir_path in directories:\n",
    "        full_path = root / dir_path\n",
    "        try:\n",
    "            full_path.mkdir(parents=True, exist_ok=True)\n",
    "            print(f\"✅ Created Directory: {dir_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to create {dir_path}: {e}\")\n",
    "\n",
    "    # 4. ファイル作成実行\n",
    "    for file_rel_path, content in files_to_create.items():\n",
    "        full_path = root / file_rel_path\n",
    "        try:\n",
    "            if not full_path.exists():\n",
    "                full_path.write_text(content, encoding='utf-8')\n",
    "                print(f\"📄 Created File:      {file_rel_path}\")\n",
    "            else:\n",
    "                print(f\"⏩ Skipped (Exists):  {file_rel_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to create file {file_rel_path}: {e}\")\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    print(\"🎉 Setup completed successfully.\")\n",
    "    print(\"   Next Step: Run 'pip install chronos-forecasting' if not installed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # カレントディレクトリをルートとして実行\n",
    "    create_project_structure(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bf7365",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 機能一覧（公式ドキュメント由来で再分類）\n",
    "\n",
    "| 分類     | 機能（何ができる？）                              | 具体の使い方/設定（見えている公式API）                                                                                  | 根拠                              |\n",
    "| ------ | --------------------------------------- | ------------------------------------------------------------------------------------------------------ | ------------------------------- |\n",
    "| 予測タスク  | 単変量（univariate：1系列）予測                   | 同一APIで実行                                                                                               | ([Hugging Face][1])             |\n",
    "| 予測タスク  | 多変量（multivariate：複数系列を同時）予測             | 同一アーキテクチャ内で対応                                                                                          | ([Hugging Face][1])             |\n",
    "| 予測タスク  | 共変量付き（covariate-informed）予測             | **過去のみ共変量**＋**既知の未来共変量**＋**カテゴリ共変量**に対応                                                                | ([Amazon Science][2])           |\n",
    "| 予測タスク  | アイテム間クロス学習（cross-learning across items） | 複数系列から学習を共有（コールドスタートに効くと説明）                                                                            | ([Hugging Face][1])             |\n",
    "| 推論特性   | ゼロショット（zero-shot：追加学習なしで予測）             | 事前学習モデルをそのまま `predict`/`predict_df`                                                                    | ([Hugging Face][1])             |\n",
    "| 出力     | 確率予測：分位点（quantile）をマルチステップで出す           | `quantile_levels=[0.1,0.5,0.9]` のように指定                                                                 | ([Hugging Face][1])             |\n",
    "| スケール   | 最大コンテキスト長 8192                          | 仕様表に明記                                                                                                 | ([Hugging Face][1])             |\n",
    "| スケール   | 最大予測長 1024                              | 仕様表に明記                                                                                                 | ([Hugging Face][1])             |\n",
    "| 性能/効率  | GPU/CPU 推論、A10Gで「毎秒300系列超」主張            | モデルカードに性能記載（※再現条件は別途確認推奨）                                                                              | ([Hugging Face][1])             |\n",
    "| ローカル利用 | Pythonパッケージ提供                           | `pip install \"chronos-forecasting>=2.0\"`                                                               | ([Hugging Face][1])             |\n",
    "| 入力I/F  | pandas API（DataFrame入力）                 | `Chronos2Pipeline.from_pretrained(...)` → `predict_df(...)`                                            | ([Hugging Face][1])             |\n",
    "| 入力I/F  | 時系列識別子・時刻列・ターゲット列を指定                    | `id_column`, `timestamp_column`, `target` を渡す                                                          | ([Hugging Face][1])             |\n",
    "| 入力I/F  | 未来共変量（known future covariates）を別DFで渡す   | `future_df=...`（ターゲット列を落としたDF例あり）                                                                      | ([Hugging Face][1])             |\n",
    "| 運用     | 本番は JumpStart 推奨（少ないコードでエンドポイント）        | READMEで「本番はJumpStart」推奨                                                                                | ([GitHub][3])                   |\n",
    "| デプロイ   | SageMaker へデプロイ（例：JumpStartModel）       | `JumpStartModel(model_id=\"pytorch-forecasting-chronos-2\", instance_type=\"ml.g5.2xlarge\")` → `deploy()` | ([Hugging Face][1])             |\n",
    "| デプロイ形態 | リアルタイム（GPU/CPU）/サーバレス/バッチ変換             | “新ガイドが3方式をカバー” と明記                                                                                     | ([GitHub][3])                   |\n",
    "| エコシステム | AWS公式ドキュメント側でも Chronos ノートブック導線         | 「JumpStartの例ノートブック」案内あり                                                                                | ([Amazon Web Services Docs][4]) |\n",
    "| 研究的裏付け | グループ注意（group attention）で系列/共変量間の情報共有    | 技術レポート要旨で明記                                                                                            | ([arXiv][5])                    |\n",
    "\n",
    "---\n",
    "\n",
    "## どういう「機能の塊」だと理解すると速いか（初心者向け）\n",
    "\n",
    "* **入力**：\n",
    "  過去のターゲット（target：予測したい値）に加えて、共変量（covariate：販促、天気、祝日など）を **過去分**・**未来既知分**として与えられる。 ([Amazon Science][2])\n",
    "* **中身**：\n",
    "  ICL（in-context learning：文脈内学習）で「今見えている過去のパターン」から、その場で予測を組み立てる。多変量や共変量の“関係性”は **group attention（グループ注意）**で取り込む、という立て付け。 ([Amazon Science][2])\n",
    "* **出力**：\n",
    "  点予測だけでなく、分位点（quantile）で不確実性つきの予測を返す（例：0.1/0.5/0.9）。 ([Hugging Face][1])\n",
    "* **運用**：\n",
    "  実験はローカル、本番は JumpStart → SageMaker エンドポイント化、という導線が公式に用意されている。 ([GitHub][3])\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0aa0570",
   "metadata": {},
   "source": [
    "## Model Downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d16872bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⬇️ Downloading Chronos-2 (Base)...\n",
      "\n",
      "📋 Arguments for `from_pretrained`:\n",
      "----------------------------------------------------------------------\n",
      "  • pretrained_model_name_or_path  (required)\n",
      "  • args                  (required)\n",
      "  • kwargs                (required)\n",
      "----------------------------------------------------------------------\n",
      "✅ Saved Chronos-2 model config & weights to: chronos_hf_local/\n",
      "\n",
      "⬇️ Downloading Chronos-Bolt (Small)...\n",
      "\n",
      "📋 Arguments for `from_pretrained`:\n",
      "----------------------------------------------------------------------\n",
      "  • pretrained_model_name_or_path  (required)\n",
      "  • args                  (required)\n",
      "  • kwargs                (required)\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf08897f4344de1a05c84bc0e2309dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f11242a8dfd4ff8907ec7dda12bda2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/191M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved Chronos-Bolt model config & weights to: chronos_bolt_local/\n",
      "\n",
      "🔍 Checking Arguments for Chronos-2 Prediction:\n",
      "\n",
      "📋 Arguments for `predict`:\n",
      "----------------------------------------------------------------------\n",
      "  • inputs               : Union (required)\n",
      "  • prediction_length    : int | None = None\n",
      "  • batch_size           : int = 256\n",
      "  • context_length       : int | None = None\n",
      "  • cross_learning       : bool = False\n",
      "  • limit_prediction_length : bool = False\n",
      "  • kwargs                (required)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "🔍 Checking Arguments for Bolt Prediction:\n",
      "\n",
      "📋 Arguments for `predict`:\n",
      "----------------------------------------------------------------------\n",
      "  • inputs               : Union (required)\n",
      "  • prediction_length    : Optional = None\n",
      "  • limit_prediction_length : bool = False\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "import torch\n",
    "from chronos import Chronos2Pipeline, ChronosBoltPipeline\n",
    "\n",
    "# --- 🛠️ 引数確認用ヘルパー関数 ---\n",
    "def show_args(func):\n",
    "    \"\"\"関数の引数（シグネチャ）を見やすく表示する\"\"\"\n",
    "    try:\n",
    "        sig = inspect.signature(func)\n",
    "        print(f\"\\n📋 Arguments for `{func.__name__}`:\")\n",
    "        print(\"-\" * 70)\n",
    "        for name, param in sig.parameters.items():\n",
    "            # デフォルト値の取得\n",
    "            if param.default == inspect.Parameter.empty:\n",
    "                default = \"(required)\"\n",
    "            else:\n",
    "                default = f\"= {param.default}\"\n",
    "            \n",
    "            # 型ヒントの取得\n",
    "            if param.annotation != inspect.Parameter.empty:\n",
    "                try:\n",
    "                    # クラス名などが取れる場合\n",
    "                    ann = f\": {param.annotation.__name__}\"\n",
    "                except AttributeError:\n",
    "                    # Typingオブジェクトなどの場合\n",
    "                    ann = f\": {str(param.annotation).replace('typing.', '')}\"\n",
    "            else:\n",
    "                ann = \"\"\n",
    "\n",
    "            print(f\"  • {name:<20} {ann} {default}\")\n",
    "        print(\"-\" * 70)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not inspect signature: {e}\")\n",
    "\n",
    "# ==========================================\n",
    "# 1. Chronos-2 (Base) のダウンロード\n",
    "# ==========================================\n",
    "print(\"\\n⬇️ Downloading Chronos-2 (Base)...\")\n",
    "\n",
    "# from_pretrained の引数を確認\n",
    "show_args(Chronos2Pipeline.from_pretrained)\n",
    "\n",
    "pipeline_c2 = Chronos2Pipeline.from_pretrained(\n",
    "    \"amazon/chronos-2\", \n",
    "    device_map=\"cpu\", \n",
    "    torch_dtype=torch.float32\n",
    ")\n",
    "\n",
    "# 【修正点】tokenizerの保存は不要（削除）\n",
    "pipeline_c2.model.save_pretrained(\"chronos_hf_local\")\n",
    "print(\"✅ Saved Chronos-2 model config & weights to: chronos_hf_local/\")\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 2. Chronos-Bolt (Small) のダウンロード\n",
    "# ==========================================\n",
    "print(\"\\n⬇️ Downloading Chronos-Bolt (Small)...\")\n",
    "\n",
    "# from_pretrained の引数を確認\n",
    "show_args(ChronosBoltPipeline.from_pretrained)\n",
    "\n",
    "pipeline_bolt = ChronosBoltPipeline.from_pretrained(\n",
    "    \"amazon/chronos-bolt-small\",\n",
    "    device_map=\"cpu\",\n",
    "    torch_dtype=torch.float32\n",
    ")\n",
    "\n",
    "# 【修正点】こちらもtokenizerの保存は不要（削除）\n",
    "pipeline_bolt.model.save_pretrained(\"chronos_bolt_local\")\n",
    "print(\"✅ Saved Chronos-Bolt model config & weights to: chronos_bolt_local/\")\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 3. 予測メソッド (predict) の引数を確認\n",
    "# ==========================================\n",
    "# 次のステップで使う重要なメソッドです\n",
    "print(\"\\n🔍 Checking Arguments for Chronos-2 Prediction:\")\n",
    "show_args(pipeline_c2.predict)\n",
    "\n",
    "print(\"\\n🔍 Checking Arguments for Bolt Prediction:\")\n",
    "show_args(pipeline_bolt.predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaiseki",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
