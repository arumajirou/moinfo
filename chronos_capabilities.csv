Category,Name,Module,Signature,Description
Class,BaseChronosPipeline,chronos.base,(inner_model: 'PreTrainedModel'),(No description available)
Class,ForecastType,chronos.base,"(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)",Create a collection of name/value pairs.
Class,PipelineRegistry,chronos.base,"(name, bases, attrs)",type(object) -> the object's type
Function,left_pad_and_stack_1D,chronos.base,(tensors: List[torch.Tensor]) -> torch.Tensor,(No description available)
Function,cache_model_from_s3,chronos.boto_utils,"(s3_uri: str, force_download: bool = False, boto3_session: boto3.session.Session | None = None)",(No description available)
Function,download_model_files_from_cloudfront,chronos.boto_utils,"(cloudfront_url: str, bucket: str, prefix: str, local_path: pathlib.Path, force_download: bool = False) -> None",(No description available)
Function,download_model_files_from_s3,chronos.boto_utils,"(bucket: str, prefix: str, local_path: pathlib.Path, force_download: bool = False, boto3_session: boto3.session.Session | None = None) -> None",(No description available)
Class,ChronosConfig,chronos.chronos,"(tokenizer_class: str, tokenizer_kwargs: Dict[str, Any], context_length: int, prediction_length: int, n_tokens: int, n_special_tokens: int, pad_token_id: int, eos_token_id: int, use_eos_token: bool, model_type: Literal['causal', 'seq2seq'], num_samples: int, temperature: float, top_k: int, top_p: float) -> None",This class holds all the configuration parameters to be used
Class,ChronosModel,chronos.chronos,"(config: chronos.chronos.ChronosConfig, model: transformers.modeling_utils.PreTrainedModel) -> None",A ``ChronosModel`` wraps a ``PreTrainedModel`` object from ``transformers``
Class,ChronosPipeline,chronos.chronos,"(tokenizer, model)",A ``ChronosPipeline`` uses the given tokenizer and model to forecast
Class,ChronosTokenizer,chronos.chronos,(),A ``ChronosTokenizer`` defines how time series are mapped into token IDs
Class,MeanScaleUniformBins,chronos.chronos,"(low_limit: float, high_limit: float, config: chronos.chronos.ChronosConfig) -> None",A ``ChronosTokenizer`` defines how time series are mapped into token IDs
Class,Chronos2CoreConfig,chronos.chronos2,"(d_model: int = 512, d_kv: int = 64, d_ff: int = 2048, num_layers: int = 6, num_heads: int = 8, dropout_rate: float = 0.1, layer_norm_epsilon: float = 1e-06, initializer_factor: float = 0.05, feed_forward_proj: str = 'relu', vocab_size: int = 2, pad_token_id: int = 0, rope_theta: float = 10000.0, attn_implementation: Optional[Literal['eager', 'sdpa']] = None, **kwargs)","HF transformers-style pretrained model config for Chronos-2.0, based on T5Config."
Class,Chronos2Dataset,chronos.chronos2,"(inputs: Sequence[Mapping[str, Union[torch.Tensor, numpy.ndarray, Mapping[str, torch.Tensor | numpy.ndarray | None]]]], context_length: int, prediction_length: int, batch_size: int, output_patch_size: int, min_past: int = 1, mode: str | chronos.chronos2.dataset.DatasetMode = <DatasetMode.TRAIN: 'train'>) -> None",A dataset wrapper for Chronos-2 models.
Class,Chronos2ForecastingConfig,chronos.chronos2,"(context_length: int, output_patch_size: int, input_patch_size: int, input_patch_stride: int, quantiles: List[float], use_reg_token: bool = False, use_arcsinh: bool = False, max_output_patches: int = 1, time_encoding_scale: int | None = None) -> None","Chronos2ForecastingConfig(context_length: int, output_patch_size: int, input_patch_size: int, input_patch_stride: int, quantiles: List[float], use_reg_token: bool = False, use_arcsinh: bool = False, max_output_patches: int = 1, time_encoding_scale: int | None = None)"
Class,Chronos2Model,chronos.chronos2,(config: chronos.chronos2.config.Chronos2CoreConfig),Base class for all models.
Class,Chronos2Pipeline,chronos.chronos2,(model: chronos.chronos2.model.Chronos2Model),(No description available)
Class,DatasetMode,chronos.chronos2.dataset,"(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)",str(object='') -> str
Function,convert_fev_window_to_list_of_dicts_input,chronos.chronos2.dataset,"(window: 'fev.EvaluationWindow', as_univariate: bool) -> tuple[list[dict[str, numpy.ndarray | dict[str, numpy.ndarray]]], list[str], list[str], list[str]]",(No description available)
Function,convert_list_of_tensors_input_to_list_of_dicts_input,chronos.chronos2.dataset,"(list_of_tensors: Sequence[torch.Tensor | numpy.ndarray]) -> list[dict[str, torch.Tensor]]",Convert a list of tensors input format to a list of dictionaries input format.
Function,convert_tensor_input_to_list_of_dicts_input,chronos.chronos2.dataset,"(tensor: torch.Tensor | numpy.ndarray) -> list[dict[str, torch.Tensor]]",Convert a tensor input format to a list of dictionaries input format.
Function,left_pad_and_cat_2D,chronos.chronos2.dataset,(tensors: list[torch.Tensor]) -> torch.Tensor,"Left pads tensors in the list to the length of the longest tensor along the second axis, then concats"
Function,validate_and_prepare_single_dict_task,chronos.chronos2.dataset,"(task: Mapping[str, Union[torch.Tensor, numpy.ndarray, Mapping[str, torch.Tensor | numpy.ndarray]]], idx: int, prediction_length: int) -> tuple[torch.Tensor, torch.Tensor, int, int, int]",Validates and prepares a single dictionary task for Chronos2Model.
Class,AttentionOutput,chronos.chronos2.layers,"(hidden_states: torch.Tensor | None = None, attn_weights: torch.Tensor | None = None) -> None","AttentionOutput(hidden_states: torch.Tensor | None = None, attn_weights: torch.Tensor | None = None)"
Class,Chronos2LayerNorm,chronos.chronos2.layers,"(hidden_size: int, eps: float = 1e-06)",Base class for all neural network modules.
Class,FeedForward,chronos.chronos2.layers,(config: chronos.chronos2.config.Chronos2CoreConfig),Base class for all neural network modules.
Class,GroupSelfAttention,chronos.chronos2.layers,(config: chronos.chronos2.config.Chronos2CoreConfig),Self-attention applied along the batch axis masked by the group attention mask
Class,MHA,chronos.chronos2.layers,"(config: chronos.chronos2.config.Chronos2CoreConfig, use_rope: bool = True)",Multi-head Attention Layer
Class,MLP,chronos.chronos2.layers,(config: chronos.chronos2.config.Chronos2CoreConfig),Base class for all neural network modules.
Class,ResidualBlock,chronos.chronos2.layers,"(in_dim: int, h_dim: int, out_dim: int, act_fn_name: str, dropout_p: float = 0.0, use_layer_norm: bool = False) -> None",A generic residual block which can be used for input and output embedding layers
Class,RoPE,chronos.chronos2.layers,"(dim: int, base: float = 10000)",Applies rotary position embeddings (RoPE) to input tensors.
Class,TimeCrossAttention,chronos.chronos2.layers,(config: chronos.chronos2.config.Chronos2CoreConfig),Base class for all neural network modules.
Class,TimeSelfAttention,chronos.chronos2.layers,(config: chronos.chronos2.config.Chronos2CoreConfig),Base class for all neural network modules.
Class,Chronos2Encoder,chronos.chronos2.model,(config: chronos.chronos2.config.Chronos2CoreConfig),Base class for all neural network modules.
Class,Chronos2EncoderBlock,chronos.chronos2.model,(config: chronos.chronos2.config.Chronos2CoreConfig),Base class for all neural network modules.
Class,Chronos2EncoderBlockOutput,chronos.chronos2.model,"(hidden_states: torch.Tensor | None = None, time_self_attn_weights: torch.Tensor | None = None, group_self_attn_weights: torch.Tensor | None = None) -> None","Chronos2EncoderBlockOutput(hidden_states: torch.Tensor | None = None, time_self_attn_weights: torch.Tensor | None = None, group_self_attn_weights: torch.Tensor | None = None)"
Class,Chronos2EncoderOutput,chronos.chronos2.model,"(last_hidden_state: torch.Tensor | None = None, all_time_self_attn_weights: tuple[torch.Tensor, ...] | None = None, all_group_self_attn_weights: tuple[torch.Tensor, ...] | None = None) -> None","Chronos2EncoderOutput(last_hidden_state: torch.Tensor | None = None, all_time_self_attn_weights: tuple[torch.Tensor, ...] | None = None, all_group_self_attn_weights: tuple[torch.Tensor, ...] | None = None)"
Class,Chronos2Output,chronos.chronos2.model,"(loss: torch.Tensor | None = None, quantile_preds: torch.Tensor | None = None, enc_time_self_attn_weights: tuple[torch.Tensor, ...] | None = None, enc_group_self_attn_weights: tuple[torch.Tensor, ...] | None = None) -> None","Chronos2Output(loss: torch.Tensor | None = None, quantile_preds: torch.Tensor | None = None, enc_time_self_attn_weights: tuple[torch.Tensor, ...] | None = None, enc_group_self_attn_weights: tuple[torch.Tensor, ...] | None = None)"
Class,InstanceNorm,chronos.chronos2.model,"(eps: float = 1e-05, use_arcsinh: bool = False) -> None",Apply standardization along the last dimension and optionally apply arcsinh after standardization.
Class,Patch,chronos.chronos2.model,"(patch_size: int, patch_stride: int) -> None",Base class for all neural network modules.
Function,convert_df_input_to_list_of_dicts_input,chronos.chronos2.pipeline,"(df: 'pd.DataFrame', future_df: 'pd.DataFrame | None', target_columns: list[str], prediction_length: int, id_column: str = 'item_id', timestamp_column: str = 'timestamp', validate_inputs: bool = True, freq: str | None = None) -> tuple[list[dict[str, numpy.ndarray | dict[str, numpy.ndarray]]], numpy.ndarray, dict[str, 'pd.DatetimeIndex']]",Convert from dataframe input format to a list of dictionaries input format.
Function,interpolate_quantiles,chronos.chronos2.pipeline,"(query_quantile_levels: torch.Tensor | list[float], original_quantile_levels: torch.Tensor | list[float], original_values: torch.Tensor) -> torch.Tensor",Interpolates quantile values at specified query levels using linear interpolation using original
Function,weighted_quantile,chronos.chronos2.pipeline,"(query_quantile_levels: torch.Tensor | list[float], sample_weights: torch.Tensor | list[float], samples: torch.Tensor)",Computes quantiles from a distribution specified by `samples` and their corresponding probability mass
Class,Chronos2Trainer,chronos.chronos2.trainer,"(model: Union[transformers.modeling_utils.PreTrainedModel, torch.nn.modules.module.Module, NoneType] = None, args: Optional[transformers.training_args.TrainingArguments] = None, data_collator: Optional[Callable[[list[Any]], dict[str, Any]]] = None, train_dataset: Union[torch.utils.data.dataset.Dataset, torch.utils.data.dataset.IterableDataset, ForwardRef('datasets.Dataset'), NoneType] = None, eval_dataset: Union[torch.utils.data.dataset.Dataset, dict[str, torch.utils.data.dataset.Dataset], ForwardRef('datasets.Dataset'), NoneType] = None, processing_class: Union[transformers.tokenization_utils_base.PreTrainedTokenizerBase, transformers.image_processing_utils.BaseImageProcessor, transformers.feature_extraction_utils.FeatureExtractionMixin, transformers.processing_utils.ProcessorMixin, NoneType] = None, model_init: Optional[Callable[..., transformers.modeling_utils.PreTrainedModel]] = None, compute_loss_func: Optional[Callable] = None, compute_metrics: Optional[Callable[[transformers.trainer_utils.EvalPrediction], dict]] = None, callbacks: Optional[list[transformers.trainer_callback.TrainerCallback]] = None, optimizers: tuple[typing.Optional[torch.optim.optimizer.Optimizer], typing.Optional[torch.optim.lr_scheduler.LambdaLR]] = (None, None), optimizer_cls_and_kwargs: Optional[tuple[type[torch.optim.optimizer.Optimizer], dict[str, Any]]] = None, preprocess_logits_for_metrics: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None)",A custom trainer based on transformers Trainer. We need to override the dataloader getters because we handle
Class,EvaluateAndSaveFinalStepCallback,chronos.chronos2.trainer,(),Callback to evaluate and save the model at last training step.
Function,seed_worker,chronos.chronos2.trainer,(worker_id: int),(No description available)
Class,ChronosBoltConfig,chronos.chronos_bolt,"(context_length: int, prediction_length: int, input_patch_size: int, input_patch_stride: int, quantiles: List[float], use_reg_token: bool = False) -> None","ChronosBoltConfig(context_length: int, prediction_length: int, input_patch_size: int, input_patch_stride: int, quantiles: List[float], use_reg_token: bool = False)"
Class,ChronosBoltModelForForecasting,chronos.chronos_bolt,(config: transformers.models.t5.configuration_t5.T5Config),This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
Class,ChronosBoltOutput,chronos.chronos_bolt,"(loss: Optional[torch.Tensor] = None, quantile_preds: Optional[torch.Tensor] = None, attentions: Optional[torch.Tensor] = None, cross_attentions: Optional[torch.Tensor] = None) -> None","ChronosBoltOutput(loss: Optional[torch.Tensor] = None, quantile_preds: Optional[torch.Tensor] = None, attentions: Optional[torch.Tensor] = None, cross_attentions: Optional[torch.Tensor] = None)"
Class,ChronosBoltPipeline,chronos.chronos_bolt,(model: chronos.chronos_bolt.ChronosBoltModelForForecasting),(No description available)
Function,validate_df_inputs,chronos.df_utils,"(df: 'pd.DataFrame', future_df: 'pd.DataFrame | None', target_columns: list[str], prediction_length: int, id_column: str = 'item_id', timestamp_column: str = 'timestamp') -> tuple['pd.DataFrame', 'pd.DataFrame | None', str, list[int], numpy.ndarray]",Validates and prepares dataframe inputs
